
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries
@article{liuhcps,
  title={Human-cyber-physical systems: concepts, challenges, and research opportunities},
  author={Liu, Zhiming and Wang, Ji},
  journal={Frontiers of Information Technology \& Electronic Engineering},
  volume={21},
  pages={1535--1553},
  year={2020},
  publisher={Springer}
}

@article{hcps,
title = {A human cyber physical system framework for operator 4.0 – artificial intelligence symbiosis},
journal = {Manufacturing Letters},
volume = {25},
pages = {10-15},
year = {2020},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2020.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S2213846320301279},
author = {Alexandros Bousdekis and Dimitris Apostolou and Gregoris Mentzas}
}

@article{dqn,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{doubleq,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{duelq,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  booktitle={International conference on machine learning},
  pages={1995--2003},
  year={2016},
  organization={PMLR}
}

@inproceedings{ac,
 author = {Konda, Vijay and Tsitsiklis, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Solla and T. Leen and K. M\"{u}ller},
 pages = {},
 publisher = {MIT Press},
 title = {Actor-Critic Algorithms},
 url = {https://proceedings.neurips.cc/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
 volume = {12},
 year = {1999}
}


@InProceedings{trpo,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{1996Reinforcement,
  title={Reinforcement learning: A survey},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Moore, Andrew W},
  journal={Journal of artificial intelligence research},
  volume={4},
  pages={237--285},
  year={1996}
}

@inproceedings{2007Go,
  title={Reinforcement Learning of Local Shape in the Game of Go.},
  author={Silver, David and Sutton, Richard S and M{\"u}ller, Martin},
  booktitle={IJCAI},
  volume={7},
  pages={1053--1058},
  year={2007}
}

@article{2016AlphaGo,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{quadrupedal1,
  title={Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer},
  author={Lai, Hang and Zhang, Weinan and He, Xialin and Yu, Chen and Tian, Zheng and Yu, Yong and Wang, Jun},
  journal={arXiv preprint arXiv:2212.07740},
  year={2022}
}

@article{quadrupedal2,
  title={Multi-embodiment Legged Robot Control as a Sequence Modeling Problem},
  author={Yu, Chen and Zhang, Weinan and Lai, Hang and Tian, Zheng and Kneip, Laurent and Wang, Jun},
  journal={arXiv preprint arXiv:2212.09078},
  year={2022}
}

@article{dqn2013,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{trustRL,
  title={Trustworthy reinforcement learning against intrinsic vulnerabilities: Robustness, safety, and generalizability},
  author={Xu, Mengdi and Liu, Zuxin and Huang, Peide and Ding, Wenhao and Cen, Zhepeng and Li, Bo and Zhao, Ding},
  journal={arXiv preprint arXiv:2209.08025},
  year={2022}
}

@article{iql,
    doi = {10.1371/journal.pone.0172395},
    author = {Tampuu, Ardi AND Matiisen, Tambet AND Kodelja, Dorian AND Kuzovkin, Ilya AND Korjus, Kristjan AND Aru, Juhan AND Aru, Jaan AND Vicente, Raul},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Multiagent cooperation and competition with deep reinforcement learning},
    year = {2017},
    month = {04},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0172395},
    pages = {1-15},
    number = {4},
}
@inproceedings{vdn,
author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
title = {Value-Decomposition Networks For Cooperative Multi-Agent Learning Based On Team Reward},
year = {2018},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {2085–2087},
numpages = {3},
keywords = {multi-agent, reinforcement learning, value-decomposition, q-learning, collaborative, neural networks, dqn},
location = {Stockholm, Sweden},
series = {AAMAS '18}
}

@Inbook{REINFORCE,
author="Williams, Ronald J.",
editor="Sutton, Richard S.",
title="Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",
bookTitle="Reinforcement Learning",
year="1992",
publisher="Springer US",
address="Boston, MA",
pages="5--32",
abstract="This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.",
isbn="978-1-4615-3618-5",
doi="10.1007/978-1-4615-3618-5_2",
url="https://doi.org/10.1007/978-1-4615-3618-5_2"
}



@article{qmix,
author = {Rashid, Tabish and Samvelyan, Mikayel and De Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
title = {Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {In many real-world settings, a team of agents must coordinate its behaviour while acting in a decentralised fashion. At the same time, it is often possible to train the agents in a centralised fashion where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a mixing network that estimates joint action-values as a monotonic combination of per-agent values. We structurally enforce that the joint-action value is monotonic in the per-agent values, through the use of non-negative weights in the mixing network, which guarantees consistency between the centralised and decentralised policies. To evaluate the performance of QMIX, we propose the StarCraft Multi-Agent Challenge (SMAC) as a new benchmark for deep multi-agent reinforcement learning. We evaluate QMIX on a challenging set of SMAC scenarios and show that it significantly outperforms existing multi-agent reinforcement learning methods.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {178},
numpages = {51},
keywords = {multi-agent coordination, multi-agent learning, reinforcement learning}
}


@InProceedings{qtran,
  title = 	 {{QTRAN}: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning},
  author =       {Son, Kyunghwan and Kim, Daewoo and Kang, Wan Ju and Hostallero, David Earl and Yi, Yung},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5887--5896},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/son19a/son19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/son19a.html},
  abstract = 	 {We explore value-based solutions for multi-agent reinforcement learning (MARL) tasks in the centralized training with decentralized execution (CTDE) regime popularized recently. However, VDN and QMIX are representative examples that use the idea of factorization of the joint action-value function into individual ones for decentralized execution. VDN and QMIX address only a fraction of factorizable MARL tasks due to their structural constraint in factorization such as additivity and monotonicity. In this paper, we propose a new factorization method for MARL, QTRAN, which is free from such structural constraints and takes on a new approach to transforming the original joint action-value function into an easily factorizable one, with the same optimal actions. QTRAN guarantees more general factorization than VDN or QMIX, thus covering a much wider class of MARL tasks than does previous methods. Our experiments for the tasks of multi-domain Gaussian-squeeze and modified predator-prey demonstrate QTRAN’s superior performance with especially larger margins in games whose payoffs penalize non-cooperative behavior more aggressively.}
}

@inproceedings{NEURIPS2019_f816dc0a,
 author = {Mahajan, Anuj and Rashid, Tabish and Samvelyan, Mikayel and Whiteson, Shimon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {MAVEN: Multi-Agent Variational Exploration},
 url = {https://proceedings.neurips.cc/paper/2019/file/f816dc0acface7498e10496222e9db10-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{qplex,
title={{\{}QPLEX{\}}: Duplex Dueling Multi-Agent Q-Learning},
author={Jianhao Wang and Zhizhou Ren and Terry Liu and Yang Yu and Chongjie Zhang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Rcmk0xxIQV}
}
@inproceedings{maddpg,
 author = {Lowe, Ryan and WU, YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
 url = {https://proceedings.neurips.cc/paper/2017/file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf},
 volume = {30},
 year = {2017}
}

@ARTICLE{7508798,
  author={Li, Li and Lv, Yisheng and Wang, Fei-Yue},
  journal={IEEE/CAA Journal of Automatica Sinica}, 
  title={Traffic signal timing via deep reinforcement learning}, 
  year={2016},
  volume={3},
  number={3},
  pages={247-254},
  doi={10.1109/JAS.2016.7508798}}

@ARTICLE{9103316,
  author={Wu, Tong and Zhou, Pan and Liu, Kai and Yuan, Yali and Wang, Xiumin and Huang, Huawei and Wu, Dapeng Oliver},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Multi-Agent Deep Reinforcement Learning for Urban Traffic Light Control in Vehicular Networks}, 
  year={2020},
  volume={69},
  number={8},
  pages={8243-8256},
  doi={10.1109/TVT.2020.2997896}}

@ARTICLE{9681232,
  author={Zhu, Yiting and He, Zhaocheng and Li, Guilong},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={A bi-Hierarchical Game-Theoretic Approach for Network-Wide Traffic Signal Control Using Trip-Based Data}, 
  year={2022},
  volume={23},
  number={9},
  pages={15408-15419},
  doi={10.1109/TITS.2022.3140511}}

@InProceedings{10.1007/978-3-030-47358-7_7,
author="Bhalla, Sushrut
and Ganapathi Subramanian, Sriram
and Crowley, Mark",
editor="Goutte, Cyril
and Zhu, Xiaodan",
title="Deep Multi Agent Reinforcement Learning for Autonomous Driving",
booktitle="Advances in Artificial Intelligence",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="67--78",
isbn="978-3-030-47358-7"
}
@ARTICLE{8638814,
  author={Yu, Chao and Wang, Xin and Xu, Xin and Zhang, Minjie and Ge, Hongwei and Ren, Jiankang and Sun, Liang and Chen, Bingcai and Tan, Guozhen},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Distributed Multiagent Coordinated Learning for Autonomous Driving in Highways Based on Dynamic Coordination Graphs}, 
  year={2020},
  volume={21},
  number={2},
  pages={735-748},
  doi={10.1109/TITS.2019.2893683}}

@ARTICLE{9694460,
  author={Huang, Zhiyu and Wu, Jingda and Lv, Chen},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Efficient Deep Reinforcement Learning With Imitative Expert Priors for Autonomous Driving}, 
  year={2022},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/TNNLS.2022.3142822}}
@article{zhou2022multi,
  title={Multi-agent reinforcement learning for cooperative lane changing of connected and autonomous vehicles in mixed traffic},
  author={Zhou, Wei and Chen, Dong and Yan, Jun and Li, Zhaojian and Yin, Huilin and Ge, Wanchen},
  journal={Autonomous Intelligent Systems},
  volume={2},
  number={1},
  pages={5},
  year={2022},
  publisher={Springer}
}
@InProceedings{pmlr-v155-zhou21a,
  title = 	 {SMARTS: An Open-Source Scalable Multi-Agent RL Training School for Autonomous Driving},
  author =       {Zhou, Ming and Luo, Jun and Villella, Julian and Yang, Yaodong and Rusu, David and Miao, Jiayu and Zhang, Weinan and Alban, Montgomery and FADAKAR, IMAN and Chen, Zheng and Huang, Chongxi and Wen, Ying and Hassanzadeh, Kimia and Graves, Daniel and Zhu, Zhengbang and Ni, Yihan and Nguyen, Nhat and Elsayed, Mohamed and Ammar, Haitham and Cowen-Rivers, Alexander and Ahilan, Sanjeevan and Tian, Zheng and Palenicek, Daniel and Rezaee, Kasra and Yadmellat, Peyman and Shao, Kun and chen, dong and Zhang, Baokuan and Zhang, Hongbo and Hao, Jianye and Liu, Wulong and Wang, Jun},
  booktitle = 	 {Proceedings of the 2020 Conference on Robot Learning},
  pages = 	 {264--285},
  year = 	 {2021},
  editor = 	 {Kober, Jens and Ramos, Fabio and Tomlin, Claire},
  volume = 	 {155},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v155/zhou21a/zhou21a.pdf},
  url = 	 {https://proceedings.mlr.press/v155/zhou21a.html}
}

@article{HADERER20221328,
title = {Education 4.0: Artificial Intelligence Assisted Task- and Time Planning System},
journal = {Procedia Computer Science},
volume = {200},
pages = {1328-1337},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.334},
url = {https://www.sciencedirect.com/science/article/pii/S187705092200343X},
author = {Bernhard Haderer and Monica Ciolacu},
keywords = {education 4.0, engineering education, graphic user interface, self-regulated learning, industry 4.0, sustainable higher education}
}

@article{SCHUMACHER2021100791,
title = {Investigating prompts for supporting students' self-regulation – A remaining challenge for learning analytics approaches?},
journal = {The Internet and Higher Education},
volume = {49},
pages = {100791},
year = {2021},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2020.100791},
url = {https://www.sciencedirect.com/science/article/pii/S1096751620300671},
author = {Clara Schumacher and Dirk Ifenthaler},
keywords = {Prompting, Self-regulated learning, Higher education, Learning analytics}
}

@INPROCEEDINGS{8990763,
  author={Ciolacu, Monica Ionita and Binder, Leon and Popp, Heribert},
  booktitle={2019 IEEE 25th International Symposium for Design and Technology in Electronic Packaging (SIITME)}, 
  title={Enabling IoT in Education 4.0 with BioSensors from Wearables and Artificial Intelligence}, 
  year={2019},
  volume={},
  number={},
  pages={17-24},
  doi={10.1109/SIITME47687.2019.8990763}}

@INPROCEEDINGS{asee_peer_40052,
author = "Ying Tang and Ryan Hare",
title = "Evaluation of an AI-assisted Adaptive Educational Game System",
booktitle = "2022 Spring ASEE Middle Atlantic Section Conference",
year = "2022",
month = "April",
address = "Newark, New Jersey",
publisher = "ASEE Conferences",
note = {https://peer.asee.org/40052}
}

@INPROCEEDINGS{9970680,
  author={Hare, Ryan and Tang, Ying},
  booktitle={2022 International Conference on Cyber-Physical Social Intelligence (ICCSI)}, 
  title={Petri Nets and Hierarchical Reinforcement Learning for Personalized Student Assistance in Serious Games}, 
  year={2022},
  volume={},
  number={},
  pages={733-737},
  doi={10.1109/ICCSI55536.2022.9970680}}

@INPROCEEDINGS{8615217,
  author={Liu, Su and Chen, Ye and Huang, Hui and Xiao, Liang and Hei, Xiaojun},
  booktitle={2018 IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE)}, 
  title={Towards Smart Educational Recommendations with Reinforcement Learning in Classroom}, 
  year={2018},
  volume={},
  number={},
  pages={1079-1084},
  doi={10.1109/TALE.2018.8615217}}

  @article{fu2022reinforcement,
  title={A Reinforcement Learning-Based Smart Educational Environment for Higher Education},
  author={Fu, Siyong},
  journal={International Journal of e-Collaboration (IJeC)},
  volume={19},
  number={6},
  pages={1--17},
  year={2022},
  publisher={IGI Global}
}

@article{maciel2019online,
  title={Online deep reinforcement learning for autonomous UAV navigation and exploration of outdoor environments},
  author={Maciel-Pearson, Bruna G and Marchegiani, Letizia and Akcay, Samet and Atapour-Abarghouei, Amir and Garforth, James and Breckon, Toby P},
  journal={arXiv preprint arXiv:1912.05684},
  year={2019}
}

@ARTICLE{9001167,
  author={Wang, Dawei and Fan, Tingxiang and Han, Tao and Pan, Jia},
  journal={IEEE Robotics and Automation Letters}, 
  title={A Two-Stage Reinforcement Learning Approach for Multi-UAV Collision Avoidance Under Imperfect Sensing}, 
  year={2020},
  volume={5},
  number={2},
  pages={3098-3105},
  doi={10.1109/LRA.2020.2974648}}

  @ARTICLE{9209079,
  author={Wang, Liang and Wang, Kezhi and Pan, Cunhua and Xu, Wei and Aslam, Nauman and Hanzo, Lajos},
  journal={IEEE Transactions on Cognitive Communications and Networking}, 
  title={Multi-Agent Deep Reinforcement Learning-Based Trajectory Planning for Multi-UAV Assisted Mobile Edge Computing}, 
  year={2021},
  volume={7},
  number={1},
  pages={73-84},
  doi={10.1109/TCCN.2020.3027695}}

@article{XU2020196,
title = {Optimized multi-UAV cooperative path planning under the complex confrontation environment},
journal = {Computer Communications},
volume = {162},
pages = {196-203},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.04.050},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420304096},
author = {Cheng Xu and Ming Xu and Chanjuan Yin},
keywords = {Multi-UAV, Path planning, GWO, Threat model}
}

@article{QIU2020515,
title = {A multi-objective pigeon-inspired optimization approach to UAV distributed flocking among obstacles},
journal = {Information Sciences},
volume = {509},
pages = {515-529},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2018.06.061},
url = {https://www.sciencedirect.com/science/article/pii/S0020025518305103},
author = {Huaxin Qiu and Haibin Duan},
keywords = {Unmanned aerial vehicle, Flocking control, Obstacle avoidance, Many-objective optimization, Pigeon-inspired optimization}
}

@article{xu_chen_2022, 
title={Autonomous and cooperative control of UAV cluster with multi-agent reinforcement learning}, 
volume={126}, 
DOI={10.1017/aer.2021.112}, 
number={1300}, 
journal={The Aeronautical Journal}, 
publisher={Cambridge University Press}, 
author={Xu, D. and Chen, G.}, 
year={2022}, 
pages={932–951}}

@Article{Xu2022,
author={Xu, Dan
and Chen, Gang},
title={The research on intelligent cooperative combat of UAV cluster with multi-agent reinforcement learning},
journal={Aerospace Systems},
year={2022},
month={Mar},
day={01},
volume={5},
number={1},
pages={107-121},
issn={2523-3955},
doi={10.1007/s42401-021-00105-x},
url={https://doi.org/10.1007/s42401-021-00105-x}
}

@article{journals/corr/abs-1803-07250,
  author    = {Huy Xuan Pham and
               Hung Manh La and
               David Feil{-}Seifer and
               Luan Van Nguyen},
  title     = {Cooperative and Distributed Reinforcement Learning of Drones for Field
               Coverage},
  journal   = {CoRR},
  volume    = {abs/1803.07250},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.07250},
  eprinttype = {arXiv},
  eprint    = {1803.07250},
  timestamp = {Mon, 13 Aug 2018 16:47:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-07250.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{9172262,
  author={Walker, Ory and Vanegas, Fernando and Gonzalez, Felipe and Koenig, Sven},
  booktitle={2020 IEEE Aerospace Conference}, 
  title={Multi-UAV Target-Finding in Simulated Indoor Environments using Deep Reinforcement Learning}, 
  year={2020},
  volume={},
  number={},
  pages={1-9},
  doi={10.1109/AERO47225.2020.9172262}}

  @ARTICLE{9453825,
  author={Mou, Zhiyu and Zhang, Yu and Gao, Feifei and Wang, Huangang and Zhang, Tao and Han, Zhu},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={Deep Reinforcement Learning Based Three-Dimensional Area Coverage With UAV Swarm}, 
  year={2021},
  volume={39},
  number={10},
  pages={3160-3176},
  doi={10.1109/JSAC.2021.3088718}}

@InProceedings{10.1007/978-981-19-2635-8_71,
author="Jo, Hyungeun
and Lee, Hoeun
and Jeon, Sangwoo
and Kaliappan, Vishnu Kumar
and Anh Nguyen, Tuan
and Min, Dugki
and Lee, Jae-Woo",
editor="Lee, Sangchul
and Han, Cheolheui
and Choi, Jeong-Yeol
and Kim, Seungkeun
and Kim, Jeong Ho",
title="Multi-agent Reinforcement Learning-Based UAS Control for Logistics Environments",
booktitle="The Proceedings of the 2021 Asia-Pacific International Symposium on Aerospace Technology (APISAT 2021), Volume 2",
year="2023",
publisher="Springer Nature Singapore",
address="Singapore",
pages="963--972",
isbn="978-981-19-2635-8"
}

@Article{en15197426,
AUTHOR = {Jeon, Sangwoo and Lee, Hoeun and Kaliappan, Vishnu Kumar and Nguyen, Tuan Anh and Jo, Hyungeun and Cho, Hyeonseo and Min, Dugki},
TITLE = {Multiagent Reinforcement Learning Based on Fusion-Multiactor-Attention-Critic for Multiple-Unmanned-Aerial-Vehicle Navigation Control},
JOURNAL = {Energies},
VOLUME = {15},
YEAR = {2022},
NUMBER = {19},
ARTICLE-NUMBER = {7426},
URL = {https://www.mdpi.com/1996-1073/15/19/7426},
ISSN = {1996-1073},
DOI = {10.3390/en15197426}
}

@ARTICLE{9993797,
  author={Shi, Haoran and Liu, Guanjun and Zhang, Kaiwen and Zhou, Ziyuan and Wang, Jiacun},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={MARL Sim2real Transfer: Merging Physical Reality With Digital Virtuality in Metaverse}, 
  year={2022},
  volume={},
  number={},
  pages={1-11},
  doi={10.1109/TSMC.2022.3229213}}

@InProceedings{10.1007/978-3-319-67361-5_40,
author="Shah, Shital
and Dey, Debadeepta
and Lovett, Chris
and Kapoor, Ashish",
editor="Hutter, Marco
and Siegwart, Roland",
title="AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles",
booktitle="Field and Service Robotics",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="621--635",
isbn="978-3-319-67361-5"
}

@inproceedings{NEURIPS2019_6646b06b,
 author = {Wang, Baoxiang and Hegde, Nidhi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Privacy-Preserving Q-Learning with Functional Noise in Continuous Spaces},
 url = {https://proceedings.neurips.cc/paper/2019/file/6646b06b90bd13dabc11ddba01270d23-Paper.pdf},
 volume = {32},
 year = {2019}
}

@INPROCEEDINGS{6736549,
  author={Venkitasubramaniam, Parv},
  booktitle={2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Privacy in stochastic control: A Markov Decision Process perspective}, 
  year={2013},
  volume={},
  number={},
  pages={381-388},
  doi={10.1109/Allerton.2013.6736549}}

  
@InProceedings{pmlr-v119-vietri20a,
  title = 	 {Private Reinforcement Learning with {PAC} and Regret Guarantees},
  author =       {Vietri, Giuseppe and Balle, Borja and Krishnamurthy, Akshay and Wu, Steven},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9754--9764},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/vietri20a/vietri20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/vietri20a.html}
}

@article{Chowdhury_Zhou_2022, title={Differentially Private Regret Minimization in Episodic Markov Decision Processes}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/20588}, DOI={10.1609/aaai.v36i6.20588}, number={6}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Chowdhury, Sayak Ray and Zhou, Xingyu}, year={2022}, month={Jun.}, pages={6375-6383} }

@inproceedings{10.1145/1390156.1390265,
author = {Sakuma, Jun and Kobayashi, Shigenobu and Wright, Rebecca N.},
title = {Privacy-Preserving Reinforcement Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390265},
doi = {10.1145/1390156.1390265},
abstract = {We consider the problem of distributed reinforcement learning (DRL) from private perceptions. In our setting, agents' perceptions, such as states, rewards, and actions, are not only distributed but also should be kept private. Conventional DRL algorithms can handle multiple agents, but do not necessarily guarantee privacy preservation and may not guarantee optimality. In this work, we design cryptographic solutions that achieve optimal policies without requiring the agents to share their private information.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {864–871},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@ARTICLE{8630059,
  author={Liu, Ximeng and Deng, Robert H. and Raymond Choo, Kim-Kwang and Yang, Yang},
  journal={IEEE Transactions on Emerging Topics in Computing}, 
  title={Privacy-Preserving Reinforcement Learning Design for Patient-Centric Dynamic Treatment Regimes}, 
  year={2021},
  volume={9},
  number={1},
  pages={456-470},
  doi={10.1109/TETC.2019.2896325}}

@inproceedings{10.5555/3306127.3331715,
author = {Pan, Xinlei and Wang, Weiyao and Zhang, Xiaoshuai and Li, Bo and Yi, Jinfeng and Song, Dawn},
title = {How You Act Tells a Lot: Privacy-Leaking Attack on Deep Reinforcement Learning},
year = {2019},
isbn = {9781450363099},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {368–376},
numpages = {9},
keywords = {privacy, dynamics recovery, deep reinforcement learning},
location = {Montreal QC, Canada},
series = {AAMAS '19}
}

@article{10.1145/3508028,
author = {Zhou, Xingyu},
title = {Differentially Private Reinforcement Learning with Linear Function Approximation},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
url = {https://doi.org/10.1145/3508028},
doi = {10.1145/3508028},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {feb},
articleno = {8},
numpages = {27},
keywords = {differential privacy, linear function approximations, reinforcement learning}
}

@article{fu2017learning,
  title={Learning robust rewards with adversarial inverse reinforcement learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  journal={arXiv preprint arXiv:1710.11248},
  year={2017}
}

@article{liu2021deceptive,
  title={Deceptive reinforcement learning for privacy-preserving planning},
  author={Liu, Zhengshang and Yang, Yue and Miller, Tim and Masters, Peta},
  journal={arXiv preprint arXiv:2102.03022},
  year={2021}
}

@article{Prakash_Husain_Paruchuri_Gujar_2022, title={How Private Is Your RL Policy? An Inverse RL Based Analysis Framework}, volume={36}, url={https://ojs.aaai.org/index.php/AAAI/article/view/20772}, DOI={10.1609/aaai.v36i7.20772}, abstractNote={Reinforcement Learning (RL) enables agents to learn how to perform various tasks from scratch. In domains like autonomous driving, recommendation systems, and more, optimal RL policies learned could cause a privacy breach if the policies memorize any part of the private reward. We study the set of existing differentially-private RL policies derived from various RL algorithms such as Value Iteration, Deep-Q Networks, and Vanilla Proximal Policy Optimization. We propose a new Privacy-Aware Inverse RL analysis framework (PRIL) that involves performing reward reconstruction as an adversarial attack on private policies that the agents may deploy. For this, we introduce the reward reconstruction attack, wherein we seek to reconstruct the original reward from a privacy-preserving policy using the Inverse RL algorithm. An adversary must do poorly at reconstructing the original reward function if the agent uses a tightly private policy. Using this framework, we empirically test the effectiveness of the privacy guarantee offered by the private algorithms on instances of the FrozenLake domain of varying complexities. Based on the analysis performed, we infer a gap between the current standard of privacy offered and the standard of privacy needed to protect reward functions in RL. We do so by quantifying the extent to which each private policy protects the reward function by measuring distances between the original and reconstructed rewards.}, number={7}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Prakash, Kritika and Husain, Fiza and Paruchuri, Praveen and Gujar, Sujit}, year={2022}, month={Jun.}, pages={8009-8016} }

@ARTICLE{9269516,
  author={Ye, Dayong and Zhu, Tianqing and Cheng, Zishuo and Zhou, Wanlei and Yu, Philip S.},
  journal={IEEE Transactions on Cybernetics}, 
  title={Differential Advising in Multiagent Reinforcement Learning}, 
  year={2022},
  volume={52},
  number={6},
  pages={5508-5521},
  doi={10.1109/TCYB.2020.3034424}}

@ARTICLE{9170873,
  author={Ye, Dayong and Zhu, Tianqing and Shen, Sheng and Zhou, Wanlei and Yu, Philip S.},
  journal={IEEE Transactions on Dependable and Secure Computing}, 
  title={Differentially Private Multi-Agent Planning for Logistic-Like Problems}, 
  year={2022},
  volume={19},
  number={2},
  pages={1212-1226},
  doi={10.1109/TDSC.2020.3017497}}

@article{yuan2022pp,
  title={PP-MARL: Efficient Privacy-Preserving MARL for Cooperative Intelligence in Communication},
  author={Yuan, Tingting and Chung, Hwei-Ming and Fu, Xiaoming},
  journal={arXiv preprint arXiv:2204.12064},
  year={2022}
}

@Article{en13010123,
AUTHOR = {Fang, Xiaohan and Wang, Jinkuan and Song, Guanru and Han, Yinghua and Zhao, Qiang and Cao, Zhiao},
TITLE = {Multi-Agent Reinforcement Learning Approach for Residential Microgrid Energy Scheduling},
JOURNAL = {Energies},
VOLUME = {13},
YEAR = {2020},
NUMBER = {1},
ARTICLE-NUMBER = {123},
URL = {https://www.mdpi.com/1996-1073/13/1/123},
ISSN = {1996-1073},
DOI = {10.3390/en13010123}
}

@article{van2021marl,
  title={MARL-iDR: Multi-Agent Reinforcement Learning for Incentive-based Residential Demand Response},
  author={van Tilburg, Jasper},
  year={2021}
}
@article{cheng2022multi,
  title={Multi-agent reinforcement learning via knowledge transfer with differentially private noise},
  author={Cheng, Zishuo and Ye, Dayong and Zhu, Tianqing and Zhou, Wanlei and Yu, Philip S and Zhu, Congcong},
  journal={International Journal of Intelligent Systems},
  volume={37},
  number={1},
  pages={799--828},
  year={2022},
  publisher={Wiley Online Library}
}

@ARTICLE{8685696,
  author={Ye, Dayong and Zhu, Tianqing and Zhou, Wanlei and Yu, Philip S.},
  journal={IEEE Transactions on Cybernetics}, 
  title={Differentially Private Malicious Agent Avoidance in Multiagent Advising Learning}, 
  year={2020},
  volume={50},
  number={10},
  pages={4214-4227},
  doi={10.1109/TCYB.2019.2906574}}

  @article{10.1145/3494672,
author = {Pessach, Dana and Shmueli, Erez},
title = {A Review on Fairness in Machine Learning},
year = {2022},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3494672},
doi = {10.1145/3494672},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {51},
numpages = {44},
keywords = {Algorithmic bias, fairness-aware machine learning, algorithmic fairness, fairness in machine learning}
}


@InProceedings{pmlr-v70-jabbari17a,
  title = 	 {Fairness in Reinforcement Learning},
  author =       {Shahin Jabbari and Matthew Joseph and Michael Kearns and Jamie Morgenstern and Aaron Roth},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1617--1626},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/jabbari17a/jabbari17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/jabbari17a.html},
  abstract = 	 {We initiate the study of fairness in reinforcement learning, where the actions of a learning algorithm may affect its environment and future rewards. Our fairness constraint requires that an algorithm never prefers one action over another if the long-term (discounted) reward of choosing the latter action is higher. Our first result is negative: despite the fact that fairness is consistent with the optimal policy, any learning algorithm satisfying fairness must take time exponential in the number of states to achieve non-trivial approximation to the optimal policy. We then provide a provably fair polynomial time algorithm under an approximate notion of fairness, thus establishing an exponential gap between exact and approximate fairness.}
}
@article{weng2019fairness,
  title={Fairness in reinforcement learning},
  author={Weng, Paul},
  journal={arXiv preprint arXiv:1907.10323},
  year={2019}
}


@INPROCEEDINGS{9488823,
  author={Chen, Jingdi and Wang, Yimeng and Lan, Tian},
  booktitle={IEEE INFOCOM 2021 - IEEE Conference on Computer Communications}, 
  title={Bringing Fairness to Actor-Critic Reinforcement Learning for Network Utility Optimization}, 
  year={2021},
  volume={},
  number={},
  pages={1-10},
  doi={10.1109/INFOCOM42981.2021.9488823}}

@article{xx2022108242,
title = {A competitive Markov decision process model and a recursive reinforcement-learning algorithm for fairness scheduling of agile satellites},
journal = {Computers \& Industrial Engineering},
volume = {169},
pages = {108242},
year = {2022},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2022.108242},
url = {https://www.sciencedirect.com/science/article/pii/S0360835222003126},
author = {Lili Ren and Xin Ning and Zheng Wang}
}

@article{deng2022reinforcement,
  title={Reinforcement Learning with Stepwise Fairness Constraints},
  author={Deng, Zhun and Sun, He and Wu, Zhiwei Steven and Zhang, Linjun and Parkes, David C},
  journal={arXiv preprint arXiv:2211.03994},
  year={2022}
}

@inproceedings{NEURIPS2019_10493aa8,
 author = {Jiang, Jiechuan and Lu, Zongqing},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Fairness in Multi-Agent Systems},
 url = {https://proceedings.neurips.cc/paper/2019/file/10493aa88605cad5ab4752b04a63d172-Paper.pdf},
 volume = {32},
 year = {2019}
}


@InProceedings{pmlr-v139-zimmer21a,
  title = 	 {Learning Fair Policies in Decentralized Cooperative Multi-Agent Reinforcement Learning},
  author =       {Zimmer, Matthieu and Glanois, Claire and Siddique, Umer and Weng, Paul},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12967--12978},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zimmer21a/zimmer21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zimmer21a.html},
  abstract = 	 {We consider the problem of learning fair policies in (deep) cooperative multi-agent reinforcement learning (MARL). We formalize it in a principled way as the problem of optimizing a welfare function that explicitly encodes two important aspects of fairness: efficiency and equity. We provide a theoretical analysis of the convergence of policy gradient for this problem. As a solution method, we propose a novel neural network architecture, which is composed of two sub-networks specifically designed for taking into account these two aspects of fairness. In experiments, we demonstrate the importance of the two sub-networks for fair optimization. Our overall approach is general as it can accommodate any (sub)differentiable welfare function. Therefore, it is compatible with various notions of fairness that have been proposed in the literature (e.g., lexicographic maximin, generalized Gini social welfare function, proportional fairness). Our method is generic and can be implemented in various MARL settings: centralized training and decentralized execution, or fully decentralized. Finally, we experimentally validate our approach in various domains and show that it can perform much better than previous methods, both in terms of efficiency and equity.}
}

@INPROCEEDINGS{9685661,
  author={Yuan, Mingqi and Cao, Qi and Pun, Man-On and Chen, Yi},
  booktitle={2021 IEEE Global Communications Conference (GLOBECOM)}, 
  title={Multi-Agent Reinforcement Learning-Based Fairness-Aware Scheduling for Bursty Traffic}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/GLOBECOM46510.2021.9685661}}

@misc{
huang2023fairnessaware,
title={Fairness-Aware Model-Based Multi-Agent Reinforcement Learning for Traffic Signal Control},
author={Xingshuai Huang and Di Wu and Benoit Boulet},
year={2023},
url={https://openreview.net/forum?id=sy0PqUr2fq9}
}
@article{WANG2022267,
title = {Multi-agent reinforcement learning for edge information sharing in vehicular networks},
journal = {Digital Communications and Networks},
volume = {8},
number = {3},
pages = {267-277},
year = {2022},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2021.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S2352864821000584},
author = {Ruyan Wang and Xue Jiang and Yujie Zhou and Zhidu Li and Dapeng Wu and Tong Tang and Alexander Fedotov and Vladimir Badenko}
}

@inproceedings{10.1145/3450268.3453525,
author = {Elmalaki, Salma},
title = {FaiR-IoT: Fairness-Aware Human-in-the-Loop Reinforcement Learning for Harnessing Human Variability in Personalized IoT},
year = {2021},
isbn = {9781450383547},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3450268.3453525},
doi = {10.1145/3450268.3453525},
abstract = {Thanks to the rapid growth in wearable technologies, monitoring complex human context becomes feasible, paving the way to develop human-in-the-loop IoT systems that naturally evolve to adapt to the human and environment state autonomously. Nevertheless, a central challenge in designing such personalized IoT applications arises from human variability. Such variability stems from the fact that different humans exhibit different behaviors when interacting with IoT applications (intra-human variability), the same human may change the behavior over time when interacting with the same IoT application (inter-human variability), and human behavior may be affected by the behaviors of other people in the same environment (multi-human variability). To that end, we propose FaiR-IoT, a general reinforcement learning-based framework for adaptive and fairness-aware human-in-the-loop IoT applications. In FaiR-IoT, three levels of reinforcement learning agents interact to continuously learn human preferences and maximize the system's performance and fairness while taking into account the intra-, inter-, and multi-human variability. We validate the proposed framework on two applications, namely (i) Human-in-the-Loop Automotive Advanced Driver Assistance Systems and (ii) Human-in-the-Loop Smart House. Results obtained on these two applications validate the generality of FaiR-IoT and its ability to provide a personalized experience while enhancing the system's performance by 40%-60% compared to non-personalized systems and enhancing the fairness of the multi-human systems by 1.5 orders of magnitude.},
booktitle = {Proceedings of the International Conference on Internet-of-Things Design and Implementation},
pages = {119–132},
numpages = {14},
keywords = {Fairness, Human-in-the-Loop, Human Adaptation, Reinforcement Learning, Personalized IoT},
location = {Charlottesvle, VA, USA},
series = {IoTDI '21}
}

@INPROCEEDINGS{9308468,
  author={Zhao, Wenshuai and Queralta, Jorge Peña and Westerlund, Tomi},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey}, 
  year={2020},
  volume={},
  number={},
  pages={737-744},
  doi={10.1109/SSCI47803.2020.9308468}}

@Article{electronics9091363,
AUTHOR = {Vithayathil Varghese, Nelson and Mahmoud, Qusay H.},
TITLE = {A Survey of Multi-Task Deep Reinforcement Learning},
JOURNAL = {Electronics},
VOLUME = {9},
YEAR = {2020},
NUMBER = {9},
ARTICLE-NUMBER = {1363},
URL = {https://www.mdpi.com/2079-9292/9/9/1363},
ISSN = {2079-9292},
DOI = {10.3390/electronics9091363}
}

@article{10.1613/jair.1.14174,
author = {Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rockt\"{a}schel, Tim},
title = {A Survey of Zero-Shot Generalisation in Deep Reinforcement Learning},
year = {2023},
issue_date = {Feb 2023},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {76},
issn = {1076-9757},
url = {https://doi.org/10.1613/jair.1.14174},
doi = {10.1613/jair.1.14174},
journal = {J. Artif. Int. Res.},
month = {feb},
numpages = {64}
}

@article{yuan2023transformer,
  title={Transformer in Reinforcement Learning for Decision-Making: A Survey},
  author={Yuan, William and Chen, Jiaxing and Chen, Shaofei and Lu, Lina and Hu, Zhenzhen and Li, Peng and Feng, Dawei and Liu, Furong and Chen, Jing},
  year={2023},
  publisher={TechRxiv}
}


@InProceedings{pmlr-v119-vezhnevets20a,
  title = 	 {{OP}tions as {RE}sponses: Grounding behavioural hierarchies in multi-agent reinforcement learning},
  author =       {Vezhnevets, Alexander and Wu, Yuhuai and Eckstein, Maria and Leblond, R{\'e}mi and Leibo, Joel Z},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9733--9742},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/vezhnevets20a/vezhnevets20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/vezhnevets20a.html}
}

@article{van2021model,
  title={Model based multi-agent reinforcement learning with tensor decompositions},
  author={Van Der Vaart, Pascal and Mahajan, Anuj and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2110.14524},
  year={2021}
}

@inproceedings{NEURIPS2019_3c3c139b,
 author = {Carion, Nicolas and Usunier, Nicolas and Synnaeve, Gabriel and Lazaric, Alessandro},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/3c3c139bd8467c1587a41081ad78045e-Paper.pdf},
 volume = {32},
 year = {2019}
}

@Article{pr11010267,
AUTHOR = {Zhu, Xiaofei and Xu, Jiazhong and Ge, Jianghua and Wang, Yaping and Xie, Zhiqiang},
TITLE = {Multi-Task Multi-Agent Reinforcement Learning for Real-Time Scheduling of a Dual-Resource Flexible Job Shop with Robots},
JOURNAL = {Processes},
VOLUME = {11},
YEAR = {2023},
NUMBER = {1},
ARTICLE-NUMBER = {267},
URL = {https://www.mdpi.com/2227-9717/11/1/267},
ISSN = {2227-9717},
DOI = {10.3390/pr11010267}
}
@inproceedings{
zhang2023discovering,
title={Discovering Generalizable Multi-agent Coordination Skills from Multi-task Offline Data},
author={Fuxiang Zhang and Chengxing Jia and Yi-Chen Li and Lei Yuan and Yang Yu and Zongzhang Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=53FyUAdP7d}
}

@ARTICLE{Liang2022-yf,
  title    = "Continuous self-adaptive optimization to learn multi-task
              multi-agent",
  author   = "Liang, Wenqian and Wang, Ji and Bao, Weidong and Zhu, Xiaomin and
              Wang, Qingyong and Han, Beibei",
  journal  = "Complex \& Intelligent Systems",
  volume   =  8,
  number   =  2,
  pages    = "1355--1367",
  month    =  apr,
  year     =  2022
}

@InProceedings{pmlr-v139-iqbal21a,
  title = 	 {Randomized Entity-wise Factorization for Multi-Agent Reinforcement Learning},
  author =       {Iqbal, Shariq and De Witt, Christian A Schroeder and Peng, Bei and Boehmer, Wendelin and Whiteson, Shimon and Sha, Fei},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4596--4606},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/iqbal21a/iqbal21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/iqbal21a.html}
}

@InProceedings{pmlr-v70-omidshafiei17a,
  title = 	 {Deep Decentralized Multi-task Multi-Agent Reinforcement Learning under Partial Observability},
  author =       {Shayegan Omidshafiei and Jason Pazis and Christopher Amato and Jonathan P. How and John Vian},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2681--2690},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/omidshafiei17a/omidshafiei17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/omidshafiei17a.html},
  abstract = 	 {Many real-world tasks involve multiple agents with partial observability and limited communication. Learning is challenging in these settings due to local viewpoints of agents, which perceive the world as non-stationary due to concurrently-exploring teammates. Approaches that learn specialized policies for individual tasks face problems when applied to the real world: not only do agents have to learn and store distinct policies for each task, but in practice identities of tasks are often non-observable, making these approaches inapplicable. This paper formalizes and addresses the problem of multi-task multi-agent reinforcement learning under partial observability. We introduce a decentralized single-task learning approach that is robust to concurrent interactions of teammates, and present an approach for distilling single-task policies into a unified policy that performs well across multiple related tasks, without explicit provision of task identity.}
}

@InProceedings{pmlr-v157-zhang21b,
  title = 	 {Multi-task Actor-Critic with Knowledge Transfer via a Shared Critic},
  author =       {Zhang, Gengzhi and Feng, Liang and Hou, Yaqing},
  booktitle = 	 {Proceedings of The 13th Asian Conference on Machine Learning},
  pages = 	 {580--593},
  year = 	 {2021},
  editor = 	 {Balasubramanian, Vineeth N. and Tsang, Ivor},
  volume = 	 {157},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--19 Nov},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v157/zhang21b/zhang21b.pdf},
  url = 	 {https://proceedings.mlr.press/v157/zhang21b.html},
  abstract = 	 {Multi-task actor-critic is a learning paradigm proposed in the literature to improve the learning efficiency of multiple actor-critics by sharing the learned policies across tasks while the reinforcement learning progresses online. However, existing multi-task actor-critic algorithms can only handle reinforcement learning tasks within the same problem domain, they may fail in cases where tasks possessing diverse state-action spaces. Taking this cue, in this paper, we embark a study on multi-task actor-critic with knowledge transfer via a share critic to enable the multi-task learning of actor-critic in heterogeneous state-action environments. Further, for efficient learning of the proposed multi-task actor-critic, a new formula for calculating the gradient of the actor network is also presented. To evaluate the performance of our approach, comprehensive empirical studies on continuous robotic tasks with different numbers of links. The experimental results confirmed the effectiveness of the proposed multi-task actor-critic algorithm.}
}
@article{LIANG2022163,
title = {Qauxi: Cooperative multi-agent reinforcement learning with knowledge transferred from auxiliary task},
journal = {Neurocomputing},
volume = {504},
pages = {163-173},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.06.091},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222008190},
author = {Wenqian Liang and Ji Wang and Weidong Bao and Xiaomin Zhu and Guanlin Wu and Dayu Zhang and Liyuan Niu},
keywords = {Multi-agent, Transfer learning, Reinforcement learning, MARL, Starcraft}
}


@InProceedings{pmlr-v161-zeng21a,
  title = 	 {A decentralized policy gradient approach to multi-task reinforcement learning},
  author =       {Zeng, Sihan and Anwar, Malik Aqeel and Doan, Thinh T. and Raychowdhury, Arijit and Romberg, Justin},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1002--1012},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/zeng21a/zeng21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/zeng21a.html}
}
@INPROCEEDINGS{9981319,
  author={Candela, Eduardo and Parada, Leandro and Marques, Luis and Georgescu, Tiberiu-Andrei and Demiris, Yiannis and Angeloudis, Panagiotis},
  booktitle={2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Transferring Multi-Agent Reinforcement Learning Policies for Autonomous Driving using Sim-to-Real}, 
  year={2022},
  volume={},
  number={},
  pages={8814-8820},
  doi={10.1109/IROS47612.2022.9981319}}

@article{gurevichreal,
  author={Gurevich, Anton and Bamani, Eran and Sintov, Avishai},
  title={Real-to-Sim-to-Real: Learning Models for Homogeneous Multi-Agent Systems},
  journal={https://eranbamani.github.io/eranbamani},
  year={2022}
}

@article{julian2019distributed,
  title={Distributed wildfire surveillance with autonomous aircraft using deep reinforcement learning},
  author={Julian, Kyle D and Kochenderfer, Mykel J},
  journal={Journal of Guidance, Control, and Dynamics},
  volume={42},
  number={8},
  pages={1768--1778},
  year={2019},
  publisher={American Institute of Aeronautics and Astronautics}
}

@article{wu2022reward,
  title={Reward Poisoning Attacks on Offline Multi-Agent Reinforcement Learning},
  author={Wu, Young and McMahan, Jermey and Zhu, Xiaojin and Xie, Qiaomin},
  journal={arXiv preprint arXiv:2206.01888},
  year={2022}
}

@INPROCEEDINGS{9283830,
  author={Lin, Jieyu and Dzeparoska, Kristina and Zhang, Sai Qian and Leon-Garcia, Alberto and Papernot, Nicolas},
  booktitle={2020 IEEE Security and Privacy Workshops (SPW)}, 
  title={On the Robustness of Cooperative Multi-Agent Reinforcement Learning}, 
  year={2020},
  volume={},
  number={},
  pages={62-68},
  doi={10.1109/SPW50608.2020.00027}}

  @InProceedings{Guo_2022_CVPR,
    author    = {Guo, Jun and Chen, Yonghong and Hao, Yihang and Yin, Zixin and Yu, Yin and Li, Simin},
    title     = {Towards Comprehensive Testing on the Robustness of Cooperative Multi-Agent Reinforcement Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    month     = {June},
    year      = {2022},
    pages     = {115-122}
}

@article{hu2022sparse,
  title={Sparse adversarial attack in multi-agent reinforcement learning},
  author={Hu, Yizheng and Zhang, Zhihua},
  journal={arXiv preprint arXiv:2205.09362},
  year={2022}
}

@article{pham2022evaluating,
  title={Evaluating Robustness of Cooperative MARL: A Model-based Approach},
  author={Pham, Nhan H and Nguyen, Lam M and Chen, Jie and Lam, Hoang Thanh and Das, Subhro and Weng, Tsui-Wei},
  journal={arXiv preprint arXiv:2202.03558},
  year={2022}
}

@article{li2023attacking,
  title={Attacking Cooperative Multi-Agent Reinforcement Learning by Adversarial Minority Influence},
  author={Li, Simin and Guo, Jun and Xiu, Jingqiao and Feng, Pu and Yu, Xin and Wang, Jiakai and Liu, Aishan and Wu, Wenjun and Liu, Xianglong},
  journal={arXiv preprint arXiv:2302.03322},
  year={2023}
}

@misc{
chen2022marnet,
title={{MARNET}: Backdoor Attacks against Value-Decomposition Multi-Agent Reinforcement Learning},
author={Yanjiao Chen and Zhicong Zheng and Xueluan Gong},
year={2022},
url={https://openreview.net/forum?id=-VsGCG_AQ69}
}

@ARTICLE{9541185,
  author={Wang, Yue and Sarkar, Esha and Li, Wenqing and Maniatakos, Michail and Jabari, Saif Eddin},
  journal={IEEE Transactions on Information Forensics and Security}, 
  title={Stop-and-Go: Exploring Backdoor Attacks on Deep Reinforcement Learning-Based Traffic Congestion Control Systems}, 
  year={2021},
  volume={16},
  number={},
  pages={4772-4787},
  doi={10.1109/TIFS.2021.3114024}}

@article{han2022solution,
  title={What is the Solution for State Adversarial Multi-Agent Reinforcement Learning?},
  author={Han, Songyang and Su, Sanbao and He, Sihong and Han, Shuo and Yang, Haizhao and Miao, Fei},
  journal={arXiv preprint arXiv:2212.02705},
  year={2022}
}

@article{zhou2022romfac,
  title={Romfac: A robust mean-field actor-critic reinforcement learning against adversarial perturbations on states},
  author={Zhou, Ziyuan and Liu, Guanjun},
  journal={arXiv preprint arXiv:2205.07229},
  year={2022}
}

@misc{
he2023robust,
title={Robust Multi-Agent Reinforcement Learning with State Uncertainties},
author={Sihong He and Songyang Han and Sanbao Su and Shuo Han and Shaofeng Zou and Fei Miao},
year={2023},
url={https://openreview.net/forum?id=Rl4ihTreFnV}
}


@InProceedings{pmlr-v80-yang18d,
  title = 	 {Mean Field Multi-Agent Reinforcement Learning},
  author =       {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5571--5580},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/yang18d/yang18d.pdf},
  url = 	 {https://proceedings.mlr.press/v80/yang18d.html}
}

@misc{
wang2023robust,
title={Robust Multi-Agent Reinforcement Learning against Adversaries on Observation},
author={Chenghe Wang and Yuhang Ran and Lei Yuan and Yang Yu and Zongzhang Zhang},
year={2023},
url={https://openreview.net/forum?id=eExA3Mk0Dxp}
}
@article{foerster2017learning,
  title={Learning with opponent-learning awareness},
  author={Foerster, Jakob N and Chen, Richard Y and Al-Shedivat, Maruan and Whiteson, Shimon and Abbeel, Pieter and Mordatch, Igor},
  journal={arXiv preprint arXiv:1709.04326},
  year={2017}
}

@article{Li_Wu_Cui_Dong_Fang_Russell_2019, title={Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4327}, DOI={10.1609/aaai.v33i01.33014213}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Li, Shihui and Wu, Yi and Cui, Xinyue and Dong, Honghua and Fang, Fei and Russell, Stuart}, year={2019}, month={Jul.}, pages={4213-4220} }

@INPROCEEDINGS{9812321,
  author={Sun, Chuangchuang and Kim, Dong-Ki and How, Jonathan P.},
  booktitle={2022 International Conference on Robotics and Automation (ICRA)}, 
  title={ROMAX: Certifiably Robust Deep Multiagent Reinforcement Learning via Convex Relaxation}, 
  year={2022},
  volume={},
  number={},
  pages={5503-5510},
  doi={10.1109/ICRA46639.2022.9812321}}

  @article{Phan_Belzner_Gabor_Sedlmeier_Ritz_Linnhoff-Popien_2021, title={Resilient Multi-Agent Reinforcement Learning with Adversarial Value Decomposition}, volume={35}, url={https://ojs.aaai.org/index.php/AAAI/article/view/17348}, DOI={10.1609/aaai.v35i13.17348}, abstractNote={We focus on resilience in cooperative multi-agent systems, where agents can change their behavior due to udpates or failures of hardware and software components. Current state-of-the-art approaches to cooperative multi-agent reinforcement learning (MARL) have either focused on idealized settings without any changes or on very specialized scenarios, where the number of changing agents is fixed, e.g., in extreme cases with only one productive agent. Therefore, we propose Resilient Adversarial value Decomposition with Antagonist-Ratios (RADAR). RADAR offers a value decomposition scheme to train competing teams of varying size for improved resilience against arbitrary agent changes. We evaluate RADAR in two cooperative multi-agent domains and show that RADAR achieves better worst case performance w.r.t. arbitrary agent changes than state-of-the-art MARL.}, number={13}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Phan, Thomy and Belzner, Lenz and Gabor, Thomas and Sedlmeier, Andreas and Ritz, Fabian and Linnhoff-Popien, Claudia}, year={2021}, month={May}, pages={11308-11316} }

  @misc{
hu2021robust,
title={Robust Multi-Agent Reinforcement Learning Driven by Correlated Equilibrium},
author={Yizheng Hu and Kun Shao and Dong Li and Jianye HAO and Wulong Liu and Yaodong Yang and Jun Wang and Zhanxing Zhu},
year={2021},
url={https://openreview.net/forum?id=JvPsKam58LX}
}

@inproceedings{NEURIPS2021_3a449677,
 author = {Wang, Yue and Zou, Shaofeng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {7193--7206},
 publisher = {Curran Associates, Inc.},
 title = {Online Robust Reinforcement Learning with Model Uncertainty},
 url = {https://proceedings.neurips.cc/paper/2021/file/3a4496776767aaa99f9804d0905fe584-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{NEURIPS2020_77441296,
 author = {Zhang, Kaiqing and SUN, TAO and Tao, Yunzhe and Genc, Sahika and Mallya, Sunil and Basar, Tamer},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {10571--10583},
 publisher = {Curran Associates, Inc.},
 title = {Robust Multi-Agent Reinforcement Learning with Model Uncertainty},
 url = {https://proceedings.neurips.cc/paper/2020/file/774412967f19ea61d448977ad9749078-Paper.pdf},
 volume = {33},
 year = {2020}
}

@article{wang2021backdoorl,
  title={Backdoorl: Backdoor attack against competitive reinforcement learning},
  author={Wang, Lun and Javed, Zaynah and Wu, Xian and Guo, Wenbo and Xing, Xinyu and Song, Dawn},
  journal={arXiv preprint arXiv:2105.00579},
  year={2021}
}

@article{guo2022backdoor,
  title={Backdoor detection in reinforcement learning},
  author={Guo, Junfeng and Li, Ang and Liu, Cong},
  journal={arXiv preprint arXiv:2202.03609},
  year={2022}
}


@InProceedings{pmlr-v139-guo21b,
  title = 	 {Adversarial Policy Learning in Two-player Competitive Games},
  author =       {Guo, Wenbo and Wu, Xian and Huang, Sui and Xing, Xinyu},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {3910--3919},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/guo21b/guo21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/guo21b.html}
}

@inproceedings{
sun2023certifiably,
title={Certifiably Robust Policy Learning against Adversarial Multi-Agent Communication},
author={Yanchao Sun and Ruijie Zheng and Parisa Hassanzadeh and Yongyuan Liang and Soheil Feizi and Sumitra Ganesh and Furong Huang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=dCOL0inGl3e}
}

@article{gu2022review,
  title={A review of safe reinforcement learning: Methods, theory and applications},
  author={Gu, Shangding and Yang, Long and Du, Yali and Chen, Guang and Walter, Florian and Wang, Jun and Yang, Yaodong and Knoll, Alois},
  journal={arXiv preprint arXiv:2205.10330},
  year={2022}
}

@article{JMLR:v16:garcia15a,
  author  = {Javier Garc{{\'i}}a and Fern and o Fern{{\'a}}ndez},
  title   = {A Comprehensive Survey on Safe Reinforcement Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {42},
  pages   = {1437--1480},
  url     = {http://jmlr.org/papers/v16/garcia15a.html}
}

@article{gu2021multi,
  title={Multi-agent constrained policy optimisation},
  author={Gu, Shangding and Kuba, Jakub Grudzien and Wen, Munning and Chen, Ruiqing and Wang, Ziyan and Tian, Zheng and Wang, Jun and Knoll, Alois and Yang, Yaodong},
  journal={arXiv preprint arXiv:2110.02793},
  year={2021}
}

@inproceedings{lu2021decentralized,
	title={Decentralized policy gradient descent ascent for safe multi-agent reinforcement learning},
	author={Lu, Songtao and Zhang, Kaiqing and Chen, Tianyi and Ba{\c{s}}ar, Tamer and Horesh, Lior},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={35},
	number={10},
	pages={8767--8775},
	year={2021}
}

@InProceedings{10.1007/978-3-030-86486-6_10,
author="Liu, Chenyi
and Geng, Nan
and Aggarwal, Vaneet
and Lan, Tian
and Yang, Yuan
and Xu, Mingwei",
editor="Oliver, Nuria
and P{\'e}rez-Cruz, Fernando
and Kramer, Stefan
and Read, Jesse
and Lozano, Jose A.",
title="CMIX: Deep Multi-agent Reinforcement Learning with Peak and Average Constraints",
booktitle="Machine Learning and Knowledge Discovery in Databases. Research Track",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="157--173",
isbn="978-3-030-86486-6"
}

@article{elsayed2021safe,
  title={Safe multi-agent reinforcement learning via shielding},
  author={ElSayed-Aly, Ingy and Bharadwaj, Suda and Amato, Christopher and Ehlers, R{\"u}diger and Topcu, Ufuk and Feng, Lu},
  journal={arXiv preprint arXiv:2101.11196},
  year={2021}
}

@article{sheebaelhamd2021safe,
  title={Safe Deep Reinforcement Learning for Multi-Agent Systems with Continuous Action Spaces},
  author={Sheebaelhamd, Ziyad and Zisis, Konstantinos and Nisioti, Athina and Gkouletsos, Dimitris and Pavllo, Dario and Kohler, Jonas},
  journal={arXiv preprint arXiv:2108.03952},
  year={2021}
}

﻿@Article{Pham2021,
author={Pham, Uyen
and Luu, Quoc
and Tran, Hien},
title={Multi-agent reinforcement learning approach for hedging portfolio problem},
journal={Soft Computing},
year={2021},
month={Jun},
day={01},
volume={25},
number={12},
pages={7877-7885},
issn={1433-7479},
doi={10.1007/s00500-021-05801-6},
url={https://doi.org/10.1007/s00500-021-05801-6}
}

@article{lee2020maps,
  title={MAPS: Multi-Agent reinforcement learning-based Portfolio management System},
  author={Lee, Jinho and Kim, Raehyun and Yi, Seok-Won and Kang, Jaewoo},
  journal={arXiv preprint arXiv:2007.05402},
  year={2020}
}

@article{huang2022mspm,
  title={MSPM: A modularized and scalable multi-agent reinforcement learning-based system for financial portfolio management},
  author={Huang, Zhenhan and Tanaka, Fumihide},
  journal={Plos one},
  volume={17},
  number={2},
  pages={e0263689},
  year={2022},
  publisher={Public Library of Science San Francisco, CA USA}
}

﻿@Article{Ma2023,
author={Ma, Cong
and Zhang, Jiangshe
and Li, Zongxin
and Xu, Shuang},
title={Multi-agent deep reinforcement learning algorithm with trend consistency regularization for portfolio management},
journal={Neural Computing and Applications},
year={2023},
month={Mar},
day={01},
volume={35},
number={9},
pages={6589-6601},
abstract={Financial portfolio management is reallocating the asset into financial products, whose goal is to maximize the profit under a certain risk. Since AlphaGo debated human professional players, deep reinforcement learning (DRL) algorithm has been widely used in various fields, including quantitative trading. The multi-agent system is a relatively new research branch in DRL, and its performance is better than that of a single agent in most cases. In this paper, we propose a novel multi-agent deep reinforcement learning algorithm with trend consistency regularization (TC-MARL) to find the optimal portfolio. Here, we divide the trend of stocks of one portfolio into two categories and train two different agents to learn the optimal trading strategy under these two stock trends. First, we build a trend consistency (TC) factor to recognize the consistency of several stocks from one portfolio. When the trend of these stocks is consistent, the factor is defined as 1; the trend is inconsistent, the factor is defined as {\$}{\$}-{\$}{\$} 1. Based on it, a novel regularization related to the weights is proposed and added to the reward function, named TC regularization. And the TC factor value is used as the sign of the regularization term. In this way, two agents with different reward functions are constructed, which have the same policy model and value model. Afterward, the proposed TC-MARL algorithm will dynamically switch between the two trained agents to find the optimal portfolio strategy according to the market status. Extensive experimental results on the Chinese Stock Market show the effectiveness of the proposed algorithm.},
issn={1433-3058},
doi={10.1007/s00521-022-08011-9},
url={https://doi.org/10.1007/s00521-022-08011-9}
}

@article{SHAVANDI2022118124,
title = {A multi-agent deep reinforcement learning framework for algorithmic trading in financial markets},
journal = {Expert Systems with Applications},
volume = {208},
pages = {118124},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118124},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422013082},
author = {Ali Shavandi and Majid Khedmati},
keywords = {Reinforcement learning, Multi-agent, Algorithmic trading, Multi-timeframe, Deep Q-learning}
}

@inproceedings{10.1145/3383455.3422570,
author = {Karpe, Micha\"{e}l and Fang, Jin and Ma, Zhongyao and Wang, Chen},
title = {Multi-Agent Reinforcement Learning in a Realistic Limit Order Book Market Simulation},
year = {2021},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422570},
doi = {10.1145/3383455.3422570},
abstract = {Optimal order execution is widely studied by industry practitioners and academic researchers because it determines the profitability of investment decisions and high-level trading strategies, particularly those involving large volumes of orders. However, complex and unknown market dynamics pose significant challenges for the development and validation of optimal execution strategies. In this paper, we propose a model-free approach by training Reinforcement Learning (RL) agents in a realistic market simulation environment with multiple agents. First, we configure a multi-agent historical order book simulation environment for execution tasks built on an Agent-Based Interactive Discrete Event Simulation (ABIDES) [6]. Second, we formulate the problem of optimal execution in an RL setting where an intelligent agent can make order execution and placement decisions based on market microstructure trading signals in High Frequency Trading (HFT). Third, we develop and train an RL execution agent using the Double Deep Q-Learning (DDQL) algorithm in the ABIDES environment. In some scenarios, our RL agent converges towards a Time-Weighted Average Price (TWAP) strategy. Finally, we evaluate the simulation with our RL agent by comparing it with a market replay simulation using real market Limit Order Book (LOB) data.},
booktitle = {Proceedings of the First ACM International Conference on AI in Finance},
articleno = {30},
numpages = {7},
keywords = {high-frequency trading, multi-agent reinforcement learning, optimal execution, limit order book, market simulation},
location = {New York, New York},
series = {ICAIF '20}
}

@article{briola2021deep,
  title={Deep reinforcement learning for active high frequency trading},
  author={Briola, Antonio and Turiel, Jeremy and Marcaccioli, Riccardo and Aste, Tomaso},
  journal={arXiv preprint arXiv:2101.07107},
  year={2021}
}

@article{bao2019multi,
  title={Multi-agent deep reinforcement learning for liquidation strategy analysis},
  author={Bao, Wenhang and Liu, Xiao-yang},
  journal={arXiv preprint arXiv:1906.11046},
  year={2019}
}

@article{patel2018optimizing,
  title={Optimizing market making using multi-agent reinforcement learning},
  author={Patel, Yagna},
  journal={arXiv preprint arXiv:1812.10252},
  year={2018}
}

@inproceedings{10.1145/3383455.3422570,
author = {Karpe, Micha\"{e}l and Fang, Jin and Ma, Zhongyao and Wang, Chen},
title = {Multi-Agent Reinforcement Learning in a Realistic Limit Order Book Market Simulation},
year = {2021},
isbn = {9781450375849},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383455.3422570},
doi = {10.1145/3383455.3422570},
articleno = {30},
numpages = {7},
keywords = {limit order book, market simulation, high-frequency trading, multi-agent reinforcement learning, optimal execution},
location = {New York, New York},
series = {ICAIF '20}
}

@inproceedings{qiu2021multi,
  title={Multi-Agent Reinforcement Learning for Automated Peer-to-Peer Energy Trading in Double-Side Auction Market.},
  author={Qiu, Dawei and Wang, Jianhong and Wang, Junkai and Strbac, Goran},
  booktitle={IJCAI},
  pages={2913--2920},
  year={2021}
}
@article{bao2019fairness,
  title={Fairness in multi-agent reinforcement learning for stock trading},
  author={Bao, Wenhang},
  journal={arXiv preprint arXiv:2001.00918},
  year={2019}
}

@ARTICLE{9931995,
  author={Qiu, Dawei and Wang, Jianhong and Dong, Zihang and Wang, Yi and Strbac, Goran},
  journal={IEEE Transactions on Power Systems}, 
  title={Mean-Field Multi-Agent Reinforcement Learning for Peer-to-Peer Multi-Energy Trading}, 
  year={2022},
  volume={},
  number={},
  pages={1-13},
  doi={10.1109/TPWRS.2022.3217922}}

@article{BAJO20126921,
title = {A multi-agent system for web-based risk management in small and medium business},
journal = {Expert Systems with Applications},
volume = {39},
number = {8},
pages = {6921-6931},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0957417412000036},
author = {Javier Bajo and María L. Borrajo and Juan F. {De Paz} and Juan M. Corchado and María A. Pellicer},
keywords = {Hybrid neural intelligent system, CBR, MAS, Business intelligence, Business risk prediction}
}

@article{GIANNAKIS201123,
title = {A multi-agent based framework for supply chain risk management},
journal = {Journal of Purchasing and Supply Management},
volume = {17},
number = {1},
pages = {23-31},
year = {2011},
issn = {1478-4092},
doi = {https://doi.org/10.1016/j.pursup.2010.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1478409210000282},
author = {Mihalis Giannakis and Michalis Louis},
keywords = {Multi-agent systems, Supply chain management, Risk management}
}

@article{ganesh2019reinforcement,
  title={Reinforcement learning for market making in a multi-agent dealer market},
  author={Ganesh, Sumitra and Vadori, Nelson and Xu, Mengda and Zheng, Hua and Reddy, Prashant and Veloso, Manuela},
  journal={arXiv preprint arXiv:1911.05892},
  year={2019}
}

@article{HE2023109985,
title = {A multi-agent virtual market model for generalization in reinforcement learning based trading strategies},
journal = {Applied Soft Computing},
volume = {134},
pages = {109985},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.109985},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623000030},
author = {Fei-Fan He and Chiao-Ting Chen and Szu-Hao Huang},
keywords = {Multi-agent systems, Market simulation, Reinforcement learning, Trading strategy}
}

@inproceedings{reddy2019risk,
  title={Risk averse reinforcement learning for mixed multi-agent environments},
  author={Reddy, D Sai Koti and Saha, Amrita and Tamilselvam, Srikanth G and Agrawal, Priyanka and Dayama, Pankaj},
  booktitle={Proceedings of the 18th international conference on autonomous agents and multiagent systems},
  pages={2171--2173},
  year={2019}
}

@InProceedings{10.1007/978-3-540-87805-6_15,
author="Servin, Arturo
and Kudenko, Daniel",
editor="Bergmann, Ralph
and Lindemann, Gabriela
and Kirn, Stefan
and P{\v{e}}chou{\v{c}}ek, Michal",
title="Multi-Agent Reinforcement Learning for Intrusion Detection: A Case Study and Evaluation",
booktitle="Multiagent System Technologies",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="159--170",
isbn="978-3-540-87805-6"
}

@article{SETHI2021102923,
title = {Attention based multi-agent intrusion detection systems using reinforcement learning},
journal = {Journal of Information Security and Applications},
volume = {61},
pages = {102923},
year = {2021},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2021.102923},
url = {https://www.sciencedirect.com/science/article/pii/S2214212621001411},
author = {Kamalakanta Sethi and Y. Venu Madhav and Rahul Kumar and Padmalochan Bera},
keywords = {Intrusion detection systems (IDS), Attention mechanism, Deep Q-Networks, Adversarial attack, Context, Denoising autoencoder decoder (DAE), Precision, Recall, F1-score, FPR, NSL-KDD, CICIDS2017}
}

@INPROCEEDINGS{9335796,
  author={Hsu, Ying-Feng and Matsuoka, Morito},
  booktitle={2020 IEEE 9th International Conference on Cloud Networking (CloudNet)}, 
  title={A Deep Reinforcement Learning Approach for Anomaly Network Intrusion Detection System}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/CloudNet51028.2020.9335796}}

@INPROCEEDINGS{9027452,
  author={Sethi, Kamalakanta and Kumar, Rahul and Prajapati, Nishant and Bera, Padmalochan},
  booktitle={2020 International Conference on COMmunication Systems \& NETworkS (COMSNETS)}, 
  title={Deep Reinforcement Learning based Intrusion Detection System for Cloud Infrastructure}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/COMSNETS48256.2020.9027452}}

@article{mohamed2021adversarial,
  title={ADVERSARIAL MULTI-AGENT REINFORCEMENT LEARNING ALGORITHM FOR ANOMALY NETWORK INTRUSION DETECTION SYSTEM.},
  author={Mohamed, Safa and Ejbali, Ridha},
  journal={International Journal on Information Technologies \& Security},
  volume={13},
  number={3},
  year={2021}
}

@inproceedings{louati2022distributed,
  title={A Distributed Intelligent Intrusion Detection System based on Parallel Machine Learning and Big Data Analysis.},
  author={Louati, Faten and Ktata, Farah Barika and Amor, Ikram Amous Ben},
  booktitle={SENSORNETS},
  pages={152--157},
  year={2022}
}

@inproceedings{chowdhary2021sdn,
  title={SDN-based Moving Target Defense using Multi-agent Reinforcement Learning},
  author={Chowdhary, Ankur and Huang, Dijiang and Sabur, Abdulhakim and Vadnere, Neha and Kang, Myong and Montrose, Bruce},
  booktitle={Proceedings of the first International Conference on Autonomous Intelligent Cyber defense Agents (AICA 2021)},
  year={2021},
  address="Paris, France",
  pages="15--16"
}

@INPROCEEDINGS{9348210,
  author={Suzuki, Akito and Harada, Shigeaki},
  booktitle={GLOBECOM 2020 - 2020 IEEE Global Communications Conference}, 
  title={Safe Multi-Agent Deep Reinforcement Learning for Dynamic Virtual Network Allocation}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
  doi={10.1109/GLOBECOM42002.2020.9348210}}

@article{SUN2020107230,
title = {MARVEL: Enabling controller load balancing in software-defined networks with multi-agent reinforcement learning},
journal = {Computer Networks},
volume = {177},
pages = {107230},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2020.107230},
url = {https://www.sciencedirect.com/science/article/pii/S138912861931566X},
author = {Penghao Sun and Zehua Guo and Gang Wang and Julong Lan and Yuxiang Hu},
keywords = {Multi-agent reinforcement learning, Neural networks, Software-defined networking, Switch migration}
}
@ARTICLE{9254093,
  author={Peng, Haixia and Shen, Xuemin},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={Multi-Agent Reinforcement Learning Based Resource Management in MEC- and UAV-Assisted Vehicular Networks}, 
  year={2021},
  volume={39},
  number={1},
  pages={131-141},
  doi={10.1109/JSAC.2020.3036962}}

@article{li2019cooperative,
  title={A cooperative multi-agent reinforcement learning framework for resource balancing in complex logistics network},
  author={Li, Xihan and Zhang, Jia and Bian, Jiang and Tong, Yunhai and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:1903.00714},
  year={2019}
}

@ARTICLE{8792117,
  author={Nasir, Yasar Sinan and Guo, Dongning},
  journal={IEEE Journal on Selected Areas in Communications}, 
  title={Multi-Agent Deep Reinforcement Learning for Dynamic Power Allocation in Wireless Networks}, 
  year={2019},
  volume={37},
  number={10},
  pages={2239-2250},
  doi={10.1109/JSAC.2019.2933973}}

@ARTICLE{9329087,
  author={Naderializadeh, Navid and Sydir, Jaroslaw J. and Simsek, Meryem and Nikopour, Hosein},
  journal={IEEE Transactions on Wireless Communications}, 
  title={Resource Management in Wireless Networks via Multi-Agent Deep Reinforcement Learning}, 
  year={2021},
  volume={20},
  number={6},
  pages={3507-3523},
  doi={10.1109/TWC.2021.3051163}}

@article{Jermin JeaunitaSarasvathi+2021+45+61,
author = {T. C. Jermin Jeaunita and V. Sarasvathi},
doi = {doi:10.2478/cait-2021-0042},
url = {https://doi.org/10.2478/cait-2021-0042},
title = {A Multi-Agent Reinforcement Learning-Based Optimized Routing for QoS in IoT},
journal = {Cybernetics and Information Technologies},
number = {4},
volume = {21},
year = {2021},
pages = {45--61}
}

@article{WANG2022102324,
title = {Solving job scheduling problems in a resource preemption environment with multi-agent reinforcement learning},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {77},
pages = {102324},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102324},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000138},
author = {Xiaohan Wang and Lin Zhang and Tingyu Lin and Chun Zhao and Kunyu Wang and Zhen Chen},
keywords = {Job shop scheduling problem, Reinforcement learning, Smart manufacturing, Multi-agent reinforcement learning, QMIX}
}

@article{ZHANG2022102412,
title = {Dynamic job shop scheduling based on deep reinforcement learning for multi-agent manufacturing systems},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102412},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102412},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000977},
author = {Yi Zhang and Haihua Zhu and Dunbing Tang and Tong Zhou and Yong Gui},
keywords = {Flexible job-shop scheduling problem, Smart manufacturing, Multi-agent manufacturing system, Reinforcement learning, Proximal policy optimization}
}

﻿@Article{Jing2022,
author={Jing, Xuan
and Yao, Xifan
and Liu, Min
and Zhou, Jiajun},
title={Multi-agent reinforcement learning based on graph convolutional network for flexible job shop scheduling},
journal={Journal of Intelligent Manufacturing},
year={2022},
month={Oct},
day={12},
issn={1572-8145},
doi={10.1007/s10845-022-02037-5},
url={https://doi.org/10.1007/s10845-022-02037-5}
}

@INPROCEEDINGS{9590925,
  author={Popper, Jens and Motsch, William and David, Alexander and Petzsche, Teresa and Ruskowski, Martin},
  booktitle={2021 International Conference on Electrical, Computer, Communications and Mechatronics Engineering (ICECCME)}, 
  title={Utilizing Multi-Agent Deep Reinforcement Learning For Flexible Job Shop Scheduling Under Sustainable Viewpoints}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICECCME52200.2021.9590925}}


@article{ZHANG2023110083,
title = {DeepMAG: Deep reinforcement learning with multi-agent graphs for flexible job shop scheduling},
journal = {Knowledge-Based Systems},
volume = {259},
pages = {110083},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.110083},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122011790},
author = {Jia-Dong Zhang and Zhixiang He and Wing-Ho Chan and Chi-Yin Chow},
keywords = {Deep learning, Reinforcement learning, Multi-agent graphs, Deep Q networks, Flexible job shop scheduling}
}

@InProceedings{10.1007/978-3-030-41913-4_1,
author="Mart{\'i}nez Jim{\'e}nez, Yailen
and Coto Palacio, Jessica
and Now{\'e}, Ann",
editor="Dorronsoro, Bernab{\'e}
and Ruiz, Patricia
and de la Torre, Juan Carlos
and Urda, Daniel
and Talbi, El-Ghazali",
title="Multi-Agent Reinforcement Learning Tool for Job Shop Scheduling Problems",
booktitle="Optimization and Learning",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="3--12",
isbn="978-3-030-41913-4"
}
@article{POPPER202263,
title = {Using Multi-Agent Deep Reinforcement Learning For Flexible Job Shop Scheduling Problems},
journal = {Procedia CIRP},
volume = {112},
pages = {63-67},
year = {2022},
note = {15th CIRP Conference on Intelligent Computation in ManufacturingEngineering, 14-16 July 2021},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2022.09.039},
url = {https://www.sciencedirect.com/science/article/pii/S2212827122011933},
author = {Jens Popper and Martin Ruskowski},
keywords = {Deep Learning, Reinforcement Learning, Flexible Job Shop Scheduling, Multi-Agent, Production Planning}
}

@article{LI202375,
title = {Deep reinforcement learning in smart manufacturing: A review and prospects},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {40},
pages = {75-101},
year = {2023},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2022.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1755581722001717},
author = {Chengxi Li and Pai Zheng and Yue Yin and Baicun Wang and Lihui Wang},
keywords = {Deep reinforcement learning, Smart manufacturing, Engineering life cycle, Artificial intelligence, Review}
}

@article{YU2021487,
title = {Optimizing task scheduling in human-robot collaboration with deep multi-agent reinforcement learning},
journal = {Journal of Manufacturing Systems},
volume = {60},
pages = {487-499},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001527},
author = {Tian Yu and Jing Huang and Qing Chang},
keywords = {Human-Robot Collaboration, Real-time task scheduling, Multi-agent reinforcement learning}
}

@article{agrawal_won_sharma_deshpande_mccomb_2021, title={A MULTI-AGENT REINFORCEMENT LEARNING FRAMEWORK FOR INTELLIGENT MANUFACTURING WITH AUTONOMOUS MOBILE ROBOTS}, volume={1}, DOI={10.1017/pds.2021.17}, journal={Proceedings of the Design Society}, publisher={Cambridge University Press}, author={Agrawal, Akash and Won, Sung Jun and Sharma, Tushar and Deshpande, Mayuri and McComb, Christopher}, year={2021}, pages={161–170}}


@ARTICLE{9700783,
  author={Liu, Xingjie and Wang, Guolei and Chen, Ken},
  journal={IEEE Transactions on Intelligent Transportation Systems}, 
  title={Option-Based Multi-Agent Reinforcement Learning for Painting With Multiple Large-Sized Robots}, 
  year={2022},
  volume={23},
  number={9},
  pages={15707-15715},
  doi={10.1109/TITS.2022.3145375}}

@article{krnjaic2022scalable,
  title={Scalable Multi-Agent Reinforcement Learning for Warehouse Logistics with Robotic and Human Co-Workers},
  author={Krnjaic, Aleksandar and Thomas, Jonathan D and Papoudakis, Georgios and Sch{\"a}fer, Lukas and B{\"o}rsting, Peter and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2212.11498},
  year={2022}
}

@INPROCEEDINGS{9376433,
  author={Lan, Xi and Qiao, Yuansong and Lee, Brian},
  booktitle={2021 7th International Conference on Automation, Robotics and Applications (ICARA)}, 
  title={Towards Pick and Place Multi Robot Coordination Using Multi-agent Deep Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={85-89},
  doi={10.1109/ICARA51699.2021.9376433}}

﻿@Article{Tan2019,
author={Tan, Qingmeng
and Tong, Yifei
and Wu, Shaofeng
and Li, Dongbo},
title={Modeling, planning, and scheduling of shop-floor assembly process with dynamic cyber-physical interactions: a case study for CPS-based smart industrial robot production},
journal={The International Journal of Advanced Manufacturing Technology},
year={2019},
month={Dec},
day={01},
volume={105},
number={9},
pages={3979-3989},
issn={1433-3015},
doi={10.1007/s00170-019-03940-7},
url={https://doi.org/10.1007/s00170-019-03940-7}
}

@article{ZHENG202116,
title = {Towards Self-X cognitive manufacturing network: An industrial knowledge graph-based multi-agent reinforcement learning approach},
journal = {Journal of Manufacturing Systems},
volume = {61},
pages = {16-26},
year = {2021},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0278612521001643},
author = {Pai Zheng and Liqiao Xia and Chengxi Li and Xinyu Li and Bufan Liu},
keywords = {Industrial knowledge graph, Graph embedding, Cognitive manufacturing, Graph neural network, Reinforcement learning}
}

@article{SU2022116323,
title = {Deep multi-agent reinforcement learning for multi-level preventive maintenance in manufacturing systems},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116323},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116323},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421016249},
author = {Jianyu Su and Jing Huang and Stephen Adams and Qing Chang and Peter A. Beling},
keywords = {Multi-level preventive maintenance, Deep multi-agent reinforcement learning, Deep reinforcement learning, Serial production line, Production loss}
}

@article{RUIZRODRIGUEZ2022102406,
title = {Multi-agent deep reinforcement learning based Predictive Maintenance on parallel machines},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {78},
pages = {102406},
year = {2022},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2022.102406},
url = {https://www.sciencedirect.com/science/article/pii/S0736584522000928},
author = {Marcelo Luis {Ruiz Rodríguez} and Sylvain Kubler and Andrea {de Giorgio} and Maxime Cordy and Jérémy Robert and Yves {Le Traon}},
keywords = {Predictive Maintenance, Scheduling, Reinforcement learning, Multi-agent systems, Industry 4.0}
}

@INPROCEEDINGS{9582667,
  author={Ho, Joshua and Wang, Chien-Min},
  booktitle={2021 IEEE 2nd International Conference on Human-Machine Systems (ICHMS)}, 
  title={Human-Centered AI using Ethical Causality and Learning Representation for Multi-Agent Deep Reinforcement Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICHMS53169.2021.9582667}}

@ARTICLE{9679742,
  author={Heuillet, Alexandre and Couthouis, Fabien and Díaz-Rodríguez, Natalia},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning With Shapley Values}, 
  year={2022},
  volume={17},
  number={1},
  pages={59-71},
  doi={10.1109/MCI.2021.3129959}}

@INPROCEEDINGS{9207564,
  author={Kazhdan, Dmitry and Shams, Zohreh and Lio, Pietro},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={MARLeME: A Multi-Agent Reinforcement Learning Model Extraction Library}, 
  year={2020},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN48605.2020.9207564}}

@InProceedings{10.1007/978-3-030-82017-6_12,
author="Ohana, Jean Jacques
and Ohana, Steve
and Benhamou, Eric
and Saltiel, David
and Guez, Beatrice",
editor="Calvaresi, Davide
and Najjar, Amro
and Winikoff, Michael
and Fr{\"a}mling, Kary",
title="Explainable AI (XAI) Models Applied to the Multi-agent Environment of Financial Markets",
booktitle="Explainable and Transparent AI and Multi-Agent Systems",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="189--207",
isbn="978-3-030-82017-6"
}

@article{liu2022mixrts,
  title={MIXRTs: Toward Interpretable Multi-Agent Reinforcement Learning via Mixing Recurrent Soft Decision Trees},
  author={Liu, Zichuan and Zhu, Yuanyang and Wang, Zhi and Chen, Chunlin},
  journal={arXiv preprint arXiv:2209.07225},
  year={2022}
}

@InProceedings{10.1007/978-3-031-26412-2_16,
author="Milani, Stephanie
and Zhang, Zhicheng
and Topin, Nicholay
and Shi, Zheyuan Ryan
and Kamhoua, Charles
and Papalexakis, Evangelos E.
and Fang, Fei",
editor="Amini, Massih-Reza
and Canu, St{\'e}phane
and Fischer, Asja
and Guns, Tias
and Kralj Novak, Petra
and Tsoumakas, Grigorios",
title="MAVIPER: Learning Decision Tree Policies for Interpretable Multi-agent Reinforcement Learning",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="251--266",
isbn="978-3-031-26412-2"
}

@article{ZHANG2021383,
title = {Structural relational inference actor-critic for multi-agent reinforcement learning},
journal = {Neurocomputing},
volume = {459},
pages = {383-394},
year = {2021},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.014},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221010481},
author = {Xianjie Zhang and Yu Liu and Xiujuan Xu and Qiong Huang and Hangyu Mao and Anil Carie},
keywords = {Multi-agent systems, Deep reinforcement learning, Variational autoencoder, Actor-critic, Graph neural network}
}


@InProceedings{pmlr-v205-zabounidis23a,
  title = 	 {Concept Learning for Interpretable Multi-Agent Reinforcement Learning},
  author =       {Zabounidis, Renos and Campbell, Joseph and Stepputtis, Simon and Hughes, Dana and Sycara, Katia P.},
  booktitle = 	 {Proceedings of The 6th Conference on Robot Learning},
  pages = 	 {1828--1837},
  year = 	 {2023},
  editor = 	 {Liu, Karen and Kulic, Dana and Ichnowski, Jeff},
  volume = 	 {205},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {14--18 Dec},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v205/zabounidis23a/zabounidis23a.pdf},
  url = 	 {https://proceedings.mlr.press/v205/zabounidis23a.html},
  abstract = 	 {Multi-agent robotic systems are increasingly operating in real-world environments in close proximity to humans, yet are largely controlled by policy models with inscrutable deep neural network representations. We introduce a method for incorporating interpretable concepts from a domain expert into models trained through multi-agent reinforcement learning, by requiring the model to first predict such concepts then utilize them for decision making. This allows an expert to both reason about the resulting concept policy models in terms of these high-level concepts at run-time, as well as intervene and correct mispredictions to improve performance. We show that this yields improved interpretability and training stability, with benefits to policy performance and sample efficiency in a simulated and real-world cooperative-competitive multi-agent game.}
}
@article{kapoor2018multi,
  title={Multi-agent reinforcement learning: A report on challenges and approaches},
  author={Kapoor, Sanyam},
  journal={arXiv preprint arXiv:1807.09427},
  year={2018}
}

@ARTICLE{9738819,
  author={Li, Tianxu and Zhu, Kun and Luong, Nguyen Cong and Niyato, Dusit and Wu, Qihui and Zhang, Yang and Chen, Bing},
  journal={IEEE Communications Surveys \& Tutorials}, 
  title={Applications of Multi-Agent Reinforcement Learning in Future Internet: A Comprehensive Survey}, 
  year={2022},
  volume={24},
  number={2},
  pages={1240-1279},
  doi={10.1109/COMST.2022.3160697}}

  @ARTICLE{9043893,
  author={Nguyen, Thanh Thi and Nguyen, Ngoc Duy and Nahavandi, Saeid},
  journal={IEEE Transactions on Cybernetics}, 
  title={Deep Reinforcement Learning for Multiagent Systems: A Review of Challenges, Solutions, and Applications}, 
  year={2020},
  volume={50},
  number={9},
  pages={3826-3839},
  doi={10.1109/TCYB.2020.2977374}}

@article{wang2022model,
  title={Model-based multi-agent reinforcement learning: Recent progress and prospects},
  author={Wang, Xihuai and Zhang, Zhicheng and Zhang, Weinan},
  journal={arXiv preprint arXiv:2203.10603},
  year={2022}
}

@article{gu2022review,
  title={A review of safe reinforcement learning: Methods, theory and applications},
  author={Gu, Shangding and Yang, Long and Du, Yali and Chen, Guang and Walter, Florian and Wang, Jun and Yang, Yaodong and Knoll, Alois},
  journal={arXiv preprint arXiv:2205.10330},
  year={2022}
}

@article{cui2022survey,
  title={A Survey on Large-Population Systems and Scalable Multi-Agent Reinforcement Learning},
  author={Cui, Kai and Tahir, Anam and Ekinci, Gizem and Elshamanhory, Ahmed and Eich, Yannick and Li, Mengguang and Koeppl, Heinz},
  journal={arXiv preprint arXiv:2209.03859},
  year={2022}
}

﻿@Article{Zhang2021,
author={Zhang, Kaiqing
and Yang, Zhuoran
and Ba{\c{s}}ar, Tamer},
title={Decentralized multi-agent reinforcement learning with networked agents: recent advances},
journal={Frontiers of Information Technology {\&} Electronic Engineering},
year={2021},
month={Jun},
day={01},
volume={22},
number={6},
pages={802-814},
abstract={Multi-agent reinforcement learning (MARL) has long been a significant research topic in both machine learning and control systems. Recent development of (single-agent) deep reinforcement learning has created a resurgence of interest in developing new MARL algorithms, especially those founded on theoretical analysis. In this paper, we review recent advances on a sub-area of this topic: decentralized MARL with networked agents. In this scenario, multiple agents perform sequential decision-making in a common environment, and without the coordination of any central controller, while being allowed to exchange information with their neighbors over a communication network. Such a setting finds broad applications in the control and operation of robots, unmanned vehicles, mobile sensor networks, and the smart grid. This review covers several of our research endeavors in this direction, as well as progress made by other researchers along the line. We hope that this review promotes additional research efforts in this exciting yet challenging area.},
issn={2095-9230},
doi={10.1631/FITEE.1900661},
url={https://doi.org/10.1631/FITEE.1900661}
}

@ARTICLE{9536399,
  author={Ilahi, Inaam and Usama, Muhammad and Qadir, Junaid and Janjua, Muhammad Umar and Al-Fuqaha, Ala and Hoang, Dinh Thai and Niyato, Dusit},
  journal={IEEE Transactions on Artificial Intelligence}, 
  title={Challenges and Countermeasures for Adversarial Attacks on Deep Reinforcement Learning}, 
  year={2022},
  volume={3},
  number={2},
  pages={90-109},
  doi={10.1109/TAI.2021.3111139}}

  ﻿@Article{Wong2022,
author={Wong, Annie
and B{\"a}ck, Thomas
and Kononova, Anna V.
and Plaat, Aske},
title={Deep multiagent reinforcement learning: challenges and directions},
journal={Artificial Intelligence Review},
year={2022},
month={Oct},
day={19},
abstract={This paper surveys the field of deep multiagent reinforcement learning (RL). The combination of deep neural networks with RL has gained increased traction in recent years and is slowly shifting the focus from single-agent to multiagent environments. Dealing with multiple agents is inherently more complex as (a) the future rewards depend on multiple players' joint actions and (b) the computational complexity increases. We present the most common multiagent problem representations and their main challenges, and identify five research areas that address one or more of these challenges: centralised training and decentralised execution, opponent modelling, communication, efficient coordination, and reward shaping. We find that many computational studies rely on unrealistic assumptions or are not generalisable to other settings; they struggle to overcome the curse of dimensionality or nonstationarity. Approaches from psychology and sociology capture promising relevant behaviours, such as communication and coordination, to help agents achieve better performance in multiagent settings. We suggest that, for multiagent RL to be successful, future research should address these challenges with an interdisciplinary approach to open up new possibilities in multiagent RL.},
issn={1573-7462},
doi={10.1007/s10462-022-10299-x},
url={https://doi.org/10.1007/s10462-022-10299-x}
}

@article{zhu2022survey,
  title={A survey of multi-agent reinforcement learning with communication},
  author={Zhu, Changxi and Dastani, Mehdi and Wang, Shihan},
  journal={arXiv preprint arXiv:2203.08975},
  year={2022}
}

@article{grimbly2021causal,
  title={Causal multi-agent reinforcement learning: Review and open problems},
  author={Grimbly, St John and Shock, Jonathan and Pretorius, Arnu},
  journal={arXiv preprint arXiv:2111.06721},
  year={2021}
}

@article{hernandez2018multiagent,
  title={Is multiagent deep reinforcement learning the answer or the question? A brief survey},
  author={Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E},
  journal={learning},
  volume={21},
  pages={22},
  year={2018}
}

@article{Liu_Wang_Hu_Hao_Chen_Gao_2020, title={Multi-Agent Game Abstraction via Graph Attention Neural Network}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6211}, DOI={10.1609/aaai.v34i05.6211}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Yong and Wang, Weixun and Hu, Yujing and Hao, Jianye and Chen, Xingguo and Gao, Yang}, year={2020}, month={Apr.}, pages={7211-7218} }

@article{Ryu_Shin_Park_2020, title={Multi-Agent Actor-Critic with Hierarchical Graph Attention Network}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6214}, DOI={10.1609/aaai.v34i05.6214}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Ryu, Heechang and Shin, Hayong and Park, Jinkyoo}, year={2020}, month={Apr.}, pages={7236-7243} }

@inproceedings{NEURIPS2021_65b9eea6,
 author = {Peng, Bei and Rashid, Tabish and Schroeder de Witt, Christian and Kamienny, Pierre-Alexandre and Torr, Philip and Boehmer, Wendelin and Whiteson, Shimon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {12208--12221},
 publisher = {Curran Associates, Inc.},
 title = {FACMAC: Factored Multi-Agent Centralised Policy Gradients},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf},
 volume = {34},
 year = {2021}
}


@InProceedings{pmlr-v80-yang18d,
  title = 	 {Mean Field Multi-Agent Reinforcement Learning},
  author =       {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5571--5580},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/yang18d/yang18d.pdf},
  url = 	 {https://proceedings.mlr.press/v80/yang18d.html},
  abstract = 	 {Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent’s optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.}
}

@inproceedings{10.5555/3398761.3398813,
author = {Ganapathi Subramanian, Sriram and Poupart, Pascal and Taylor, Matthew E. and Hegde, Nidhi},
title = {Multi Type Mean Field Reinforcement Learning},
year = {2020},
isbn = {9781450375184},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Mean field theory provides an effective way of scaling multiagent reinforcement learning algorithms to environments with many agents that can be abstracted by a virtual mean agent. In this paper, we extend mean field multiagent algorithms to multiple types. The types enable the relaxation of a core assumption in mean field games, which is that all agents in the environment are playing almost similar strategies and have the same goal. We conduct experiments on three different testbeds for the field of many agent reinforcement learning, based on the standard MAgents framework. We consider two different kinds of mean field games: a) Games where agents belong to predefined types that are known a priori and b) Games where the type of each agent is unknown and therefore must be learned based on observations. We introduce new algorithms for each type of game and demonstrate their superior performance over state of the art algorithms that assume that all agents belong to the same type and other baseline algorithms in the MAgent framework.},
booktitle = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {411–419},
numpages = {9},
keywords = {reinforcement learning, mean field methods, multiagent systems, many-agent learning},
location = {Auckland, New Zealand},
series = {AAMAS '20}
}

@inproceedings{10.5555/3463952.3464019,
author = {Ganapathi Subramanian, Sriram and Taylor, Matthew E. and Crowley, Mark and Poupart, Pascal},
title = {Partially Observable Mean Field Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Traditional multi-agent reinforcement learning algorithms are not scalable to environments with more than a few agents, since these algorithms are exponential in the number of agents. Recent research has introduced successful methods to scale multi-agent reinforcement learning algorithms to many agent scenarios using mean field theory. Previous work in this field assumes that an agent has access to exact cumulative metrics regarding the mean field behaviour of the system, which it can then use to take its actions. In this paper, we relax this assumption and maintain a distribution to model the uncertainty regarding the mean field of the system. We consider two different settings for this problem. In the first setting, only agents in a fixed neighbourhood are visible, while in the second setting, the visibility of agents is determined at random based on distances. For each of these settings, we introduce a Q-learning based algorithm that can learn effectively. We prove that this Q-learning estimate stays very close to the Nash Q-value (under a common set of assumptions) for the first setting. We also empirically show our algorithms outperform multiple baselines in three different games in the MAgents framework, which supports large environments with many agents learning simultaneously to achieve possibly distinct goals.},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {537–545},
numpages = {9},
keywords = {multi-agent reinforcement learning, partial observation, mean field theory, reinforcement learning},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{DBLP:conf/ijcai/ZhangY0XL21,
  author    = {Tianhao Zhang and
               Qiwei Ye and
               Jiang Bian and
               Guangming Xie and
               Tie{-}Yan Liu},
  editor    = {Zhi{-}Hua Zhou},
  title     = {{MFVFD:} {A} Multi-Agent Q-Learning Approach to Cooperative and Non-Cooperative
               Tasks},
  booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial
               Intelligence, {IJCAI} 2021, Virtual Event / Montreal, Canada, 19-27
               August 2021},
  pages     = {500--506},
  publisher = {ijcai.org},
  year      = {2021},
  url       = {https://doi.org/10.24963/ijcai.2021/70},
  doi       = {10.24963/ijcai.2021/70},
  timestamp = {Wed, 25 Aug 2021 17:11:16 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcai/ZhangY0XL21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ASHOK2022102433,
title = {Ethical framework for Artificial Intelligence and Digital technologies},
journal = {International Journal of Information Management},
volume = {62},
pages = {102433},
year = {2022},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2021.102433},
url = {https://www.sciencedirect.com/science/article/pii/S0268401221001262},
author = {Mona Ashok and Rohit Madan and Anton Joha and Uthayasankar Sivarajah},
keywords = {Artificial Intelligence (AI) ethics, Digital ethics, Digital technologies and archetypes, PRISMA, Systematic literature review, Ontological framework},
abstract = {The use of Artificial Intelligence (AI) in Digital technologies (DT) is proliferating a profound socio-technical transformation. Governments and AI scholarship have endorsed key AI principles but lack direction at the implementation level. Through a systematic literature review of 59 papers, this paper contributes to the critical debate on the ethical use of AI in DTs beyond high-level AI principles. To our knowledge, this is the first paper that identifies 14 digital ethics implications for the use of AI in seven DT archetypes using a novel ontological framework (physical, cognitive, information, and governance). The paper presents key findings of the review and a conceptual model with twelve propositions highlighting the impact of digital ethics implications on societal impact, as moderated by DT archetypes and mediated by organisational impact. The implications of intelligibility, accountability, fairness, and autonomy (under the cognitive domain), and privacy (under the information domain) are the most widely discussed in our sample. Furthermore, ethical implications related to the governance domain are shown to be generally applicable for most DT archetypes. Implications under the physical domain are less prominent when it comes to AI diffusion with one exception (safety). The key findings and resulting conceptual model have academic and professional implications.}

@inproceedings{kuzmin2002connectionist,
  title={Connectionist Q-learning in robot control task},
  author={Kuzmin, Valery},
  booktitle={Scientific proceedings of riga technical university},
  volume={5},
  pages={88--98},
  year={2002},
  organization={Citeseer}
}

@INPROCEEDINGS{7354126,
  author={Mordatch, Igor and Lowrey, Kendall and Todorov, Emanuel},
  booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Ensemble-CIO: Full-body dynamic motion planning that transfers to physical humanoids}, 
  year={2015},
  volume={},
  number={},
  pages={5307-5314},
  doi={10.1109/IROS.2015.7354126}}

@article{rajeswaran2016epopt,
  title={Epopt: Learning robust neural network policies using model ensembles},
  author={Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.01283},
  year={2016}
}
@InProceedings{Sadeghi_2018_CVPR,
author = {Sadeghi, Fereshteh and Toshev, Alexander and Jang, Eric and Levine, Sergey},
title = {Sim2Real Viewpoint Invariant Visual Servoing by Recurrent Control},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{ke2019learning,
  title={Learning neural causal models from unknown interventions},
  author={Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Sch{\"o}lkopf, Bernhard and Mozer, Michael C and Pal, Chris and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1910.01075},
  year={2019}
}

@article{scherrer2021learning,
  title={Learning neural causal models with active interventions},
  author={Scherrer, Nino and Bilaniuk, Olexa and Annadani, Yashas and Goyal, Anirudh and Schwab, Patrick and Sch{\"o}lkopf, Bernhard and Mozer, Michael C and Bengio, Yoshua and Bauer, Stefan and Ke, Nan Rosemary},
  journal={arXiv preprint arXiv:2109.02429},
  year={2021}
}


@InProceedings{pmlr-v119-zhang20t,
  title = 	 {Invariant Causal Prediction for Block {MDP}s},
  author =       {Zhang, Amy and Lyle, Clare and Sodhani, Shagun and Filos, Angelos and Kwiatkowska, Marta and Pineau, Joelle and Gal, Yarin and Precup, Doina},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {11214--11224},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/zhang20t/zhang20t.pdf},
  url = 	 {https://proceedings.mlr.press/v119/zhang20t.html},
  abstract = 	 {Generalization across environments is critical to the successful application of reinforcement learning (RL) algorithms to real-world challenges. In this work we propose a method for learning state abstractions which generalize to novel observation distributions in the multi-environment RL setting. We prove that for certain classes of environments, this approach outputs, with high probability, a state abstraction corresponding to the causal feature set with respect to the return. We give empirical evidence that analogous methods for the nonlinear setting can also attain improved generalization over single- and multi-task baselines. Lastly, we provide bounds on model generalization error in the multi-environment setting, in the process showing a connection between causal variable identification and the state abstraction framework for MDPs.}
}


@InProceedings{pmlr-v70-finn17a,
  title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author =       {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1126--1135},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/finn17a.html},
  abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}

@INPROCEEDINGS{9196540,
  author={Arndt, Karol and Hazara, Murtaza and Ghadirzadeh, Ali and Kyrki, Ville},
  booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Meta Reinforcement Learning for Sim-to-real Domain Adaptation}, 
  year={2020},
  volume={},
  number={},
  pages={2725-2731},
  doi={10.1109/ICRA40945.2020.9196540}}


@InProceedings{pmlr-v80-kaplanis18a,
  title = 	 {Continual Reinforcement Learning with Complex Synapses},
  author =       {Kaplanis, Christos and Shanahan, Murray and Clopath, Claudia},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2497--2506},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/kaplanis18a/kaplanis18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/kaplanis18a.html},
  abstract = 	 {Unlike humans, who are capable of continual learning over their lifetimes, artificial neural networks have long been known to suffer from a phenomenon known as catastrophic forgetting, whereby new learning can lead to abrupt erasure of previously acquired knowledge. Whereas in a neural network the parameters are typically modelled as scalar values, an individual synapse in the brain comprises a complex network of interacting biochemical components that evolve at different timescales. In this paper, we show that by equipping tabular and deep reinforcement learning agents with a synaptic model that incorporates this biological complexity (Benna \&amp; Fusi, 2016), catastrophic forgetting can be mitigated at multiple timescales. In particular, we find that as well as enabling continual learning across sequential training of two simple tasks, it can also be used to overcome within-task forgetting by reducing the need for an experience replay database.}
}

@article{de2020overview,
  title={An overview of privacy in machine learning},
  author={De Cristofaro, Emiliano},
  journal={arXiv preprint arXiv:2005.08679},
  year={2020}
}

@InProceedings{10.1007/978-3-030-57321-8_5,
author="Puiutta, Erika
and Veith, Eric M. S. P.",
editor="Holzinger, Andreas
and Kieseberg, Peter
and Tjoa, A Min
and Weippl, Edgar",
title="Explainable Reinforcement Learning: A Survey",
booktitle="Machine Learning and Knowledge Extraction",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="77--95",
isbn="978-3-030-57321-8"
}

@article{10.1145/3527448,
author = {Vouros, George A.},
title = {Explainable Deep Reinforcement Learning: State of the Art and Challenges},
year = {2022},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3527448},
doi = {10.1145/3527448},
journal = {ACM Comput. Surv.},
month = {dec},
articleno = {92},
numpages = {39},
keywords = {transparency, explainability, interpretability, Deep learning, deep reinforcement learning}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@INPROCEEDINGS{7467366,
  author={Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
  booktitle={2016 IEEE European Symposium on Security and Privacy (EuroS\&P)}, 
  title={The Limitations of Deep Learning in Adversarial Settings}, 
  year={2016},
  volume={},
  number={},
  pages={372-387},
  doi={10.1109/EuroSP.2016.36}}

@article{huang2017adversarial,
  title={Adversarial attacks on neural network policies},
  author={Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1702.02284},
  year={2017}
}

@article{jiao2022asynchronous,
  title={Asynchronous Distributed Bilevel Optimization},
  author={Jiao, Yang and Yang, Kai and Wu, Tiancheng and Song, Dongjin and Jian, Chengtao},
  journal={arXiv preprint arXiv:2212.10048},
  year={2022}
}

@ARTICLE{9705079,
  author={Jiao, Yang and Yang, Kai and Song, Dongjing and Tao, Dacheng},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={TimeAutoAD: Autonomous Anomaly Detection With Self-Supervised Contrastive Loss for Multivariate Time Series}, 
  year={2022},
  volume={9},
  number={3},
  pages={1604-1619},
  doi={10.1109/TNSE.2022.3148276}}

@article{jiao2022distributed,
  title={Distributed Distributionally Robust Optimization with Non-Convex Objectives},
  author={Jiao, Yang and Yang, Kai and Song, Dongjin},
  journal={arXiv preprint arXiv:2210.07588},
  year={2022}
}

﻿@Article{Uc-Cetina2023,
author={Uc-Cetina, V{\'i}ctor
and Navarro-Guerrero, Nicol{\'a}s
and Martin-Gonzalez, Anabel
and Weber, Cornelius
and Wermter, Stefan},
title={Survey on reinforcement learning for language processing},
journal={Artificial Intelligence Review},
year={2023},
month={Feb},
day={01},
volume={56},
number={2},
pages={1543-1575},
issn={1573-7462},
doi={10.1007/s10462-022-10205-5},
url={https://doi.org/10.1007/s10462-022-10205-5}
}

@article{li2016deep,
  title={Deep reinforcement learning for dialogue generation},
  author={Li, Jiwei and Monroe, Will and Ritter, Alan and Galley, Michel and Gao, Jianfeng and Jurafsky, Dan},
  journal={arXiv preprint arXiv:1606.01541},
  year={2016}
}

@ARTICLE{9025776,
  author={Yang, Min and Huang, Weiyi and Tu, Wenting and Qu, Qiang and Shen, Ying and Lei, Kai},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Multitask Learning and Reinforcement Learning for Personalized Dialog Generation: An Empirical Study}, 
  year={2021},
  volume={32},
  number={1},
  pages={49-62},
  doi={10.1109/TNNLS.2020.2975035}}
  
@article{Lu_Zhang_Chen_2019, title={Goal-Oriented Dialogue Policy Learning from Failures}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/4107}, DOI={10.1609/aaai.v33i01.33012596}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Lu, Keting and Zhang, Shiqi and Chen, Xiaoping}, year={2019}, month={Jul.}, pages={2596-2603} }

@inproceedings{chen-etal-2017-line,
    title = "On-line Dialogue Policy Learning with Companion Teaching",
    author = "Chen, Lu  and
      Yang, Runzhe  and
      Chang, Cheng  and
      Ye, Zihao  and
      Zhou, Xiang  and
      Yu, Kai",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-2032",
    pages = "198--204"
}

@article{SU201824,
title = {Reward estimation for dialogue policy optimisation},
journal = {Computer Speech \& Language},
volume = {51},
pages = {24-43},
year = {2018},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2018.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0885230817300840},
author = {Pei-Hao Su and Milica Gašić and Steve Young},
keywords = {Dialogue systems, Reinforcement learning, Deep learning, Reward estimation, Gaussian process, Active learning},
}

@inproceedings{NIPS2016_5b69b9cb,
 author = {He, Di and Xia, Yingce and Qin, Tao and Wang, Liwei and Yu, Nenghai and Liu, Tie-Yan and Ma, Wei-Ying},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Dual Learning for Machine Translation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/5b69b9cb83065d403869739ae7f0995e-Paper.pdf},
 volume = {29},
 year = {2016}
}

@ARTICLE{8801910,
  author={Keneshloo, Yaser and Shi, Tian and Ramakrishnan, Naren and Reddy, Chandan K.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Deep Reinforcement Learning for Sequence-to-Sequence Models}, 
  year={2020},
  volume={31},
  number={7},
  pages={2469-2489},
  doi={10.1109/TNNLS.2019.2929141}}

@article{li2017paraphrase,
  title={Paraphrase generation with deep reinforcement learning},
  author={Li, Zichao and Jiang, Xin and Shang, Lifeng and Li, Hang},
  journal={arXiv preprint arXiv:1711.00279},
  year={2017}
}

@inproceedings{NEURIPS2022_b1efde53,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {27730--27744},
 publisher = {Curran Associates, Inc.},
 title = {Training language models to follow instructions with human feedback},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@inproceedings{NEURIPS2022_8636419d,
 author = {Le, Hung and Wang, Yue and Gotmare, Akhilesh Deepak and Savarese, Silvio and Hoi, Steven Chu Hong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {21314--21328},
 publisher = {Curran Associates, Inc.},
 title = {CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8636419dea1aa9fbd25fc4248e702da4-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@article{shojaee2023execution,
  title={Execution-based Code Generation using Deep Reinforcement Learning},
  author={Shojaee, Parshin and Jain, Aneesh and Tipirneni, Sindhu and Reddy, Chandan K},
  journal={arXiv preprint arXiv:2301.13816},
  year={2023}
}
@article{ESNAASHARI2021115446,
title = {Automation of software test data generation using genetic algorithm and reinforcement learning},
journal = {Expert Systems with Applications},
volume = {183},
pages = {115446},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.115446},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421008605},
author = {Mehdi Esnaashari and Amir Hossein Damia},
keywords = {Software test, Structural test, Test data generation, Genetic algorithms, Reinforcement learning}
}

@inproceedings{10.1145/3383313.3412233,
author = {HE, Xu and An, Bo and Li, Yanghua and Chen, Haikai and Wang, Rundong and Wang, Xinrun and Yu, Runsheng and Li, Xin and Wang, Zhirong},
title = {Learning to Collaborate in Multi-Module Recommendation via Multi-Agent Reinforcement Learning without Communication},
year = {2020},
isbn = {9781450375832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383313.3412233},
doi = {10.1145/3383313.3412233},
booktitle = {Proceedings of the 14th ACM Conference on Recommender Systems},
pages = {210–219},
numpages = {10},
keywords = {Reinforcement learning},
location = {Virtual Event, Brazil},
series = {RecSys '20}
}

@inproceedings{10.1145/3109859.3109914,
author = {Zhang, Yang and Zhang, Chenwei and Liu, Xiaozhong},
title = {Dynamic Scholarly Collaborator Recommendation via Competitive Multi-Agent Reinforcement Learning},
year = {2017},
isbn = {9781450346528},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3109859.3109914},
doi = {10.1145/3109859.3109914},
pages = {331–335},
numpages = {5},
keywords = {multi-agent, reinforcement learning, collaborator recommendation, competition, dynamic},
location = {Como, Italy},
series = {RecSys '17}
}

@inproceedings{10.1145/3331184.3331237,
author = {Gui, Tao and Liu, Peng and Zhang, Qi and Zhu, Liang and Peng, Minlong and Zhou, Yunhua and Huang, Xuanjing},
title = {Mention Recommendation in Twitter with Cooperative Multi-Agent Reinforcement Learning},
year = {2019},
isbn = {9781450361729},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3331184.3331237},
doi = {10.1145/3331184.3331237},
pages = {535–544},
numpages = {10},
keywords = {mention recommendation, social medias, reinforcement learning},
location = {Paris, France},
series = {SIGIR'19}
}

@inproceedings{10.1145/3269206.3272021,
author = {Jin, Junqi and Song, Chengru and Li, Han and Gai, Kun and Wang, Jun and Zhang, Weinan},
title = {Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising},
year = {2018},
isbn = {9781450360142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3269206.3272021},
doi = {10.1145/3269206.3272021},
booktitle = {Proceedings of the 27th ACM International Conference on Information and Knowledge Management},
pages = {2193–2201},
numpages = {9},
keywords = {real-time bidding, multi-agent reinforcement learning, display advertising, bid optimization},
location = {Torino, Italy},
series = {CIKM '18}
}

@INPROCEEDINGS{10016386,
  author={Yinggang, Li and Xiangrong, Tong},
  booktitle={2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)}, 
  title={Social Recommendation System Based on Multi-agent Deep Reinforcement Learning}, 
  year={2022},
  volume={},
  number={},
  pages={371-377},
  doi={10.1109/CCIS57298.2022.10016386}}

@inproceedings{DIAL,
 author = {Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf},
 volume = {29},
 year = {2016}
}
@article{MADDPG-M,
  title={Multi-agent deep reinforcement learning with extremely noisy observations},
  author={Kilinc, Ozsel and Montana, Giovanni},
  journal={arXiv preprint arXiv:1812.00922},
  year={2018}
}

@article{ETCNet,
  title={Event-triggered multi-agent reinforcement learning with communication under limited-bandwidth constraint},
  author={Hu, Guangzheng and Zhu, Yuanheng and Zhao, Dongbin and Zhao, Mengchen and Hao, Jianye},
  journal={arXiv preprint arXiv:2010.04978},
  year={2020}
}

@article{gupta2021hammer,
  title={Hammer: Multi-level coordination of reinforcement learning agents via learned messaging},
  author={Gupta, Nikunj and Srinivasaraghavan, G and Mohalik, Swarup Kumar and Taylor, Matthew E},
  journal={arXiv preprint arXiv:2102.00824},
  year={2021}
}

@inproceedings{NIPS2016_55b1927f,
 author = {Sukhbaatar, Sainbayar and szlam, arthur and Fergus, Rob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Multiagent Communication with Backpropagation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{peng2017multiagent,
  title={Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games},
  author={Peng, Peng and Wen, Ying and Yang, Yaodong and Yuan, Quan and Tang, Zhenkun and Long, Haitao and Wang, Jun},
  journal={arXiv preprint arXiv:1703.10069},
  year={2017}
}

@inproceedings{
Jiang2020Graph,
title={Graph Convolutional Reinforcement Learning},
author={Jiechuan Jiang and Chen Dun and Tiejun Huang and Zongqing Lu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HkxdQkSYDB}
}


@InProceedings{pmlr-v119-wang20i,
  title = 	 {Learning Efficient Multi-agent Communication: An Information Bottleneck Approach},
  author =       {Wang, Rundong and He, Xu and Yu, Runsheng and Qiu, Wei and An, Bo and Rabinovich, Zinovi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9908--9918},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wang20i/wang20i.pdf},
  url = 	 {https://proceedings.mlr.press/v119/wang20i.html}
}

@inproceedings{10.5555/3463952.3464010,
author = {Du, Yali and Liu, Bo and Moens, Vincent and Liu, Ziqi and Ren, Zhicheng and Wang, Jun and Chen, Xu and Zhang, Haifeng},
title = {Learning Correlated Communication Topology in Multi-Agent Reinforcement Learning},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {456–464},
numpages = {9},
keywords = {multi-agent systems, reinforcement learning, communication topology},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@inproceedings{10.5555/3463952.3464065,
author = {Niu, Yaru and Paleja, Rohan and Gombolay, Matthew},
title = {Multi-Agent Graph-Attention Communication and Teaming},
year = {2021},
isbn = {9781450383073},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
booktitle = {Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems},
pages = {964–973},
numpages = {10},
keywords = {multi-agent reinforcement learning, graph-based communication, multi-agent communication},
location = {Virtual Event, United Kingdom},
series = {AAMAS '21}
}

@INPROCEEDINGS{9341079,
  author={Freed, Benjamin and James, Rohan and Sartoretti, Guillaume and Choset, Howie},
  booktitle={2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={Sparse Discrete Communication Learning for Multi-Agent Cooperation Through Backpropagation}, 
  year={2020},
  volume={},
  number={},
  pages={7993-7998},
  doi={10.1109/IROS45743.2020.9341079}}

﻿@Article{Khalilpourazari2022,
author={Khalilpourazari, Soheyl
and Hashemi Doulabi, Hossein},
title={Designing a hybrid reinforcement learning based algorithm with application in prediction of the COVID-19 pandemic in Quebec},
journal={Annals of Operations Research},
year={2022},
month={May},
day={01},
volume={312},
number={2},
pages={1261-1305},
issn={1572-9338},
doi={10.1007/s10479-020-03871-7},
url={https://doi.org/10.1007/s10479-020-03871-7}
}

@ARTICLE{10.3389/fpubh.2021.744100,
  
AUTHOR={Kumar, R. Lakshmana and Khan, Firoz and Din, Sadia and Band, Shahab S. and Mosavi, Amir and Ibeke, Ebuka},   
	 
TITLE={Recurrent Neural Network and Reinforcement Learning Model for COVID-19 Prediction},      
	
JOURNAL={Frontiers in Public Health},      
	
VOLUME={9},           
	
YEAR={2021},      
	  
URL={https://www.frontiersin.org/articles/10.3389/fpubh.2021.744100},       
	
DOI={10.3389/fpubh.2021.744100},      
	
ISSN={2296-2565}
}

@INPROCEEDINGS{9551174,
  author={Khalilpourazari, Soheyl and Doulabi, Hossein Hashemi},
  booktitle={2021 IEEE International Conference on Autonomous Systems (ICAS)}, 
  title={Using reinforcement learning to forecast the spread of COVID-19 in France}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/ICAS49788.2021.9551174}}

@article{JALALI2021107675,
title = {An oppositional-Cauchy based GSK evolutionary algorithm with a novel deep ensemble reinforcement learning strategy for COVID-19 diagnosis},
journal = {Applied Soft Computing},
volume = {111},
pages = {107675},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107675},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621005962},
author = {Seyed Mohammad Jafar Jalali and Milad Ahmadian and Sajad Ahmadian and Abbas Khosravi and Mamoun Alazab and Saeid Nahavandi},
keywords = {COVID-19, Deep convolutional neural network, Evolutionary computation, Optimization, Deep reinforcement learning, Image classification}
}

﻿@Article{Zheng2021,
author={Zheng, Hua
and Zhu, Jiahao
and Xie, Wei
and Zhong, Judy},
title={Reinforcement learning assisted oxygen therapy for COVID-19 patients under intensive care},
journal={BMC Medical Informatics and Decision Making},
year={2021},
month={Dec},
day={17},
volume={21},
number={1},
pages={350},
issn={1472-6947},
doi={10.1186/s12911-021-01712-6},
url={https://doi.org/10.1186/s12911-021-01712-6}
}

@ARTICLE{9855449,
  author={Chen, Siying and Liu, Minghui and Deng, Pan and Deng, Jiali and Yuan, Yi and Cheng, Xuan and Xie, Tianshu and Xie, Libo and Zhang, Wei and Gong, Haigang and Wang, Xiaomin and Xu, Lifeng and Pu, Hong and Liu, Ming},
  journal={IEEE Journal of Biomedical and Health Informatics}, 
  title={Reinforcement Learning Based Diagnosis and Prediction for COVID-19 by Optimizing a Mixed Cost Function From CT Images}, 
  year={2022},
  volume={26},
  number={11},
  pages={5344-5354},
  doi={10.1109/JBHI.2022.3197666}}
  
@Article{jpm12020309,
AUTHOR = {Allioui, Hanane and Mohammed, Mazin Abed and Benameur, Narjes and Al-Khateeb, Belal and Abdulkareem, Karrar Hameed and Garcia-Zapirain, Begonya and Damaševičius, Robertas and Maskeliūnas, Rytis},
TITLE = {A Multi-Agent Deep Reinforcement Learning Approach for Enhancement of COVID-19 CT Image Segmentation},
JOURNAL = {Journal of Personalized Medicine},
VOLUME = {12},
YEAR = {2022},
NUMBER = {2},
ARTICLE-NUMBER = {309},
URL = {https://www.mdpi.com/2075-4426/12/2/309},
PubMedID = {35207796},
ISSN = {2075-4426},
DOI = {10.3390/jpm12020309}
}

@InProceedings{Liao_2020_CVPR,
author = {Liao, Xuan and Li, Wenhao and Xu, Qisen and Wang, Xiangfeng and Jin, Bo and Zhang, Xiaoyun and Wang, Yanfeng and Zhang, Ya},
title = {Iteratively-Refined Interactive 3D Medical Image Segmentation With Multi-Agent Reinforcement Learning},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@ARTICLE{9311659,
  author={Ma, Chaofan and Xu, Qisen and Wang, Xiangfeng and Jin, Bo and Zhang, Xiaoyun and Wang, Yanfeng and Zhang, Ya},
  journal={IEEE Transactions on Medical Imaging}, 
  title={Boundary-Aware Supervoxel-Level Iteratively Refined Interactive 3D Image Segmentation With Multi-Agent Reinforcement Learning}, 
  year={2021},
  volume={40},
  number={10},
  pages={2563-2574},
  doi={10.1109/TMI.2020.3048477}}

@inproceedings{zheng2021multi,
  title={Multi-agent reinforcement learning for prostate localization based on multi-scale image representation},
  author={Zheng, Chenyang and Si, Xiangyu and Sun, Lei and Chen, Zhang and Yu, Linghao and Tian, Zhiqiang},
  booktitle={International Symposium on Artificial Intelligence and Robotics 2021},
  volume={11884},
  pages={487--494},
  year={2021},
  organization={SPIE}
}

@InProceedings{10.1007/978-3-030-66843-3_18,
author="Leroy, Guy
and Rueckert, Daniel
and Alansary, Amir",
editor="Kia, Seyed Mostafa
and Mohy-ud-Din, Hassan
and Abdulkadir, Ahmed
and Bass, Cher
and Habes, Mohamad
and Rondina, Jane Maryam
and Tax, Chantal
and Wang, Hongzhi
and Wolfers, Thomas
and Rathore, Saima
and Ingalhalikar, Madhura",
title="Communicative Reinforcement Learning Agents for Landmark Detection in Brain Images",
booktitle="Machine Learning in Clinical Neuroimaging and Radiogenomics in Neuro-oncology",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="177--186",
isbn="978-3-030-66843-3"
}

@InProceedings{10.1007/978-3-030-32251-9_29,
author="Vlontzos, Athanasios
and Alansary, Amir
and Kamnitsas, Konstantinos
and Rueckert, Daniel
and Kainz, Bernhard",
editor="Shen, Dinggang
and Liu, Tianming
and Peters, Terry M.
and Staib, Lawrence H.
and Essert, Caroline
and Zhou, Sean
and Yap, Pew-Thian
and Khan, Ali",
title="Multiple Landmark Detection Using Multi-agent Reinforcement Learning",
booktitle="Medical Image Computing and Computer Assisted Intervention -- MICCAI 2019",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="262--270",
isbn="978-3-030-32251-9"
}

@InProceedings{10.1007/978-3-030-78191-0_59,
author="Kasseroller, Klemens
and Thaler, Franz
and Payer, Christian
and {\v{S}}tern, Darko",
editor="Feragen, Aasa
and Sommer, Stefan
and Schnabel, Julia
and Nielsen, Mads",
title="Collaborative Multi-agent Reinforcement Learning for Landmark Localization Using Continuous Action Space",
booktitle="Information Processing in Medical Imaging",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="767--778",
isbn="978-3-030-78191-0"
}

@INPROCEEDINGS{10010747,
  author={Rajesh, Thota Radha and Rajendran, Surendran},
  booktitle={2022 International Conference on Augmented Intelligence and Sustainable Systems (ICAISS)}, 
  title={Intelligent Multi-Agent Reinforcement Learning Based Disease Prediction and Treatment Recommendation Model}, 
  year={2022},
  volume={},
  number={},
  pages={216-221},
  doi={10.1109/ICAISS55157.2022.10010747}}


@InProceedings{pmlr-v68-ling17a,
  title = 	 {Diagnostic Inferencing via Improving Clinical Concept Extraction with Deep Reinforcement Learning: A Preliminary Study},
  author = 	 {Ling, Yuan and Hasan, Sadid A. and Datla, Vivek and Qadir, Ashequl and Lee, Kathy and Liu, Joey and Farri, Oladimeji},
  booktitle = 	 {Proceedings of the 2nd Machine Learning for Healthcare Conference},
  pages = 	 {271--285},
  year = 	 {2017},
  editor = 	 {Doshi-Velez, Finale and Fackler, Jim and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
  volume = 	 {68},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--19 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v68/ling17a/ling17a.pdf},
  url = 	 {https://proceedings.mlr.press/v68/ling17a.html}
}
@inproceedings{ling-etal-2017-learning,
    title = "Learning to Diagnose: Assimilating Clinical Narratives using Deep Reinforcement Learning",
    author = "Ling, Yuan  and
      Hasan, Sadid A.  and
      Datla, Vivek  and
      Qadir, Ashequl  and
      Lee, Kathy  and
      Liu, Joey  and
      Farri, Oladimeji",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1090",
    pages = "895--905"
}

@inproceedings{10.1117/12.2216550,
author = {Tianshu Chu and Jie Wang and Jiayu Chen},
title = {{An adaptive online learning framework for practical breast cancer diagnosis}},
volume = {9785},
booktitle = {Medical Imaging 2016: Computer-Aided Diagnosis},
editor = {Georgia D. Tourassi and Samuel G. Armato III},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {978524},
keywords = {online learning, reinforcement learning, breast cancer diagnosis, intelligent healthcare system, stream data mining, machine learning},
year = {2016},
doi = {10.1117/12.2216550},
URL = {https://doi.org/10.1117/12.2216550}
}

@inproceedings{tang2016inquire,
  title={Inquire and diagnose: Neural symptom checking ensemble using deep reinforcement learning},
  author={Tang, Kai-Fu and Kao, Hao-Cheng and Chou, Chun-Nan and Chang, Edward Y},
  booktitle={NIPS workshop on deep reinforcement learning},
  year={2016}
}


@InProceedings{a3c,
  title = 	 {Asynchronous Methods for Deep Reinforcement Learning},
  author = 	 {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1928--1937},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/mniha16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/mniha16.html},
  abstract = 	 {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.}
}


@InProceedings{maac,
  title = 	 {Actor-Attention-Critic for Multi-Agent Reinforcement Learning},
  author =       {Iqbal, Shariq and Sha, Fei},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2961--2970},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/iqbal19a/iqbal19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/iqbal19a.html}
}

@article{seo2021feedforward,
  title={Feedforward beta control in the KSTAR tokamak by deep reinforcement learning},
  author={Seo, Jaemin and Na, Y-S and Kim, B and Lee, CY and Park, MS and Park, SJ and Lee, YH},
  journal={Nuclear Fusion},
  volume={61},
  number={10},
  pages={106010},
  year={2021},
  publisher={IOP Publishing}
}

@INPROCEEDINGS{9966863,
  author={Miao, Qinghai and Huang, Min and Lv, Yisheng and Wang, Fei-Yue},
  booktitle={2022 Australian \& New Zealand Control Conference (ANZCC)}, 
  title={Parallel Learning between Science for AI and AI for Science: A Brief Overview and Perspective}, 
  year={2022},
  volume={},
  number={},
  pages={171-175},
  doi={10.1109/ANZCC56036.2022.9966863}}

﻿@Article{Degrave2022,
author={Degrave, Jonas
and Felici, Federico
and Buchli, Jonas
and Neunert, Michael
and Tracey, Brendan
and Carpanese, Francesco
and Ewalds, Timo
and Hafner, Roland
and Abdolmaleki, Abbas
and de las Casas, Diego
and Donner, Craig
and Fritz, Leslie
and Galperti, Cristian
and Huber, Andrea
and Keeling, James
and Tsimpoukelli, Maria
and Kay, Jackie
and Merle, Antoine
and Moret, Jean-Marc
and Noury, Seb
and Pesamosca, Federico
and Pfau, David
and Sauter, Olivier
and Sommariva, Cristian
and Coda, Stefano
and Duval, Basil
and Fasoli, Ambrogio
and Kohli, Pushmeet
and Kavukcuoglu, Koray
and Hassabis, Demis
and Riedmiller, Martin},
title={Magnetic control of tokamak plasmas through deep reinforcement learning},
journal={Nature},
year={2022},
month={Feb},
day={01},
volume={602},
number={7897},
pages={414-419},
issn={1476-4687},
doi={10.1038/s41586-021-04301-9},
url={https://doi.org/10.1038/s41586-021-04301-9}
}

﻿@Article{Bae2022,
author={Bae, H. Jane
and Koumoutsakos, Petros},
title={Scientific multi-agent reinforcement learning for wall-models of turbulent flows},
journal={Nature Communications},
year={2022},
month={Mar},
day={17},
volume={13},
number={1},
pages={1443},
issn={2041-1723},
doi={10.1038/s41467-022-28957-7},
url={https://doi.org/10.1038/s41467-022-28957-7}
}



