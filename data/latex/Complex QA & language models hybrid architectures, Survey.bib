@article{{auth:lower}{year},
  title = {A {{Deep Learning Approach}} for {{Ontology Enrichment}} from {{Unstructured Text}}},
  author = {Sanagavarapu, Lalit Mohan and Iyer, Vivek and Reddy, Raghu},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.08554 [cs]},
  eprint = {2112.08554},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Information Security in the cyber world is a major cause for concern, with significant increase in the number of attack surfaces. Existing information on vulnerabilities, attacks, controls, and advisories available on web provides an opportunity to represent knowledge and perform security analytics to mitigate some of the concerns. Representing security knowledge in the form of ontology facilitates anomaly detection, threat intelligence, reasoning and relevance attribution of attacks, and many more. This necessitates dynamic and automated enrichment of information security ontologies. However, existing ontology enrichment algorithms based on natural language processing and ML models have issues with contextual extraction of concepts in words, phrases and sentences. This motivates the need for sequential Deep Learning architectures that traverse through dependency paths in text and extract embedded vulnerabilities, threats, controls, products and other security related concepts and instances from learned path representations. In the proposed approach, Bidirectional LSTMs trained on a large DBpedia dataset and Wikipedia corpus of 2.8 GB along with Universal Sentence Encoder is deployed to enrich ISO 27001 [1] based information security ontology. The model is trained and tested on an high performance computing (HPC) environment to handle Wiki text dimensionality. The approach yielded a test accuracy of over 80\% when tested with knocked out concepts from ontology and web page instances to validate the robustness.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/K2D2SGPN/Sanagavarapu et al. - 2021 - A Deep Learning Approach for Ontology Enrichment f.pdf}
}

@misc{AAAIAssociationAdvancement,
  title = {{{AAAI}} | {{Association}} for the {{Advancement}} of {{Artificial Intelligence}}},
  journal = {AAAI},
  urldate = {2023-02-15},
  abstract = {AAAI advances the scientific understanding of the mechanisms underlying thought and intelligent behavior and their embodiment in machines.},
  howpublished = {https://aaai.org/},
  langid = {american},
  file = {/home/xav/Zotero/storage/TG8HPY2T/aaai.org.html}
}

@incollection{abasoloComponentsCaseBasedReasoning2002,
  title = {Components for {{Case-Based Reasoning Systems}}},
  booktitle = {Topics in {{Artificial Intelligence}}},
  author = {Ab{\'a}solo, Chema and Plaza, Enric and Arcos, Josep-Llu{\'i}s},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Escrig, M. Teresa and Toledo, Francisco and Golobardes, Elisabet},
  year = {2002},
  volume = {2504},
  pages = {1--16},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-36079-4_1},
  urldate = {2022-09-01},
  isbn = {978-3-540-00011-2 978-3-540-36079-7},
  file = {/home/xav/Zotero/storage/LSTS2QX7/Ab√°solo et al. - 2002 - Components for Case-Based Reasoning Systems.pdf}
}

@misc{AbstractsBERTRelatedPapers,
  title = {Abstracts of {{BERT-Related Papers}}},
  urldate = {2022-08-30},
  abstract = {Survey paper     Downstream task         QA, MC, Dialogue         Slot filling and Intent Detection         Analysis         Word segmentation, parsing, NER         Pronoun/coreference resolution         Word sense disambiguation         Sentiment analysis         Relation extraction         Knowledge base         Text classification         WSC, WNLI, NLI         Commonsense         Extractive summarization         Grammatical error correction         IR     Generation     Quality evaluator     Modification (multi-task, masking strategy, etc.)         Tokenization         Prompt     Sentence embedding     Transformer variants     Probe     Inside BERT     Multi-lingual     Other than English models     Domain specific     Multi-modal     Model compression     Misc.},
  howpublished = {https://ayaka14732.github.io/bert-related-paper-abstracts/},
  file = {/home/xav/Zotero/storage/5YMZ8K6T/Abstracts of BERT-Related Papers.pdf}
}

@misc{achiepoSimilarityMeasureCaseBased2015,
  title = {Similarity {{Measure}} in the {{Case-Based Reasoning Systems}} for {{Medical Diagnostics}} in {{Traditional Medicine}}},
  author = {Achiepo, O. Y. and N'guessan, Behou G{\'e}rard and Marcellin, K.},
  year = {2015},
  urldate = {2022-05-07},
  abstract = {A measure of similarity is proposed for the development of CBR systems automatically diagnose diseases appropriate to the field of traditional medicine. The Case-Based Reasoning (CBR) is a problem solving paradigm based on the reuse of past experiences stored in a cases base. The CBR has been used in many industrial systems to solve problems in various fields including medical diagnosis. In CBR systems, the concept of similarity is very important because it is the basis for the entire system. However, the effectiveness of similarity measures used depends on the problem being addressed. In this paper, we propose a measure of similarity for the development of CBR systems automatically diagnose diseases appropriate to the field of traditional medicine},
  howpublished = {https://www.semanticscholar.org/paper/Similarity-Measure-in-the-Case-Based-Reasoning-for-Achiepo-N\%27GUESSAN/eb75c8b31ebd5112815f1182fd7c615f660c8567},
  langid = {english},
  file = {/home/xav/Zotero/storage/DZFMAJEM/eb75c8b31ebd5112815f1182fd7c615f660c8567.html}
}

@misc{adatraoSurveyConversationalSearch2022,
  title = {A {{Survey}} on {{Conversational Search}} and {{Applications}} in {{Biomedicine}}},
  author = {Adatrao, Naga Sai Krishna and Gadireddy, Gowtham Reddy and Noh, Jiho},
  year = {2022},
  month = nov,
  number = {arXiv:2211.15328},
  eprint = {arXiv:2211.15328},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {This paper aims to provide a radical rundown on Conversation Search (ConvSearch), an approach to enhance the information retrieval method where users engage in a dialogue for the informationseeking tasks. In this survey, we predominantly focused on the human interactive characteristics of the ConvSearch systems, highlighting the operations of the action modules, likely the Retrieval system, Question-Answering, and Recommender system. We labeled various ConvSearch research problems in knowledge bases, natural language processing, and dialogue management systems along with the action modules. We further categorized the framework to ConvSearch and the application is directed towards biomedical and healthcare fields for the utilization of clinical social technology. Finally, we conclude by talking through the challenges and issues of ConvSearch, particularly in Bio-Medicine. Our main aim is to provide an integrated and unified vision of the ConvSearch components from different fields, which benefit the informationseeking process in healthcare systems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/EH8LTKHX/Adatrao et al. - 2022 - A Survey on Conversational Search and Applications.pdf}
}

@misc{adolphsDecodingNeuralRetriever2022,
  title = {Decoding a {{Neural Retriever}}'s {{Latent Space}} for {{Query Suggestion}}},
  author = {Adolphs, Leonard and Huebscher, Michelle Chen and Buck, Christian and Girgin, Sertan and Bachem, Olivier and Ciaramita, Massimiliano and Hofmann, Thomas},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12084},
  eprint = {arXiv:2210.12084},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.12084},
  urldate = {2022-11-23},
  abstract = {Neural retrieval models have superseded classic bag-of-words methods such as BM25 as the retrieval framework of choice. However, neural systems lack the interpretability of bag-of-words models; it is not trivial to connect a query change to a change in the latent space that ultimately determines the retrieval results. To shed light on this embedding space, we learn a "query decoder" that, given a latent representation of a neural search engine, generates the corresponding query. We show that it is possible to decode a meaningful query from its latent representation and, when moving in the right direction in latent space, to decode a query that retrieves the relevant paragraph. In particular, the query decoder can be useful to understand "what should have been asked" to retrieve a particular paragraph from the collection. We employ the query decoder to generate a large synthetic dataset of query reformulations for MSMarco, leading to improved retrieval performance. On this data, we train a pseudo-relevance feedback (PRF) T5 model for the application of query suggestion that outperforms both query reformulation and PRF information retrieval baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/B6CNDMSS/Adolphs et al. - 2022 - Decoding a Neural Retriever's Latent Space for Que.pdf}
}

@article{agbaegbuOntologiesCloudComputing2021,
  title = {Ontologies in {{Cloud Computing}}\textemdash{{Review}} and {{Future Directions}}},
  author = {Agbaegbu, JohnBosco and Arogundade, Oluwasefunmi Tale and Misra, Sanjay and Dama{\v s}evi{\v c}ius, Robertas},
  year = {2021},
  month = dec,
  journal = {Future Internet},
  volume = {13},
  number = {12},
  pages = {302},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1999-5903},
  doi = {10.3390/fi13120302},
  urldate = {2022-05-04},
  abstract = {Cloud computing as a technology has the capacity to enhance cooperation, scalability, accessibility, and offers discount prospects using improved and effective computing, and this capability helps organizations to stay focused. Ontologies are used to model knowledge. Once knowledge is modeled, knowledge management systems can be used to search, match, visualize knowledge, and also infer new knowledge. Ontologies use semantic analysis to define information within an environment with interconnecting relationships between heterogeneous sets. This paper aims to provide a comprehensive review of the existing literature on ontology in cloud computing and defines the state of the art. We applied the systematic literature review (SLR) approach and identified 400 articles; 58 of the articles were selected after further selection based on set selection criteria, and 35 articles were considered relevant to the study. The study shows that four predominant areas of cloud computing\textemdash cloud security, cloud interoperability, cloud resources and service description, and cloud services discovery and selection\textemdash have attracted the attention of researchers as dominant areas where cloud ontologies have made great impact. The proposed methods in the literature applied 30 ontologies in the cloud domain, and five of the methods are still practiced in the legacy computing environment. From the analysis, it was found that several challenges exist, including those related to the application of ontologies to enhance business operations in the cloud and multi-cloud. Based on this review, the study summarizes some unresolved challenges and possible future directions for cloud ontology researchers.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Business process,cloud computing,compliance,multi-cloud,ontology},
  file = {/home/xav/Zotero/storage/INP5XZ7X/Agbaegbu et al. - 2021 - Ontologies in Cloud Computing‚ÄîReview and Future Di.pdf;/home/xav/Zotero/storage/ZFN9L9Q5/302.html}
}

@misc{aGenerateQuestionsAutomatically2021,
  title = {Generate {{Questions Automatically}} for {{Faster Annotation Workflows}}},
  author = {A, Andrey},
  year = {2021},
  month = nov,
  journal = {deepset-ai},
  urldate = {2022-05-04},
  abstract = {Let Transformer models do the annotation work for you with Haystack's new Question Generator},
  langid = {english},
  file = {/home/xav/Zotero/storage/IH342B9M/generate-questions-automatically-for-faster-annotation-workflows-f9df4e14ec82.html}
}

@article{alohalyAutomatedExtractionAttributes2019,
  title = {Automated Extraction of Attributes from Natural Language Attribute-Based Access Control ({{ABAC}}) {{Policies}}},
  author = {Alohaly, Manar and Takabi, H. and Blanco, Eduardo},
  year = {2019},
  journal = {Cybersecur.},
  doi = {10.1186/s42400-018-0019-2},
  abstract = {This paper decouple the primary contributions of this work into developing a practical framework to extract authorization attributes of hierarchical ABAC system from natural language artifacts, and generating a set of realistic synthetic natural language access control policies (NLACPs) to evaluate the proposed framework. The National Institute of Standards and Technology (NIST) has identified natural language policies as the preferred expression of policy and implicitly called for an automated translation of ABAC natural language access control policy (NLACP) to a machine-readable form. To study the automation process, we consider the hierarchical ABAC model as our reference model since it better reflects the requirements of real-world organizations. Therefore, this paper focuses on the questions of: how can we automatically infer the hierarchical structure of an ABAC model given NLACPs; and, how can we extract and define the set of authorization attributes based on the resulting structure. To address these questions, we propose an approach built upon recent advancements in natural language processing and machine learning techniques. For such a solution, the lack of appropriate data often poses a bottleneck. Therefore, we decouple the primary contributions of this work into: (1) developing a practical framework to extract authorization attributes of hierarchical ABAC system from natural language artifacts, and (2) generating a set of realistic synthetic natural language access control policies (NLACPs) to evaluate the proposed framework. Our experimental results are promising as we achieved - in average - an F1-score of 0.96 when extracting attributes values of subjects, and 0.91 when extracting the values of objects' attributes from natural language access control policies.},
  file = {/home/xav/Zotero/storage/A2ZA2TNY/s42400-018-0019-2.pdf;/home/xav/Zotero/storage/TYE4TLTQ/Alohaly et al. - 2019 - Automated extraction of attributes from natural la.pdf}
}

@article{alonNeuroSymbolicLanguageModeling2022,
  title = {Neuro-{{Symbolic Language Modeling}} with {{Automaton-augmented Retrieval}}},
  author = {Alon, Uri and Xu, Frank F. and He, Junxian and Sengupta, Sudipta and Roth, Dan and Neubig, Graham},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.12431 [cs]},
  eprint = {2201.12431},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Retrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RETOMATON \textendash{} retrieval automaton \textendash which approximates the datastore search, based on (1) clustering of entries into ``states'', and (2) state transitions from previous entries. This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RETOMATON can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity, or alternatively saves up to 83\% of the nearest neighbor searches over kNN-LM (Khandelwal et al., 2020), without hurting perplexity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/3E8YC3XR/Alon et al. - 2022 - Neuro-Symbolic Language Modeling with Automaton-au.pdf}
}

@misc{alpaca,
  title = {Stanford Alpaca: {{An}} Instruction-Following {{LLaMA}} Model},
  author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and {Tatsunori B. Hashimoto}},
  year = {2023},
  publisher = {{GitHub}}
}

@misc{alshomaryArgumentUnderminingCounterArgument2021,
  title = {Argument {{Undermining}}: {{Counter-Argument Generation}} by {{Attacking Weak Premises}}},
  shorttitle = {Argument {{Undermining}}},
  author = {Alshomary, Milad and Syed, Shahbaz and Dhar, Arkajit and Potthast, Martin and Wachsmuth, Henning},
  year = {2021},
  month = may,
  number = {arXiv:2105.11752},
  eprint = {arXiv:2105.11752},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.11752},
  urldate = {2023-01-03},
  abstract = {Text generation has received a lot of attention in computational argumentation research as of recent. A particularly challenging task is the generation of counter-arguments. So far, approaches primarily focus on rebutting a given conclusion, yet other ways to counter an argument exist. In this work, we go beyond previous research by exploring argument undermining, that is, countering an argument by attacking one of its premises. We hypothesize that identifying the argument's weak premises is key to effective countering. Accordingly, we propose a pipeline approach that first assesses the premises' strength and then generates a counter-argument targeting the weak ones. On the one hand, both manual and automatic evaluation proves the importance of identifying weak premises in counter-argument generation. On the other hand, when considering correctness and content richness, human annotators favored our approach over state-of-the-art counter-argument generation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/3JM9JE4I/Alshomary et al. - 2021 - Argument Undermining Counter-Argument Generation .pdf;/home/xav/Zotero/storage/9SXL5YNG/2105.html}
}

@article{amjadApprocheOntologiePour,
  title = {{Approche ontologie pour l'int\'egration des entreprises distribu\'ees}},
  author = {Amjad, Fahd},
  pages = {173},
  langid = {french},
  file = {/home/xav/Zotero/storage/K9BZGMLG/Amjad - Approche ontologie pour l'int√©gration des entrepri.pdf}
}

@misc{amplayoQueryRefinementPrompts2022,
  title = {Query {{Refinement Prompts}} for {{Closed-Book Long-Form Question Answering}}},
  author = {Amplayo, Reinald Kim and Webster, Kellie and Collins, Michael and Das, Dipanjan and Narayan, Shashi},
  year = {2022},
  month = oct,
  number = {arXiv:2210.17525},
  eprint = {arXiv:2210.17525},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.17525},
  urldate = {2023-01-16},
  abstract = {Large language models (LLMs) have been shown to perform well in answering questions and in producing long-form texts, both in few-shot closed-book settings. While the former can be validated using well-known evaluation metrics, the latter is difficult to evaluate. We resolve the difficulties to evaluate long-form output by doing both tasks at once -- to do question answering that requires long-form answers. Such questions tend to be multifaceted, i.e., they may have ambiguities and/or require information from multiple sources. To this end, we define query refinement prompts that encourage LLMs to explicitly express the multifacetedness in questions and generate long-form answers covering multiple facets of the question. Our experiments on two long-form question answering datasets, ASQA and AQuAMuSe, show that using our prompts allows us to outperform fully finetuned models in the closed book setting, as well as achieve results comparable to retrieve-then-generate open-book models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/UKBC7BRD/Amplayo et al. - 2022 - Query Refinement Prompts for Closed-Book Long-Form.pdf;/home/xav/Zotero/storage/8V6CCLW2/2210.html}
}

@misc{andreasNeuralModuleNetworks2017,
  title = {Neural {{Module Networks}}},
  author = {Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Klein, Dan},
  year = {2017},
  month = jul,
  number = {arXiv:1511.02799},
  eprint = {arXiv:1511.02799},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.02799},
  urldate = {2023-02-10},
  abstract = {Visual question answering is fundamentally compositional in nature---a question like "where is the dog?" shares substructure with questions like "what color is the dog?" and "where is the cat?" This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning *neural module networks*, which compose collections of jointly-trained neural "modules" into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/xav/Zotero/storage/IJW8ZKWU/Andreas et al. - 2017 - Neural Module Networks.pdf;/home/xav/Zotero/storage/Q4CYIZGV/1511.html}
}

@misc{anEnhancingScientificPapers2021,
  title = {Enhancing {{Scientific Papers Summarization}} with {{Citation Graph}}},
  author = {An, Chenxin and Zhong, Ming and Chen, Yiran and Wang, Danqing and Qiu, Xipeng and Huang, Xuanjing},
  year = {2021},
  month = apr,
  number = {arXiv:2104.03057},
  eprint = {arXiv:2104.03057},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.03057},
  urldate = {2022-11-21},
  abstract = {Previous work for text summarization in scientific domain mainly focused on the content of the input document, but seldom considering its citation network. However, scientific papers are full of uncommon domain-specific terms, making it almost impossible for the model to understand its true meaning without the help of the relevant research community. In this paper, we redefine the task of scientific papers summarization by utilizing their citation graph and propose a citation graph-based summarization model CGSum which can incorporate the information of both the source paper and its references. In addition, we construct a novel scientific papers summarization dataset Semantic Scholar Network (SSN) which contains 141K research papers in different domains and 661K citation relationships. The entire dataset constitutes a large connected citation graph. Extensive experiments show that our model can achieve competitive performance when compared with the pretrained models even with a simple architecture. The results also indicates the citation graph is crucial to better understand the content of papers and generate high-quality summaries.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/SMRMCM9T/An et al. - 2021 - Enhancing Scientific Papers Summarization with Cit.pdf}
}

@article{annaneOntologieProcessusMetier2019,
  title = {{Une Ontologie des Processus M\'etier (BBO) pour guider un Agent Virtuel}},
  author = {Annane, Amina and {Aussenac-Gilles}, Nathalie and Kamel, Mouna},
  year = {2019},
  pages = {17},
  langid = {french},
  file = {/home/xav/Zotero/storage/IUP6Y6M5/Annane et al. - 2019 - Une Ontologie des Processus M√©tier (BBO) pour guid.pdf}
}

@inproceedings{anonymousTaskAmbiguityHumans2022,
  title = {Task {{Ambiguity}} in {{Humans}} and {{Language Models}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {{Anonymous}},
  year = {2022},
  month = nov,
  urldate = {2022-12-21},
  abstract = {Language models have recently achieved strong performance across a wide range of NLP benchmarks. However, real world tasks are often poorly specified, and agents must deduce the intended behavior from a combination of context, instructions, and examples. We investigate how both humans and models behave in the face of such task ambiguity by proposing AmbiBench, a new benchmark of six ambiguously-specified classification tasks. We evaluate humans and models on AmbiBench by seeing how well they identify the intended task using 1) instructions with varying degrees of ambiguity, and 2) different numbers of labeled examples. We find that the combination of model scaling (to 175B parameters) and reinforcement learning from human feedback (RLHF) enables models to approach or exceed the accuracy of human participants across tasks, but that either one of these alone is not sufficient. In addition, we show how to dramatically improve the accuracy of language models trained without RLHF by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.},
  langid = {english},
  file = {/home/xav/Zotero/storage/WAJ62TML/Anonymous - 2022 - Task Ambiguity in Humans and Language Models.pdf}
}

@misc{AnseriniBERTSQuADSemantic,
  title = {Anserini+{{BERT-SQuAD}} for {{Semantic Corpus Search}}},
  urldate = {2022-06-11},
  abstract = {The objective of this project is to integrate the state of the art Natural Language Processing (NLP) techniques to create a method to ask a database a question and have it return sensible results that answer that question. In addition, we can apply modern document analysis methods to exctract tabular, figure, and numeric data when possible. Eventually, this could be built into a scientific search system that can reduce a body of knowledge to a small number of records and then extract data into machine readable formats to support further modeling and data analysis.},
  howpublished = {https://kaggle.com/dirktheeng/anserini-bert-squad-for-semantic-corpus-search},
  langid = {english},
  file = {/home/xav/Zotero/storage/BFWKZ2Y3/anserini-bert-squad-for-semantic-corpus-search.html}
}

@misc{arakelyanNS3NeuroSymbolicSemantic2022,
  title = {{{NS3}}: {{Neuro-Symbolic Semantic Code Search}}},
  shorttitle = {{{NS3}}},
  author = {Arakelyan, Shushan and Hakhverdyan, Anna and Allamanis, Miltiadis and Garcia, Luis and Hauser, Christophe and Ren, Xiang},
  year = {2022},
  month = nov,
  number = {arXiv:2205.10674},
  eprint = {arXiv:2205.10674},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10674},
  urldate = {2022-11-21},
  abstract = {Semantic code search is the task of retrieving a code snippet given a textual description of its functionality. Recent work has been focused on using similarity metrics between neural embeddings of text and code. However, current language models are known to struggle with longer, compositional text, and multi-step reasoning. To overcome this limitation, we propose supplementing the query sentence with a layout of its semantic structure. The semantic layout is used to break down the final reasoning decision into a series of lower-level decisions. We use a Neural Module Network architecture to implement this idea. We compare our model - NS3 (Neuro-Symbolic Semantic Search) - to a number of baselines, including state-of-the-art semantic code retrieval methods, and evaluate on two datasets - CodeSearchNet and Code Search and Question Answering. We demonstrate that our approach results in more precise code retrieval, and we study the effectiveness of our modular design when handling compositional queries.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/R38KMVG7/2205.10674.pdf}
}

@article{arnoldQuestionansweringSystemAircraft2020,
  title = {A Question-Answering System for Aircraft Pilots' Documentation},
  author = {Arnold, Alexandre and Dupont, G{\'e}rard and Furger, F{\'e}lix and Kobus, Catherine and Lancelot, Fran{\c c}ois},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.13284 [cs]},
  eprint = {2011.13284},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {The aerospace industry relies on massive collections of complex and technical documents covering system descriptions, manuals or procedures. This paper presents a question answering (QA) system that would help aircraft pilots access information in this documentation by naturally interacting with the system and asking questions in natural language. After describing each module of the dialog system, we present a multi-task based approach for the QA module which enables performance improvement on a Flight Crew Operating Manual (FCOM) dataset. A method to combine scores from the retriever and the QA modules is also presented.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/4KIP67C2/Arnold et al. - 2020 - A question-answering system for aircraft pilots' d.pdf;/home/xav/Zotero/storage/LDRPI6NV/2011.html}
}

@inproceedings{artetxeCrosslingualTransferabilityMonolingual2020,
  title = {On the {{Cross-lingual Transferability}} of {{Monolingual Representations}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
  year = {2020},
  eprint = {1910.11856},
  primaryclass = {cs},
  pages = {4623--4637},
  doi = {10.18653/v1/2020.acl-main.421},
  urldate = {2023-03-07},
  abstract = {State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/Z8YPWIIZ/Artetxe et al. - 2020 - On the Cross-lingual Transferability of Monolingua.pdf;/home/xav/Zotero/storage/94CZ8VMQ/1910.html}
}

@misc{ArtificialIntelligenceStructures2008,
  title = {Artificial {{Intelligence}}: {{Structures}} and {{Strategies}} for {{Complex Problem Solving}} - {{AbeBooks}} - {{Luger}}, {{George}}: 0321545893},
  shorttitle = {9780321545893},
  year = {2008},
  urldate = {2022-09-20},
  abstract = {AbeBooks.com: Artificial Intelligence: Structures and Strategies for Complex Problem Solving (9780321545893) by Luger, George and a great selection of similar New, Used and Collectible Books available now at great prices.},
  howpublished = {https://www.abebooks.com/9780321545893/Artificial-Intelligence-Structures-Strategies-Complex-0321545893/plp},
  langid = {english},
  file = {/home/xav/Zotero/storage/W5XV3F7G/9780321545893 Artificial Intelligence Structures.pdf;/home/xav/Zotero/storage/AU5G9RZK/9780321545893 Artificial Intelligence Structures.pdf;/home/xav/Zotero/storage/S9I9787H/plp.html}
}

@misc{ArXivTitlePrediction,
  title = {{{ArXiv Title Prediction}} from {{Abstract}} Using {{BART}}},
  urldate = {2022-06-11},
  abstract = {In this notebook, we use BART Transformer model to perform title generation from abstracts. BART is a sequence-to-sequence model where both the input and targets are text sequences. BART is commonly used for text summarization. In our case, we would want to summarize titles from abstracts.},
  howpublished = {https://kaggle.com/balraj98/arxiv-title-prediction-from-abstract-using-bart},
  langid = {english},
  file = {/home/xav/Zotero/storage/Z3RXRRUB/arxiv-title-prediction-from-abstract-using-bart.html}
}

@article{asaiTaskawareRetrievalInstructions2022,
  title = {Task-Aware {{Retrieval}} with {{Instructions}}},
  author = {Asai, Akari and Schick, Timo and Lewis, Patrick and Chen, Xilun and Izacard, Gautier and Riedel, Sebastian and Hajishirzi, Hannaneh and Yih, Wen-tau},
  year = {2022},
  journal = {ArXiv},
  urldate = {2022-12-19},
  abstract = {TART shows strong capabilities to adapt to a new task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We study the problem of retrieval with instructions , where users of a retrieval system explicitly describe their intent along with their queries, making the system task-aware . We aim to develop a general-purpose task-aware retrieval systems using multi-task instruction tuning that can follow human-written instructions to find the best documents for a given query. To this end, we introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, and present TART, a multi-task retrieval system trained on the diverse retrieval tasks with instructions. TART shows strong capabilities to adapt to a new task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup to better re-flect real-world scenarios, pooling diverse documents and tasks. In this setup, TART signif-icantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions. year in developed countries. Behavioural risk factors identified in epidemiological studies include prone and side positions for infant sleep, smoke exposure, soft bedding and sleep surfaces, and overheating. (Scientific},
  langid = {english},
  file = {/home/xav/Zotero/storage/L7WIEWCZ/Asai et al. - 2022 - Task-aware Retrieval with Instructions.pdf}
}

@misc{askellGeneralLanguageAssistant2021,
  title = {A {{General Language Assistant}} as a {{Laboratory}} for {{Alignment}}},
  author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  year = {2021},
  month = dec,
  number = {arXiv:2112.00861},
  eprint = {arXiv:2112.00861},
  publisher = {{arXiv}},
  urldate = {2023-02-08},
  abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/GPXEV5JY/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf}
}

@misc{bachPromptSourceIntegratedDevelopment2022,
  title = {{{PromptSource}}: {{An Integrated Development Environment}} and {{Repository}} for {{Natural Language Prompts}}},
  shorttitle = {{{PromptSource}}},
  author = {Bach, Stephen H. and Sanh, Victor and Yong, Zheng-Xin and Webson, Albert and Raffel, Colin and Nayak, Nihal V. and Sharma, Abheesht and Kim, Taewoon and Bari, M. Saiful and Fevry, Thibault and Alyafeai, Zaid and Dey, Manan and Santilli, Andrea and Sun, Zhiqing and {Ben-David}, Srulik and Xu, Canwen and Chhablani, Gunjan and Wang, Han and Fries, Jason Alan and {Al-shaibani}, Maged S. and Sharma, Shanya and Thakker, Urmish and Almubarak, Khalid and Tang, Xiangru and Radev, Dragomir and Jiang, Mike Tian-Jian and Rush, Alexander M.},
  year = {2022},
  month = mar,
  number = {arXiv:2202.01279},
  eprint = {2202.01279},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-30},
  abstract = {PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/LTWTZLRB/Bach et al. - 2022 - PromptSource An Integrated Development Environmen.pdf}
}

@misc{baiConstitutionalAIHarmlessness2022,
  title = {Constitutional {{AI}}: {{Harmlessness}} from {{AI Feedback}}},
  shorttitle = {Constitutional {{AI}}},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and {Tran-Johnson}, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and {Telleen-Lawton}, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and {Hatfield-Dodds}, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08073},
  eprint = {arXiv:2212.08073},
  publisher = {{arXiv}},
  urldate = {2023-01-21},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through selfimprovement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as `Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use `RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but nonevasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/MRI6Z2X3/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf}
}

@misc{baiCrossLingualAbstractiveSummarization2021,
  title = {Cross-{{Lingual Abstractive Summarization}} with {{Limited Parallel Resources}}},
  author = {Bai, Yu and Gao, Yang and Huang, Heyan},
  year = {2021},
  month = jun,
  number = {arXiv:2105.13648},
  eprint = {arXiv:2105.13648},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.13648},
  urldate = {2023-02-02},
  abstract = {Parallel cross-lingual summarization data is scarce, requiring models to better use the limited available cross-lingual resources. Existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks. Such approaches apply multiple decoders, each of which is utilized for a specific task. However, these independent decoders share no parameters, hence fail to capture the relationships between the discrete phrases of summaries in different languages, breaking the connections in order to transfer the knowledge of the high-resource languages to low-resource languages. To bridge these connections, we propose a novel Multi-Task framework for Cross-Lingual Abstractive Summarization (MCLAS) in a low-resource setting. Employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries, MCLAS makes the monolingual summarization task a prerequisite of the cross-lingual summarization (CLS) task. In this way, the shared decoder learns interactions involving alignments and summary patterns across languages, which encourages attaining knowledge transfer. Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios. Moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS, which benefits the CLS task under limited parallel resources.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/7RKZKSPF/Bai et al. - 2021 - Cross-Lingual Abstractive Summarization with Limit.pdf;/home/xav/Zotero/storage/WDPVNRTG/2105.html}
}

@article{baiMoreReadingComprehension2022,
  title = {More {{Than Reading Comprehension}}: {{A Survey}} on {{Datasets}} and {{Metrics}} of {{Textual Question Answering}}},
  shorttitle = {More {{Than Reading Comprehension}}},
  author = {Bai, Yang and Wang, Daisy Zhe},
  year = {2022},
  month = feb,
  journal = {arXiv:2109.12264 [cs]},
  eprint = {2109.12264},
  primaryclass = {cs},
  urldate = {2022-04-29},
  abstract = {Textual Question Answering (QA) aims to provide precise answers to user's questions in natural language using unstructured data. One of the most popular approaches to this goal is machine reading comprehension(MRC). In recent years, many novel datasets and evaluation metrics based on classical MRC tasks have been proposed for broader textual QA tasks. In this paper, we survey 47 recent textual QA benchmark datasets and propose a new taxonomy from an application point of view. In addition, We summarize 8 evaluation metrics of textual QA tasks. Finally, we discuss current trends in constructing textual QA benchmarks and suggest directions for future work.},
  archiveprefix = {arxiv},
  keywords = {68T50,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/IDN45HA2/Bai et Wang - 2022 - More Than Reading Comprehension A Survey on Datas.pdf;/home/xav/Zotero/storage/RBS43J5M/2109.html}
}

@misc{baiTrainingHelpfulHarmless2022,
  title = {Training a {{Helpful}} and {{Harmless Assistant}} with {{Reinforcement Learning}} from {{Human Feedback}}},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05862},
  eprint = {arXiv:2204.05862},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.05862},
  urldate = {2022-10-29},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/NIQY7JZD/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf;/home/xav/Zotero/storage/D7LHLKKJ/2204.html}
}

@misc{bakerVideoPreTrainingVPT2022,
  title = {Video {{PreTraining}} ({{VPT}}): {{Learning}} to {{Act}} by {{Watching Unlabeled Online Videos}}},
  shorttitle = {Video {{PreTraining}} ({{VPT}})},
  author = {Baker, Bowen and Akkaya, Ilge and Zhokhov, Peter and Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and Houghton, Brandon and Sampedro, Raul and Clune, Jeff},
  year = {2022},
  month = jun,
  number = {arXiv:2206.11795},
  eprint = {arXiv:2206.11795},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.11795},
  urldate = {2022-11-27},
  abstract = {Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/U7ZHWJU9/Baker et al. - 2022 - Video PreTraining (VPT) Learning to Act by Watchi.pdf}
}

@misc{bakhtinMasteringGameNoPress2022,
  title = {Mastering the {{Game}} of {{No-Press Diplomacy}} via {{Human-Regularized Reinforcement Learning}} and {{Planning}}},
  author = {Bakhtin, Anton and Wu, David J. and Lerer, Adam and Gray, Jonathan and Jacob, Athul Paul and Farina, Gabriele and Miller, Alexander H. and Brown, Noam},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05492},
  eprint = {arXiv:2210.05492},
  publisher = {{arXiv}},
  urldate = {2023-02-10},
  abstract = {No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitationlearned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/xav/Zotero/storage/2384HQD6/Bakhtin et al. - 2022 - Mastering the Game of No-Press Diplomacy via Human.pdf}
}

@misc{bakkerFinetuningLanguageModels2022,
  title = {Fine-Tuning Language Models to Find Agreement among Humans with Diverse Preferences},
  author = {Bakker, Michiel A. and Chadwick, Martin J. and Sheahan, Hannah R. and Tessler, Michael Henry and {Campbell-Gillingham}, Lucy and Balaguer, Jan and McAleese, Nat and Glaese, Amelia and Aslanides, John and Botvinick, Matthew M. and Summerfield, Christopher},
  year = {2022},
  month = nov,
  number = {arXiv:2211.15006},
  eprint = {arXiv:2211.15006},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.15006},
  urldate = {2023-01-16},
  abstract = {Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single "generic" user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g., "should we raise taxes on the rich?"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs ({$>$}70\%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions ({$>$}65\%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ZMWHELUS/Bakker et al. - 2022 - Fine-tuning language models to find agreement amon.pdf;/home/xav/Zotero/storage/XY3834E5/2211.html}
}

@misc{ballRepresentingNLUMeaning2021,
  title = {Representing {{NLU}} with {{Meaning}}: {{Examples}}},
  shorttitle = {Representing {{NLU}} with {{Meaning}}},
  author = {Ball, John},
  year = {2021},
  month = mar,
  journal = {Pat Inc},
  urldate = {2022-05-04},
  abstract = {Human language is like a code. Our brain takes meaningful ideas and converts them into sequences of muscle movements to communicate with\ldots},
  langid = {english},
  file = {/home/xav/Zotero/storage/UKNNQCKT/representing-nlu-with-meaning-examples-4cd5ea671d1a.html}
}

@misc{ballWhatNewNLU2022,
  title = {What's New with {{NLU}} in 2022?},
  author = {Ball, John},
  year = {2022},
  month = feb,
  journal = {Pat Inc},
  urldate = {2022-05-04},
  abstract = {Natural Language Understanding (NLU) will change how we store information in the future and will provide the tools that we are missing\ldots},
  langid = {english},
  file = {/home/xav/Zotero/storage/RLTB8ESW/whats-new-with-nlu-in-2022-1e04b72b7892.html}
}

@misc{bansalCoSeCoTextConditioned2022,
  title = {{{CoSe-Co}}: {{Text Conditioned Generative CommonSense Contextualizer}}},
  shorttitle = {{{CoSe-Co}}},
  author = {Bansal, Rachit and Aggarwal, Milan and Bhatia, Sumit and Kaur, Jivat Neet and Krishnamurthy, Balaji},
  year = {2022},
  month = jun,
  number = {arXiv:2206.05706},
  eprint = {2206.05706},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-31},
  abstract = {Pre-trained Language Models (PTLMs) have been shown to perform well on natural language tasks. Many prior works have leveraged structured commonsense present in the form of entities linked through labeled relations in Knowledge Graphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static module which limits coverage since KGs contain finite knowledge. Generative methods train PTLMs on KG triples to improve the scale at which knowledge can be obtained. However, training on symbolic KG entities limits their applicability in tasks involving natural language text where they ignore overall context. To mitigate this, we propose a CommonSense Contextualizer (CoSe-Co) conditioned on sentences as input to make it generically usable in tasks for generating knowledge relevant to the overall context of input text. To train CoSe-Co, we propose a novel dataset comprising of sentence and commonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and contain novel entities not present in the underlying KG. We augment generated knowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading to improvements over current best methods on CSQA, ARC, QASC and OBQA datasets. We also demonstrate its applicability in improving performance of a baseline model for paraphrase generation task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/9DXZ2KWW/Bansal et al. - 2022 - CoSe-Co Text Conditioned Generative CommonSense C.pdf}
}

@article{baralNaturalLanguageQA2020,
  title = {Natural {{Language QA Approaches}} Using {{Reasoning}} with {{External Knowledge}}},
  author = {Baral, Chitta and Banerjee, Pratyay and Pal, Kuntal Kumar and Mitra, Arindam},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.03446 [cs]},
  eprint = {2003.03446},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Question answering (QA) in natural language (NL) has been an important aspect of AI from its early days. Winograd's ``councilmen'' example in his 1972 paper and McCarthy's Mr. Hug example of 1976 highlight the role of external knowledge in NL understanding. While Machine Learning has been the go-to approach in NL processing as well as NL question answering (NLQA) for the last 30 years, recently there has been an increasingly emphasized thread on NLQA where external knowledge plays an important role. The challenges inspired by Winograd's councilmen example, and recent developments such as the Rebooting AI book, various NLQA datasets, research on knowledge acquisition in the NLQA context, and their use in various NLQA models have brought the issue of NLQA using ``reasoning'' with external knowledge to the forefront. In this paper we present a survey of the recent work on them. We believe our survey will help establish a bridge between multiple fields of AI, especially between (a) the traditional fields of knowledge representation and reasoning and (b) the field of NL understanding and NLQA.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/22CFST5J/Baral et al. - 2020 - Natural Language QA Approaches using Reasoning wit.pdf}
}

@incollection{baranova-bolotovaMultiDocumentAnswerGeneration2020,
  title = {Multi-{{Document Answer Generation}} for {{Non-Factoid Questions}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {{Baranova-Bolotova}, Valeriia},
  year = {2020},
  month = jul,
  pages = {2477},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  urldate = {2022-05-04},
  abstract = {The current research will be devoted to the challenging and under-investigated task of multi-source answer generation for complex non-factoid questions. We will start with experimenting with generative models on one particular type of non-factoid questions - instrumental/procedural questions which often start with "how-to". For this, a new dataset, comprised of more than 100,000 QA-pairs which were crawled from a dedicated web-resource where each answer has a set of references to the articles it was written upon, will be used. We will also compare different ways of model evaluation to choose a metric which better correlates with human assessment. To be able to do this, the way people evaluate answers to non-factoid questions and set some formal criteria of what makes a good quality answer is needed to be understood. Eye-tracking and crowdsourcing methods will be employed to study how users interact with answers and evaluate them, and how the answer features correlate with task complexity. We hope that our research will help to redefine the way users interact and work with search engines so as to transform IR finally into the answer retrieval systems that users have always desired.},
  isbn = {978-1-4503-8016-4},
  keywords = {deep learning,eye-tracking,non-factoid questions,question answering,user study}
}

@article{basukiStatisticalbasedApproachIndonesian2016,
  title = {Statistical-Based {{Approach}} for {{Indonesian Complex Factoid Question Decomposition}}},
  author = {Basuki, Setio and Purwarianti, Ayu},
  year = {2016},
  month = jun,
  journal = {International Journal on Electrical Engineering and Informatics},
  volume = {8},
  pages = {356--373},
  doi = {10.15676/ijeei.2016.8.2.9},
  abstract = {This research has proposed a method to decompose complex factoid question into several independent questions. The method comprises four stages: (1) classifying input question into several categories such as sub-question, coordination, exemplification, or double question, (2) generating all possible question boundary candidates, (3) selecting the best question boundary, and (4) performing the question decomposition rule using the best question boundary. This study compared several machine learning algorithms in the first stage (complex factoid question classification) and third stage (question decomposition boundary selection). The features used in the classification are specific word lists with its related information including the syntactic features of POS (Part of Speech) tag. For the experiments, we annotated 916 sentences for training data and 226 sentences for testing data. The perplexity of the annotated corpus achieved 1.000586 with 307 Out of Vocabulary (OOV). The complex factoid question classification accuracy reached 93.8\% with Random Forest algorithm. The question decomposition boundary selection accuracy achieved 93.80\% for sub-question (using Random Forest algorithm), 86.11\% for double question (using Random Forest algorithm), 88.23\% for coordination (using SMO), and 60.87\% for exemplification (using kNN, NB, and RF). A revision rule was provided for the question decomposition boundary selection that improved the accuracy into 97.22\% for double question, 94.11\% for coordination, and 65.21\% for exemplification. \textcopyright{} 2016, School of Electrical Engineering and Informatics. All rights reserved.},
  file = {/home/xav/Zotero/storage/CI7RT5VI/Basuki et Purwarianti - 2016 - Statistical-based Approach for Indonesian Complex .pdf}
}

@incollection{baxterTheoreticalModelsLearning1998,
  title = {Theoretical {{Models}} of {{Learning}} to {{Learn}}},
  booktitle = {Learning to {{Learn}}},
  author = {Baxter, Jonathan},
  editor = {Thrun, Sebastian and Pratt, Lorien},
  year = {1998},
  pages = {71--94},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4615-5529-2_4},
  urldate = {2023-02-02},
  abstract = {A Machine can only learn if it is biased in some way. Typically the bias is supplied by hand, for example through the choice of an appropriate set of features. However, if the learning machine is embedded within an environment of related tasks, then it can learn its own bias by learning sufficiently many tasks from the environment [Baxter, 1995b; Baxter, 1997]. In this paper two models of bias learning (or equivalently, learning to learn) are introduced and the main theoretical results presented. The first model is a PAC-type model based on empirical process theory, while the second is a hierarchical Bayes model.},
  isbn = {978-1-4615-5529-2},
  langid = {english},
  keywords = {Empirical Process,Feature Weight,Hypothesis Space,Output Weight,Prior Distribution},
  file = {/home/xav/Zotero/storage/HNJDXE6K/Baxter - 1998 - Theoretical Models of Learning to Learn.pdf}
}

@inproceedings{becattiniComputerAidedProblemSolving2011,
  title = {Computer-{{Aided Problem Solving}} - {{Part}} 1: {{Objectives}}, {{Approaches}}, {{Opportunities}}},
  shorttitle = {Computer-{{Aided Problem Solving}} - {{Part}} 1},
  booktitle = {4th {{Computer-Aided Innovation}} ({{CAI}})},
  author = {Becattini, Niccol{\`o} and Borgianni, Yuri and Cascini, Gaetano and Rotini, Federico},
  editor = {Cavallucci, Denis and Guio, Roland and Cascini, Gaetano},
  year = {2011},
  month = jun,
  series = {Building {{Innovation Pipelines}} through {{Computer-Aided Innovation}}},
  volume = {AICT-355},
  pages = {117--131},
  publisher = {{Springer}},
  address = {{Strasbourg, France}},
  doi = {10.1007/978-3-642-22182-8_10},
  urldate = {2022-05-11},
  abstract = {Among the different aims and scopes of Computer-Aided Innovation (CAI) systems a relevant topic is the support of inventive problem solving tasks. The paper presents the research activity developed by the authors in this domain, encompassing the review of the distinctive features of problems encountered by designers and the common approaches employed to overcome them. A further thread of the investigation carried out in this paper concerns the limitations of computer-based approaches exploiting acknowledged models for problem solving. Downstream of the performed analysis the authors highlight the requirements that a novel CAI application should fulfil, supporting the opportunities for building a dialogue-based system.},
  keywords = {Computer-Aided Innovation,conceptual design,dialogue-based system,inventive problem solving},
  annotation = {QID: Q59167292},
  file = {/home/xav/Zotero/storage/FCZQ4G8R/Becattini et al. - 2011 - Computer-Aided Problem Solving - Part 1 Objective.pdf}
}

@inproceedings{becattiniComputerAidedProblemSolving2011a,
  title = {Computer-{{Aided Problem Solving}} - {{Part}} 2: {{A Dialogue-Based System}} to {{Support}} the {{Analysis}} of {{Inventive Problems}}},
  shorttitle = {Computer-{{Aided Problem Solving}} - {{Part}} 2},
  booktitle = {4th {{Computer-Aided Innovation}} ({{CAI}})},
  author = {Becattini, Niccol{\`o} and Borgianni, Yuri and Cascini, Gaetano and Rotini, Federico},
  editor = {Cavallucci, Denis and Guio, Roland and Cascini, Gaetano},
  year = {2011},
  month = jun,
  series = {Building {{Innovation Pipelines}} through {{Computer-Aided Innovation}}},
  volume = {AICT-355},
  pages = {132--148},
  publisher = {{Springer}},
  address = {{Strasbourg, France}},
  doi = {10.1007/978-3-642-22182-8_11},
  urldate = {2022-05-11},
  abstract = {The paper illustrates an original model and a dialogue-based software application that have been developed by integrating the logic of ARIZ with some OTSM-TRIZ models, in order to guide a user, also with no TRIZ background, to the analysis of inventive problems. The dialogue-based procedure brings to the construction of a model of the inventive problem, which is used both to trigger new solutions by highlighting different solving perspectives and to start an automatic knowledge search within technical and scientific information. The prototype system has been tested with students at Politecnico di Milano and at the University of Florence. The paper details the structure of the algorithm and the results of the first validation activity.},
  keywords = {Computer-Aided Innovation,conceptual design,dialogue-based system,OTSM-TRIZ,problem solving},
  annotation = {QID: Q59167295},
  file = {/home/xav/Zotero/storage/RM97JDMS/Becattini et al. - 2011 - Computer-Aided Problem Solving - Part 2 A Dialogu.pdf}
}

@article{ben-davidPADAExamplebasedPrompt2022,
  title = {{{PADA}}: {{Example-based Prompt Learning}} for on-the-Fly {{Adaptation}} to {{Unseen Domains}}},
  shorttitle = {{{PADA}}},
  author = {{Ben-David}, Eyal and Oved, Nadav and Reichart, Roi},
  year = {2022},
  month = jan,
  journal = {arXiv:2102.12206 [cs]},
  eprint = {2102.12206},
  primaryclass = {cs},
  urldate = {2022-04-25},
  abstract = {Natural Language Processing algorithms have made incredible progress, but they still struggle when applied to out-of-distribution examples. We address a challenging and underexplored version of this domain adaptation problem, where an algorithm is trained on several source domains, and then applied to examples from unseen domains that are unknown at training time. Particularly, no examples, labeled or unlabeled, or any other knowledge about the target domain are available to the algorithm at training time. We present PADA: An example-based autoregressive Prompt learning algorithm for on-the-fly Any-Domain Adaptation, based on the T5 language model. Given a test example, PADA first generates a unique prompt for it and then, conditioned on this prompt, labels the example with respect to the NLP prediction task. PADA is trained to generate a prompt which is a token sequence of unrestricted length, consisting of Domain Related Features (DRFs) that characterize each of the source domains. Intuitively, the generated prompt is a unique signature that maps the test example to a semantic space spanned by the source domains. In experiments with 3 tasks (text classification and sequence tagging), for a total of 14 multi-source adaptation scenarios, PADA substantially outperforms strong baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/QSS2PXW6/Ben-David et al. - 2022 - PADA Example-based Prompt Learning for on-the-fly.pdf;/home/xav/Zotero/storage/YVKYBE5N/2102.html}
}

@article{betzThinkingAloudDynamic2021,
  title = {Thinking {{Aloud}}: {{Dynamic Context Generation Improves Zero-Shot Reasoning Performance}} of {{GPT-2}}},
  shorttitle = {Thinking {{Aloud}}},
  author = {Betz, Gregor and Richardson, Kyle and Voigt, Christian},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.13033 [cs]},
  eprint = {2103.13033},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Thinking aloud is an effective meta-cognitive strategy human reasoners apply to solve difficult problems. We suggest to improve the reasoning ability of pre-trained neural language models in a similar way, namely by expanding a task's context with problem elaborations that are dynamically generated by the language model itself. Our main result is that dynamic problem elaboration significantly improves the zero-shot performance of GPT-2 in a deductive reasoning and natural language inference task: While the model uses a syntactic heuristic for predicting an answer, it is capable (to some degree) of generating reasoned additional context which facilitates the successful application of its heuristic. We explore different ways of generating elaborations, including fewshot learning, and find that their relative performance varies with the specific problem characteristics (such as problem difficulty). Moreover, the effectiveness of an elaboration can be explained in terms of the degree to which the elaboration semantically coheres with the corresponding problem. In particular, elaborations that are most faithful to the original problem description may boost accuracy by up to 24\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/54GF3VJ3/Betz et al. - 2021 - Thinking Aloud Dynamic Context Generation Improve.pdf;/home/xav/Zotero/storage/GJBG7TGQ/2103.html}
}

@misc{beurer-kellnerPromptingProgrammingQuery2022,
  title = {Prompting {{Is Programming}}: {{A Query Language For Large Language Models}}},
  shorttitle = {Prompting {{Is Programming}}},
  author = {{Beurer-Kellner}, Luca and Fischer, Marc and Vechev, Martin},
  year = {2022},
  month = dec,
  number = {arXiv:2212.06094},
  eprint = {arXiv:2212.06094},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.06094},
  urldate = {2023-02-07},
  abstract = {Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction. Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks, while abstracting language model internals and providing high-level semantics. To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (13-85\% cost savings).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/FC7CY2XJ/Beurer-Kellner et al. - 2022 - Prompting Is Programming A Query Language For Lar.pdf;/home/xav/Zotero/storage/YY4SURDY/2212.html}
}

@misc{bigetal.ImitationGameQuantifying2022,
  title = {Beyond the {{Imitation Game}}: {{Quantifying}} and Extrapolating the Capabilities of Language Models},
  shorttitle = {Beyond the {{Imitation Game}}},
  author = {{BIG et al.}},
  year = {2022},
  month = jun,
  number = {arXiv:2206.04615},
  eprint = {arXiv:2206.04615},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.04615},
  urldate = {2023-02-10},
  abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/V83IEPGY/Srivastava et al. - 2022 - Beyond the Imitation Game Quantifying and extrapo.pdf;/home/xav/Zotero/storage/N5JGDKCX/2206.html}
}

@misc{bigscienceetal.BLOOM176BParameterOpenAccess2022,
  title = {{{BLOOM}}: {{A 176B-Parameter Open-Access Multilingual Language Model}}},
  shorttitle = {{{BLOOM}}},
  author = {{BigScience et al.}},
  year = {2022},
  month = dec,
  number = {arXiv:2211.05100},
  eprint = {arXiv:2211.05100},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.05100},
  urldate = {2023-01-21},
  abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/2D2P8WEJ/Workshop et al. - 2022 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf;/home/xav/Zotero/storage/6DQX8IX9/2211.html}
}

@article{bilginOntologyBasedApproachDelay2017,
  title = {An {{Ontology-Based Approach}} for {{Delay Analysis}} in {{Construction}}},
  author = {Bilgin, Gozde and Dikmen, Irem and Birgonul, M.},
  year = {2017},
  month = apr,
  journal = {KSCE Journal of Civil Engineering},
  volume = {22},
  pages = {1--15},
  doi = {10.1007/s12205-017-0651-5},
  abstract = {Delay is a common problem of the construction sector and it is one of the major reasons of claims between project participants. Systematic and reliable delay analysis is critical for successful management of claims. In this study, a delay analysis ontology is proposed that may facilitate development of databases, information sharing as well as retrieval for delay analysis within construction companies. A detailed literature review on construction delays has been carried out during the development of the ontology and it is evaluated by using five case studies. The delay analysis ontology may be used for different purposes especially to support decision-making during risk and claim management processes. It may enable companies to create their own databases, corporate memories and develop decision support systems for better analysis of delays.},
  file = {/home/xav/Zotero/storage/4TQ5ERKE/Bilgin et al. - 2017 - An Ontology-Based Approach for Delay Analysis in C.pdf}
}

@article{biskPIQAReasoningPhysical2020,
  title = {{{PIQA}}: {{Reasoning}} about {{Physical Commonsense}} in {{Natural Language}}},
  shorttitle = {{{PIQA}}},
  author = {Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao, Jianfeng and Choi, Yejin},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {7432--7439},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6239},
  urldate = {2023-02-01},
  abstract = {To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains \textendash{} such as news articles and encyclopedia entries, where text is plentiful \textendash{} in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical commonsense questions without experiencing the physical world?In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95\% accuracy), large pretrained models struggle ({$\sim$}75\%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/home/xav/Zotero/storage/IZQ7BNNW/Bisk et al. - 2020 - PIQA Reasoning about Physical Commonsense in Natu.pdf}
}

@misc{blakeneyReduceReuseRecycle2022,
  title = {Reduce, {{Reuse}}, {{Recycle}}: {{Improving Training Efficiency}} with {{Distillation}}},
  shorttitle = {Reduce, {{Reuse}}, {{Recycle}}},
  author = {Blakeney, Cody and Forde, Jessica Zosa and Frankle, Jonathan and Zong, Ziliang and Leavitt, Matthew L.},
  year = {2022},
  month = nov,
  number = {arXiv:2211.00683},
  eprint = {arXiv:2211.00683},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.00683},
  urldate = {2023-02-09},
  abstract = {Methods for improving the efficiency of deep network training (i.e. the resources required to achieve a given level of model quality) are of immediate benefit to deep learning practitioners. Distillation is typically used to compress models or improve model quality, but it's unclear if distillation actually improves training efficiency. Can the quality improvements of distillation be converted into training speed-ups, or do they simply increase final model quality with no resource savings? We conducted a series of experiments to investigate whether and how distillation can be used to accelerate training using ResNet-50 trained on ImageNet and BERT trained on C4 with a masked language modeling objective and evaluated on GLUE, using common enterprise hardware (8x NVIDIA A100). We found that distillation can speed up training by up to 1.96x in ResNet-50 trained on ImageNet and up to 1.42x on BERT when evaluated on GLUE. Furthermore, distillation for BERT yields optimal results when it is only performed for the first 20-50\% of training. We also observed that training with distillation is almost always more efficient than training without distillation, even when using the poorest-quality model as a teacher, in both ResNet-50 and BERT. Finally, we found that it's possible to gain the benefit of distilling from an ensemble of teacher models, which has O(n) runtime cost, by randomly sampling a single teacher from the pool of teacher models on each step, which only has a O(1) runtime cost. Taken together, these results show that distillation can substantially improve training efficiency in both image classification and language modeling, and that a few simple optimizations to distillation protocols can further enhance these efficiency improvements.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/2T58WI6X/Blakeney et al. - 2022 - Reduce, Reuse, Recycle Improving Training Efficie.pdf;/home/xav/Zotero/storage/8CMKQ2BC/2211.html}
}

@inproceedings{bolotovaNonFactoidQuestionAnsweringTaxonomy2022,
  title = {A {{Non-Factoid Question-Answering Taxonomy}}},
  booktitle = {Proceedings of the 45th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Bolotova, Valeriia and Blinov, Vladislav and Scholer, Falk and Croft, W. Bruce and Sanderson, Mark},
  year = {2022},
  month = jul,
  pages = {1196--1207},
  publisher = {{ACM}},
  address = {{Madrid Spain}},
  doi = {10.1145/3477495.3531926},
  urldate = {2022-09-06},
  abstract = {Non-factoid question answering (NFQA) is a challenging and underresearched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-factoid questions \textendash NFQs. There is still little understanding of the categories of NFQs that people tend to ask, what form of answers they expect to see in return, and what the key research challenges of each category are. This work presents the first comprehensive taxonomy of NFQ categories and the expected structure of answers. The taxonomy was constructed with a transparent methodology and extensively evaluated via crowdsourcing. The most challenging categories were identified through an editorial user study. We also release a dataset of categorised NFQs and a question category classifier1.},
  isbn = {978-1-4503-8732-3},
  langid = {english},
  file = {/home/xav/Zotero/storage/9EEPJGQT/Bolotova et al. - 2022 - A Non-Factoid Question-Answering Taxonomy.pdf}
}

@inproceedings{borchmannDUEEndtoEndDocument2021,
  title = {{{DUE}}: {{End-to-End Document Understanding Benchmark}}},
  shorttitle = {{{DUE}}},
  booktitle = {Thirty-Fifth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}} ({{Round}} 2)},
  author = {Borchmann, {\L}ukasz and Pietruszka, Micha{\l} and Stanislawek, Tomasz and Jurkiewicz, Dawid and Turski, Micha{\l} and Szyndler, Karolina and Grali{\'n}ski, Filip},
  year = {2021},
  month = aug,
  urldate = {2022-05-17},
  abstract = {Description of a benchmark spanning multiple end-to-end tasks related to understanding multi-modal documents with complex layouts.},
  langid = {english},
  file = {/home/xav/Zotero/storage/2LQMPLZE/Borchmann et al. - 2021 - DUE End-to-End Document Understanding Benchmark.pdf;/home/xav/Zotero/storage/GTIFMNV8/forum.html}
}

@article{boreshbanImprovingQuestionAnswering2021,
  title = {Improving {{Question Answering Performance Using Knowledge Distillation}} and {{Active Learning}}},
  author = {Boreshban, Yasaman and Mirbostani, Seyed Morteza and {Ghassem-Sani}, Gholamreza and Mirroshandel, Seyed Abolghasem and Amiriparian, Shahin},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.12662 [cs]},
  eprint = {2109.12662},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Contemporary question answering (QA) systems, including transformer-based architectures, suffer from increasing computational and model complexity which render them inefficient for real-world applications with limited resources. Further, training or even fine-tuning such models requires a vast amount of labeled data which is often not available for the task at hand. In this manuscript, we conduct a comprehensive analysis of the mentioned challenges and introduce suitable countermeasures. We propose a novel knowledge distillation (KD) approach to reduce the parameter and model complexity of a pre-trained BERT system and utilize multiple active learning (AL) strategies for immense reduction in annotation efforts. In particular, we demonstrate that our model achieves the performance of a 6-layer TinyBERT and DistilBERT, whilst using only 2\% of their total parameters. Finally, by the integration of our AL approaches into the BERT framework, we show that state-of-the-art results on the SQuAD dataset can be achieved when we only use 20\% of the training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/NK8N4HJA/Boreshban et al. - 2021 - Improving Question Answering Performance Using Kno.pdf;/home/xav/Zotero/storage/8LIK6AHG/2109.html}
}

@article{borgeaudImprovingLanguageModels2022,
  title = {Improving Language Models by Retrieving from Trillions of Tokens},
  author = {Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and van den Driessche, George and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan, Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris and Cassirer, Albin and Brock, Andy and Paganini, Michela and Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and Simonyan, Karen and Rae, Jack W. and Elsen, Erich and Sifre, Laurent},
  year = {2022},
  month = feb,
  journal = {arXiv:2112.04426 [cs]},
  eprint = {2112.04426},
  primaryclass = {cs},
  urldate = {2022-03-29},
  abstract = {We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a \$2\$ trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25\$\textbackslash times\$ fewer parameters. After fine-tuning, RETRO performance translates to downstream knowledge-intensive tasks such as question answering. RETRO combines a frozen Bert retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/LEY5BYAU/Borgeaud et al. - 2022 - Improving language models by retrieving from trill.pdf;/home/xav/Zotero/storage/8AVA3VDV/2112.html}
}

@book{borjiCategoricalArchiveChatGPT2023,
  title = {A {{Categorical Archive}} of {{ChatGPT Failures}}},
  author = {Borji, Ali},
  year = {2023},
  month = feb,
  abstract = {Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Ten categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.},
  file = {/home/xav/Zotero/storage/R5HK42IY/Borji - 2023 - A Categorical Archive of ChatGPT Failures.pdf}
}

@article{bourtouleMachineUnlearning2020,
  title = {Machine {{Unlearning}}},
  author = {Bourtoule, Lucas and Chandrasekaran, Varun and {Choquette-Choo}, Christopher A. and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  year = {2020},
  month = dec,
  journal = {arXiv:1912.03817 [cs]},
  eprint = {1912.03817},
  primaryclass = {cs},
  urldate = {2022-05-17},
  abstract = {Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult. We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning. Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63x, and 2.45x for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36x in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/4HVUDAHB/Bourtoule et al. - 2020 - Machine Unlearning.pdf}
}

@article{bozzonIntroductionHumanComputation2013,
  title = {An {{Introduction}} to {{Human Computation}} and {{Games}} with a {{Purpose}}},
  author = {Bozzon, Alessandro and Galli, Luca},
  year = {2013},
  journal = {Lecture Notes in Computer Science},
  pages = {514--517},
  issn = {0302-9743},
  urldate = {2022-05-04},
  abstract = {(Full-text PDF) An introduction to human computation and games with a purpose},
  langid = {english},
  file = {/home/xav/Zotero/storage/IRRE4ABF/1 An Introduction to Human Computation and Games With A Purpose - Part I .pdf;/home/xav/Zotero/storage/S7V5N5CN/An_introduction_to_human_computation_and.pdf;/home/xav/Zotero/storage/P9XDXRCC/An_introduction_to_human_computation_and_games_with_a_purpose.html}
}

@article{braggFLEXUnifyingEvaluation2021,
  title = {{{FLEX}}: {{Unifying Evaluation}} for {{Few-Shot NLP}}},
  shorttitle = {{{FLEX}}},
  author = {Bragg, Jonathan and Cohan, Arman and Lo, Kyle and Beltagy, Iz},
  year = {2021},
  month = nov,
  journal = {arXiv:2107.07170 [cs]},
  eprint = {2107.07170},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {/home/xav/Zotero/storage/NNUFLIR7/Bragg et al. - 2021 - FLEX Unifying Evaluation for Few-Shot NLP.pdf;/home/xav/Zotero/storage/3BN7F4G9/2107.html}
}

@misc{bratanicConstructBiomedicalKnowledge2021,
  title = {Construct a Biomedical Knowledge Graph with {{NLP}}},
  author = {Bratanic, Tomaz},
  year = {2021},
  month = oct,
  journal = {Medium},
  urldate = {2022-05-04},
  abstract = {Learn how to combine OCR, named entity linking, relation extraction and external enrichment databases to construct a biomedical knowledge\ldots},
  howpublished = {https://towardsdatascience.com/construct-a-biomedical-knowledge-graph-with-nlp-1f25eddc54a0},
  langid = {english},
  file = {/home/xav/Zotero/storage/I3S7P26J/construct-a-biomedical-knowledge-graph-with-nlp-1f25eddc54a0.html}
}

@misc{briggsHowCreateAnswer2021,
  title = {How to {{Create}} an {{Answer From}} a {{Question With DPR}}},
  author = {Briggs, James},
  year = {2021},
  month = sep,
  journal = {Medium},
  urldate = {2022-05-04},
  abstract = {Next-gen Q\&A techniques for next-gen intelligent solutions},
  howpublished = {https://towardsdatascience.com/how-to-create-an-answer-from-a-question-with-dpr-d76e29cc5d60},
  langid = {english},
  file = {/home/xav/Zotero/storage/JT2KB4D3/how-to-create-an-answer-from-a-question-with-dpr-d76e29cc5d60.html}
}

@misc{brooksInContextPolicyIteration2022,
  title = {In-{{Context Policy Iteration}}},
  author = {Brooks, Ethan and Walls, Logan and Lewis, Richard L. and Singh, Satinder},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03821},
  eprint = {arXiv:2210.03821},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03821},
  urldate = {2023-02-08},
  abstract = {This work presents In-Context Policy Iteration, an algorithm for performing Reinforcement Learning (RL), in-context, using foundation models. While the application of foundation models to RL has received considerable attention, most approaches rely on either (1) the curation of expert demonstrations (either through manual design or task-specific pretraining) or (2) adaptation to the task of interest using gradient methods (either fine-tuning or training of adapter layers). Both of these techniques have drawbacks. Collecting demonstrations is labor-intensive, and algorithms that rely on them do not outperform the experts from which the demonstrations were derived. All gradient techniques are inherently slow, sacrificing the "few-shot" quality that made in-context learning attractive to begin with. In this work, we present an algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients. Instead we present a policy-iteration method in which the prompt content is the entire locus of learning. ICPI iteratively updates the contents of the prompt from which it derives its policy through trial-and-error interaction with an RL environment. In order to eliminate the role of in-weights learning (on which approaches like Decision Transformer rely heavily), we demonstrate our algorithm using Codex, a language model with no prior knowledge of the domains on which we evaluate it.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/FWEVV6BD/Brooks et al. - 2022 - In-Context Policy Iteration.pdf;/home/xav/Zotero/storage/GC896YJH/2210.html}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {arXiv:2005.14165},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.14165},
  urldate = {2023-02-06},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/JNYG42GN/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/xav/Zotero/storage/8D4M469T/2005.html}
}

@article{brownWhatDoesIt2022,
  title = {What {{Does}} It {{Mean}} for a {{Language Model}} to {{Preserve Privacy}}?},
  author = {Brown, Hannah and Lee, Katherine and Mireshghallah, Fatemehsadat and Shokri, Reza and Tram{\`e}r, Florian},
  year = {2022},
  month = feb,
  doi = {10.48550/arXiv.2202.05520},
  urldate = {2022-07-19},
  abstract = {Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.},
  langid = {english},
  annotation = {QID: Q116754344},
  file = {/home/xav/Zotero/storage/V5L32M6G/Brown et al. - 2022 - What Does it Mean for a Language Model to Preserve.pdf;/home/xav/Zotero/storage/9HCGTX2P/2202.html}
}

@article{bruauxOntologicalApproachConstruction2005,
  title = {An Ontological Approach to the Construction of Problem-Solving Models},
  author = {Bruaux, Sabine and Kassel, Gilles and Morel, Gilles},
  year = {2005},
  month = may,
  journal = {arXiv:cs/0505081},
  eprint = {cs/0505081},
  urldate = {2022-05-04},
  abstract = {Our ongoing work aims at defining an ontology-centered approach for building expertise models for the CommonKADS methodology. This approach (which we have named "OntoKADS") is founded on a core problem-solving ontology which distinguishes between two conceptualization levels: at an object level, a set of concepts enable us to define classes of problem-solving situations, and at a meta level, a set of meta-concepts represent modeling primitives. In this article, our presentation of OntoKADS will focus on the core ontology and, in particular, on roles - the primitive situated at the interface between domain knowledge and reasoning, and whose ontological status is still much debated. We first propose a coherent, global, ontological framework which enables us to account for this primitive. We then show how this novel characterization of the primitive allows definition of new rules for the construction of expertise models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,I.2.4},
  file = {/home/xav/Zotero/storage/8LTHU3GA/Bruaux et al. - 2005 - An ontological approach to the construction of pro.pdf;/home/xav/Zotero/storage/A25WYGQQ/0505081.html}
}

@article{bruijnUnifiedSemanticWeb2005,
  title = {A Unified {{Semantic Web}} Services Architecture Based on {{WSMF}} and {{UPML}}},
  author = {Bruijn, J. and Lara, Rub{\'e}n and Arroyo, Sinuh{\'e} and G{\'o}mez, J. and Han, Sung-Kook and Fensel, D.},
  year = {2005},
  journal = {Int. J. Web Eng. Technol.},
  doi = {10.1504/IJWET.2005.008482},
  abstract = {This work proposes a unified architecture based on the principles of WSMF and UPML that introduces goal- and domain-independent web services and introduces bridges and refiners for goal, web service and domain descriptions. Current semantic web services lack reusability and conceptual separation between services and goals. We propose a unified architecture based on the principles of WSMF and UPML. We introduce goal- and domain-independent web services. Reuse is achieved through the use of bridges and refiners for goal, web service and domain descriptions.},
  file = {/home/xav/Zotero/storage/DBRIL3PZ/Bruijn et al. - 2005 - A unified Semantic Web services architecture based.pdf}
}

@misc{bubeckSparksArtificialGeneral2023,
  title = {Sparks of {{Artificial General Intelligence}}: {{Early}} Experiments with {{GPT-4}}},
  shorttitle = {Sparks of {{Artificial General Intelligence}}},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = mar,
  number = {arXiv:2303.12712},
  eprint = {arXiv:2303.12712},
  publisher = {{arXiv}},
  urldate = {2023-03-24},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4 [Ope23], was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/MHUA4GUU/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf}
}

@misc{budachEffectsDataQuality2022,
  title = {The {{Effects}} of {{Data Quality}} on {{Machine Learning Performance}}},
  author = {Budach, Lukas and Feuerpfeil, Moritz and Ihde, Nina and Nathansen, Andrea and Noack, Nele and Patzlaff, Hendrik and Naumann, Felix and Harmouch, Hazar},
  year = {2022},
  month = nov,
  number = {arXiv:2207.14529},
  eprint = {arXiv:2207.14529},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.14529},
  urldate = {2023-02-02},
  abstract = {Modern artificial intelligence (AI) applications require large quantities of training and test data. This need creates critical challenges not only concerning the availability of such data, but also regarding its quality. For example, incomplete, erroneous or inappropriate training data can lead to unreliable models that produce ultimately poor decisions. Trustworthy AI applications require high-quality training and test data along many dimensions, such as accuracy, completeness, consistency, and uniformity. We explore empirically the relationship between six of the traditional data quality dimensions and the performance of fifteen widely used machine learning (ML) algorithms covering the tasks of classification, regression, and clustering, with the goal of explaining their performance in terms of data quality. Our experiments distinguish three scenarios based on the AI pipeline steps that were fed with polluted data: polluted training data, test data, or both. We conclude the paper with an extensive discussion of our observations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases},
  file = {/home/xav/Zotero/storage/7T98LBQS/Budach et al. - 2022 - The Effects of Data Quality on Machine Learning Pe.pdf;/home/xav/Zotero/storage/4DVD6LPL/2207.html}
}

@article{buddSurveyActiveLearning2021,
  title = {A {{Survey}} on {{Active Learning}} and {{Human-in-the-Loop Deep Learning}} for {{Medical Image Analysis}}},
  author = {Budd, Samuel and Robinson, Emma C. and Kainz, Bernhard},
  year = {2021},
  month = jul,
  journal = {Medical Image Analysis},
  volume = {71},
  eprint = {1910.02923},
  pages = {102062},
  issn = {13618415},
  doi = {10.1016/j.media.2021.102062},
  urldate = {2022-03-21},
  abstract = {Fully automatic deep learning has become the state-of-the-art technique for many tasks including image acquisition, analysis and interpretation, and for the extraction of clinically useful information for computer-aided detection, diagnosis, treatment planning, intervention and therapy. However, the unique challenges posed by medical image analysis suggest that retaining a human end user in any deep learning enabled system will be beneficial. In this review we investigate the role that humans might play in the development and deployment of deep learning enabled diagnostic applications and focus on techniques that will retain a significant input from a human end user. Human-in-the-Loop computing is an area that we see as increasingly important in future research due to the safety-critical nature of working in the medical domain. We evaluate four key areas that we consider vital for deep learning in the clinical practice: (1) Active Learning to choose the best data to annotate for optimal model performance; (2) Interaction with model outputs - using iterative feedback to steer models to optima for a given prediction and offering meaningful ways to interpret and respond to predictions; (3) Practical considerations - developing full scale applications and the key considerations that need to be made before deployment; (4) Future Prospective and Unanswered Questions - knowledge gaps and related research fields that will benefit human-in-the-loop computing as they evolve. We offer our opinions on the most promising directions of research and how various aspects of each area might be unified towards common goals.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/xav/Zotero/storage/CNGPJQIY/Budd et al. - 2021 - A Survey on Active Learning and Human-in-the-Loop .pdf;/home/xav/Zotero/storage/JV5VDUGH/1910.html}
}

@misc{BuildingGPT3Call,
  title = {Building a {{GPT-3 Call}} to {{Action Generator Tool}} That Constantly Improves | {{Width}}.Ai},
  urldate = {2022-11-20},
  abstract = {Let's build a custom CTA generator that you'll actually want to use for your website copy. A call to action is used in website copy to guide the reader to the preferred next step in your funnel. A compelling CTA is a written directive that blends well with the information that is read above and how the information makes the audience feel. What leads to well blended CTA copy and reaching the "goal" feelings can be broken down into things such as tone, target audience, personal connection, and other points. A call to action generator allows you to generate these text fields used in subtitles and buttons instantly based on a few inputs. Many of these inputs focus on the key ideas by having you choose the tone of voice, what you're selling (or where you want the reader to go), and what industry your target audience is in.},
  howpublished = {https://www.width.ai/post/call-to-action-generator-tool},
  file = {/home/xav/Zotero/storage/DCREZVIQ/call-to-action-generator-tool.html}
}

@misc{bulatovRecurrentMemoryTransformer2022,
  title = {Recurrent {{Memory Transformer}}},
  author = {Bulatov, Aydar and Kuratov, Yuri and Burtsev, Mikhail S.},
  year = {2022},
  month = dec,
  number = {arXiv:2207.06881},
  eprint = {arXiv:2207.06881},
  publisher = {{arXiv}},
  urldate = {2023-02-06},
  abstract = {Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of selfattention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/CQ6AFKJ4/Bulatov et al. - 2022 - Recurrent Memory Transformer.pdf}
}

@misc{bulianTomaytoTomahtoTokenlevel2022,
  title = {Tomayto, {{Tomahto}}. {{Beyond Token-level Answer Equivalence}} for {{Question Answering Evaluation}}},
  author = {Bulian, Jannis and Buck, Christian and Gajewski, Wojciech and Boerschinger, Benjamin and Schuster, Tal},
  year = {2022},
  month = oct,
  number = {arXiv:2202.07654},
  eprint = {arXiv:2202.07654},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.07654},
  urldate = {2023-01-16},
  abstract = {The predictions of question answering (QA)systems are typically evaluated against manually annotated finite sets of one or more answers. This leads to a coverage limitation that results in underestimating the true performance of systems, and is typically addressed by extending over exact match (EM) with pre-defined rules or with the token-level F1 measure. In this paper, we present the first systematic conceptual and data-driven analysis to examine the shortcomings of token-level equivalence measures. To this end, we define the asymmetric notion of answer equivalence (AE), accepting answers that are equivalent to or improve over the reference, and publish over 23k human judgments for candidates produced by multiple QA systems on SQuAD. Through a careful analysis of this data, we reveal and quantify several concrete limitations of the F1 measure, such as a false impression of graduality, or missing dependence on the question. Since collecting AE annotations for each evaluated model is expensive, we learn a BERT matching (BEM) measure to approximate this task. Being a simpler task than QA, we find BEM to provide significantly better AE approximations than F1, and to more accurately reflect the performance of systems. Finally, we demonstrate the practical utility of AE and BEM on the concrete application of minimal accurate prediction sets, reducing the number of required answers by up to x2.6.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/F89G2HDN/Bulian et al. - 2022 - Tomayto, Tomahto. Beyond Token-level Answer Equiva.pdf;/home/xav/Zotero/storage/5ZSQJJ78/2202.html}
}

@article{calegariFrameworkBuildGames2018,
  title = {A {{Framework}} to Build {{Games}} with a {{Purpose}} for {{Linked Data Refinement}}},
  author = {Calegari, Gloria Re and Fiano, Andrea and Celino, Irene},
  year = {2018},
  journal = {arXiv:1811.02848 [cs]},
  volume = {11137},
  eprint = {1811.02848},
  primaryclass = {cs},
  pages = {154--169},
  doi = {10.1007/978-3-030-00668-6_10},
  urldate = {2022-05-04},
  abstract = {With the rise of linked data and knowledge graphs, the need becomes compelling to find suitable solutions to increase the coverage and correctness of datasets, to add missing knowledge and to identify and remove errors. Several approaches - mostly relying on machine learning and NLP techniques - have been proposed to address this refinement goal; they usually need a partial gold standard, i.e. some "ground truth" to train automatic models. Gold standards are manually constructed, either by involving domain experts or by adopting crowdsourcing and human computation solutions. In this paper, we present an open source software framework to build Games with a Purpose for linked data refinement, i.e. web applications to crowdsource partial ground truth, by motivating user participation through fun incentive. We detail the impact of this new resource by explaining the specific data linking "purposes" supported by the framework (creation, ranking and validation of links) and by defining the respective crowdsourcing tasks to achieve those goals. To show this resource's versatility, we describe a set of diverse applications that we built on top of it; to demonstrate its reusability and extensibility potential, we provide references to detailed documentation, including an entire tutorial which in a few hours guides new adopters to customize and adapt the framework to a new use case.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/xav/Zotero/storage/AQMA6T3R/Calegari et al. - 2018 - A Framework to build Games with a Purpose for Link.pdf;/home/xav/Zotero/storage/QXRXIE6Q/1811.html}
}

@article{camposSurveyTemporalInformation2014,
  title = {Survey of {{Temporal Information Retrieval}} and {{Related Applications}}},
  author = {Campos, Ricardo and Dias, Ga{\"e}l and Jorge, Al{\'i}pio and Jatowt, Adam},
  year = {2014},
  month = jan,
  journal = {ACM Computing Surveys},
  volume = {47},
  pages = {1--41},
  doi = {10.1145/2619088},
  abstract = {Temporal information retrieval has been a topic of great interest in recent years. Its purpose is to improve the effectiveness of information retrieval methods by exploiting temporal information in documents and queries. In this article, we present a survey of the existing literature on temporal information retrieval. In addition to giving an overview of the field, we categorize the relevant research, describe the main contributions, and compare different approaches. We organize existing research to provide a coherent view, discuss several open issues, and point out some possible future research directions in this area. Despite significant advances, the area lacks a systematic arrangement of prior efforts and an overview of state-of-the-art approaches. Moreover, an effective end-to-end temporal retrieval system that exploits temporal information to improve the quality of the presented results remains undeveloped.},
  file = {/home/xav/Zotero/storage/F3ISDVRD/Campos et al. - 2014 - Survey of Temporal Information Retrieval and Relat.pdf}
}

@misc{casaresProblemTheory2016,
  title = {Problem {{Theory}}},
  author = {Casares, Ram{\'o}n},
  year = {2016},
  month = sep,
  number = {arXiv:1412.1044},
  eprint = {arXiv:1412.1044},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.1044},
  urldate = {2022-09-16},
  abstract = {The Turing machine, as it was presented by Turing himself, models the calculations done by a person. This means that we can compute whatever any Turing machine can compute, and therefore we are Turing complete. The question addressed here is why, Why are we Turing complete? Being Turing complete also means that somehow our brain implements the function that a universal Turing machine implements. The point is that evolution achieved Turing completeness, and then the explanation should be evolutionary, but our explanation is mathematical. The trick is to introduce a mathematical theory of problems, under the basic assumption that solving more problems provides more survival opportunities. So we build a problem theory by fusing set and computing theories. Then we construct a series of resolvers, where each resolver is defined by its computing capacity, that exhibits the following property: all problems solved by a resolver are also solved by the next resolver in the series if certain condition is satisfied. The last of the conditions is to be Turing complete. This series defines a resolvers hierarchy that could be seen as a framework for the evolution of cognition. Then the answer to our question would be: to solve most problems. By the way, the problem theory defines adaptation, perception, and learning, and it shows that there are just three ways to resolve any problem: routine, trial, and analogy. And, most importantly, this theory demonstrates how problems can be used to found mathematics and computing on biology.},
  archiveprefix = {arxiv},
  keywords = {68T20 (Primary); 68T05 (Secondary),Computer Science - Artificial Intelligence,I.2.6,I.2.8},
  file = {/home/xav/Zotero/storage/6PUNPRWC/Casares - 2016 - Problem Theory.pdf}
}

@article{casaresSyntaxEvolutionProblems2019,
  title = {Syntax {{Evolution}}: {{Problems}} and {{Recursion}}},
  shorttitle = {Syntax {{Evolution}}},
  author = {Casares, Ram{\'o}n},
  year = {2019},
  eprint = {1508.03040},
  primaryclass = {cs},
  pages = {185317 Bytes},
  doi = {10.6084/m9.figshare.4956359},
  urldate = {2022-10-28},
  abstract = {To investigate the evolution of syntax, we need to ascertain the evolutionary r\textbackslash\^ole of syntax and, before that, the very nature of syntax. Here, we will assume that syntax is computing. And then, since we are computationally Turing complete, we meet an evolutionary anomaly, the anomaly of sytax: we are syntactically too competent for syntax. Assuming that problem solving is computing, and realizing that the evolutionary advantage of Turing completeness is full problem solving and not syntactic proficiency, we explain the anomaly of syntax by postulating that syntax and problem solving co-evolved in humans towards Turing completeness. Examining the requirements that full problem solving impose on language, we find firstly that semantics is not sufficient and that syntax is necessary to represent problems. Our final conclusion is that full problem solving requires a functional semantics on an infinite tree-structured syntax. Besides these results, the introduction of Turing completeness and problem solving to explain the evolution of syntax should help us to fit the evolution of language within the evolution of cognition, giving us some new clues to understand the elusive relation between language and thinking.},
  archiveprefix = {arxiv},
  keywords = {91F20; 68T20; 68T50; 92D15,Computer Science - Computation and Language,I.2.7,I.2.8},
  file = {/home/xav/Zotero/storage/X2C94UIN/Casares - 2019 - Syntax Evolution Problems and Recursion.pdf}
}

@inproceedings{caselliHowTimeProbing2022,
  title = {How about {{Time}}? {{Probing}} a {{Multilingual Language Model}} for {{Temporal Relations}}},
  shorttitle = {How about {{Time}}?},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Caselli, Tommaso and Dini, Irene and Dell'Orletta, Felice},
  year = {2022},
  month = oct,
  pages = {3197--3209},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Gyeongju, Republic of Korea}},
  urldate = {2023-01-30},
  abstract = {This paper presents a comprehensive set of probing experiments using a multilingual language model, XLM-R, for temporal relation classification between events in four languages. Results show an advantage of contextualized embeddings over static ones and a detrimen- tal role of sentence level embeddings. While obtaining competitive results against state-of-the-art systems, our probes indicate a lack of suitable encoded information to properly address this task.},
  file = {/home/xav/Zotero/storage/THUB2NWI/Caselli et al. - 2022 - How about Time Probing a Multilingual Language Mo.pdf}
}

@misc{changWebQAMultihopMultimodal2022a,
  title = {{{WebQA}}: {{Multihop}} and {{Multimodal QA}}},
  shorttitle = {{{WebQA}}},
  author = {Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},
  year = {2022},
  month = mar,
  number = {arXiv:2109.00590},
  eprint = {arXiv:2109.00590},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.00590},
  urldate = {2023-01-24},
  abstract = {Scaling Visual Question Answering (VQA) to the open-domain and multi-hop nature of web searches, requires fundamental advances in visual representation learning, knowledge aggregation, and language generation. In this work, we introduce WebQA, a challenging new benchmark that proves difficult for large-scale state-of-the-art models which lack language groundable visual representations for novel objects and the ability to reason, yet trivial for humans. WebQA mirrors the way humans use the web: 1) Ask a question, 2) Choose sources to aggregate, and 3) Produce a fluent language response. This is the behavior we should be expecting from IoT devices and digital assistants. Existing work prefers to assume that a model can either reason about knowledge in images or in text. WebQA includes a secondary text-only QA task to ensure improved visual performance does not come at the cost of language understanding. Our challenge for the community is to create unified multimodal reasoning models that answer questions regardless of the source modality, moving us closer to digital assistants that not only query language knowledge, but also the richer visual online world.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/WW9SF2EJ/Chang et al. - 2022 - WebQA Multihop and Multimodal QA.pdf;/home/xav/Zotero/storage/G9UKLP2M/2109.html}
}

@inproceedings{chanScalableEvaluationCooperativeness2023,
  title = {Towards the {{Scalable Evaluation}} of {{Cooperativeness}} in {{Language Models}}},
  author = {Chan, Alan and Rich'e, Maxime and Clifton, Jesse},
  year = {2023},
  month = mar,
  urldate = {2023-03-25},
  abstract = {It is likely that AI systems driven by pre-trained language models (PLMs) will increasingly be used to assist humans in high-stakes interactions with other agents, such as negotiation or conflict resolution. Consistent with the goals of Cooperative AI \textbackslash citep\{dafoe\_open\_2020\}, we wish to understand and shape the multi-agent behaviors of PLMs in a pro-social manner. An important first step is the evaluation of model behaviour across diverse cooperation problems. Since desired behaviour in an interaction depends upon precise game-theoretic structure, we focus on generating scenarios with particular structures with both crowdworkers and a language model. Our work proceeds as follows. First, we discuss key methodological issues in the generation of scenarios corresponding to particular game-theoretic structures. Second, we employ both crowdworkers and a language model to generate such scenarios. We find that the quality of generations tends to be mediocre in both cases. We additionally get both crowdworkers and a language model to judge whether given scenarios align with their intended game-theoretic structure, finding mixed results depending on the game. Third, we provide a dataset of scenario based on our data generated. We provide both quantitative and qualitative evaluations of UnifiedQA and GPT-3 on this dataset. We find that instruct-tuned models tend to act in a way that could be perceived as cooperative when scaled up, while other models seemed to have flat scaling trends.}
}

@article{chebel-morelloCasebasedMaintenanceStructuring2015,
  title = {Case-Based Maintenance: {{Structuring}} and Incrementing the Case Base},
  shorttitle = {Case-Based Maintenance},
  author = {{Chebel-Morello}, B. and Haouchine, M. and Zerhouni, N.},
  year = {2015},
  journal = {Knowl. Based Syst.},
  doi = {10.1016/j.knosys.2015.07.034},
  abstract = {Semantic Scholar extracted view of "Case-based maintenance: Structuring and incrementing the case base" by B. Chebel-Morello et al.},
  file = {/home/xav/Zotero/storage/NURDMIBF/Chebel-Morello et al. - 2015 - Case-based maintenance Structuring and incrementi.pdf}
}

@article{chenAugmentingPretrainedLanguage2022,
  title = {Augmenting {{Pre-trained Language Models}} with {{QA-Memory}} for {{Open-Domain Question Answering}}},
  author = {Chen, Wenhu and Verga, Pat and Jong, M. D. and Wieting, J. and Cohen, W.},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2204.04581},
  abstract = {A new QA system is described which augments a text-to-text model with a large memory of question-answer pairs, and a new pretraining task for the latent step of question retrieval, which substantially simplifies training, and greatly improves performance on smaller QA benchmarks. Existing state-of-the-art methods for open-domain question-answering (ODQA) gener-ally used a open book approach, in which information is retrieved from a large text corpus or knowledge base (KB), and then rea-soned with to produce an answer. A recent alternative is to retrieve from a collection of previously-generated question-answer pairs. This has several practical advantages, including being more memory- and compute-efficient. Question-answer pairs are also appealing in that they seem to be an intermediate between text and KB triples: like KB triples, they usually concisely express a single rela-tionship, but like text, they have good coverage. We describe a new QA system which aug-ments a text-to-text model with a large memory of question-answer pairs, and a new pre-training task for the latent step of question retrieval. The pre-training task substantially sim-plifies training, and greatly improves performance on smaller QA benchmarks. Unlike prior systems of this sort, our QA system can also answer multi-hop questions that do not explicitly appear in the collection of stored question-answer pairs.},
  file = {/home/xav/Zotero/storage/52PTNQ79/Chen et al. - 2022 - Augmenting Pre-trained Language Models with QA-Mem.pdf}
}

@misc{chenConvergeTruthFactual2022,
  title = {Converge to the {{Truth}}: {{Factual Error Correction}} via {{Iterative Constrained Editing}}},
  shorttitle = {Converge to the {{Truth}}},
  author = {Chen, Jiangjie and Xu, Rui and Zeng, Wenxuan and Sun, Changzhi and Li, Lei and Xiao, Yanghua},
  year = {2022},
  month = dec,
  number = {arXiv:2211.12130},
  eprint = {arXiv:2211.12130},
  publisher = {{arXiv}},
  urldate = {2023-02-14},
  abstract = {Given a possibly false claim sentence, how can we automatically correct it with minimal editing? Existing methods either require a large number of pairs of false and corrected claims for supervised training or do not handle well errors spanning over multiple tokens within an utterance. In this paper, we propose VENCE, a novel method for factual error correction (FEC) with minimal edits. VENCE formulates the FEC problem as iterative sampling editing actions with respect to a target density function. We carefully design the target function with predicted truthfulness scores from an offline trained fact verification model. VENCE samples the most probable editing positions based on back-calculated gradients of the truthfulness score concerning input tokens and the editing actions using a distantly-supervised language model (T5). Experiments on a public dataset show that VENCE improves the well-adopted SARI metrics by 5.3 (or a relative improvement of 11.8\%) over the previous best distantlysupervised methods. Resources of VENCE can be found at https://github.com/jiangjiechen/VENCE.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/DI9CVJAE/Chen et al. - 2022 - Converge to the Truth Factual Error Correction vi.pdf}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {arXiv:2107.03374},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.03374},
  urldate = {2023-01-16},
  abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/F4TYFFKX/Chen et al. - 2021 - Evaluating Large Language Models Trained on Code.pdf;/home/xav/Zotero/storage/M93F3P5S/2107.html}
}

@inproceedings{chenEvaluatingQuestionAnswering2019,
  title = {Evaluating {{Question Answering Evaluation}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Machine Reading}} for {{Question Answering}}},
  author = {Chen, Anthony and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  year = {2019},
  month = nov,
  pages = {119--124},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-5817},
  urldate = {2023-01-22},
  abstract = {As the complexity of question answering (QA) datasets evolve, moving away from restricted formats like span extraction and multiple-choice (MC) to free-form answer generation, it is imperative to understand how well current metrics perform in evaluating QA. This is especially important as existing metrics (BLEU, ROUGE, METEOR, and F1) are computed using n-gram similarity and have a number of well-known drawbacks. In this work, we study the suitability of existing metrics in QA. For generative QA, we show that while current metrics do well on existing datasets, converting multiple-choice datasets into free-response datasets is challenging for current metrics. We also look at span-based QA, where F1 is a reasonable metric. We show that F1 may not be suitable for all extractive QA tasks depending on the answer types. Our study suggests that while current metrics may be suitable for existing QA datasets, they limit the complexity of QA datasets that can be created. This is especially true in the context of free-form QA, where we would like our models to be able to generate more complex and abstractive answers, thus necessitating new metrics that go beyond n-gram based matching. As a step towards a better QA metric, we explore using BERTScore, a recently proposed metric for evaluating translation, for QA. We find that although it fails to provide stronger correlation with human judgements, future work focused on tailoring a BERT-based metric to QA evaluation may prove fruitful.},
  file = {/home/xav/Zotero/storage/MLAULZRR/Chen et al. - 2019 - Evaluating Question Answering Evaluation.pdf}
}

@inproceedings{chenFedMatchFederatedLearning2021,
  title = {{{FedMatch}}: {{Federated Learning Over Heterogeneous Question Answering Data}}},
  shorttitle = {{{FedMatch}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Chen, Jiangui and Zhang, Ruqing and Guo, Jiafeng and Fan, Yixing and Cheng, Xueqi},
  year = {2021},
  month = oct,
  eprint = {2108.05069},
  primaryclass = {cs},
  pages = {181--190},
  doi = {10.1145/3459637.3482345},
  urldate = {2023-02-09},
  abstract = {Question Answering (QA), a popular and promising technique for intelligent information access, faces a dilemma about data as most other AI techniques. On one hand, modern QA methods rely on deep learning models which are typically data-hungry. Therefore, it is expected to collect and fuse all the available QA datasets together in a common site for developing a powerful QA model. On the other hand, real-world QA datasets are typically distributed in the form of isolated islands belonging to different parties. Due to the increasing awareness of privacy security, it is almost impossible to integrate the data scattered around, or the cost is prohibited. A possible solution to this dilemma is a new approach known as federated learning, which is a privacy-preserving machine learning technique over distributed datasets. In this work, we propose to adopt federated learning for QA with the special concern on the statistical heterogeneity of the QA data. Here the heterogeneity refers to the fact that annotated QA data are typically with non-identical and independent distribution (non-IID) and unbalanced sizes in practice. Traditional federated learning methods may sacrifice the accuracy of individual models under the heterogeneous situation. To tackle this problem, we propose a novel Federated Matching framework for QA, named FedMatch, with a backbone-patch architecture. The shared backbone is to distill the common knowledge of all the participants while the private patch is a compact and efficient module to retain the domain information for each participant. To facilitate the evaluation, we build a benchmark collection based on several QA datasets from different domains to simulate the heterogeneous situation in practice. Empirical studies demonstrate that our model can achieve significant improvements against the baselines over all the datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/ATFEKAUR/Chen et al. - 2021 - FedMatch Federated Learning Over Heterogeneous Qu.pdf;/home/xav/Zotero/storage/8PR2KYIN/2108.html}
}

@misc{chengMappingDesignSpace2022,
  title = {Mapping the {{Design Space}} of {{Human-AI Interaction}} in {{Text Summarization}}},
  author = {Cheng, Ruijia and {Smith-Renner}, Alison and Zhang, Ke and Tetreault, Joel R. and Jaimes, Alejandro},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14863},
  eprint = {2206.14863},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-25},
  abstract = {Automatic text summarization systems commonly involve humans for preparing data or evaluating model performance, yet, there lacks a systematic understanding of humans' roles, experience, and needs when interacting with or being assisted by AI. From a human-centered perspective, we map the design opportunities and considerations for human-AI interaction in text summarization and broader text generation tasks. We first conducted a systematic literature review of 70 papers, developing a taxonomy of five interactions in AI-assisted text generation and relevant design dimensions. We designed text summarization prototypes for each interaction. We then interviewed 16 users, aided by the prototypes, to understand their expectations, experience, and needs regarding efficiency, control, and trust with AI in text summarization and propose design considerations accordingly.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/xav/Zotero/storage/YIABE2KF/Cheng et al. - 2022 - Mapping the Design Space of Human-AI Interaction i.pdf}
}

@misc{chengMu2SLAM2022,
  title = {Mu\$\^\{2\}\${{SLAM}}: {{Multitask}}, {{Multilingual Speech}} and {{Language Models}}},
  shorttitle = {Mu\$\^\{2\}\${{SLAM}}},
  author = {Cheng, Yong and Zhang, Yu and Johnson, Melvin and Macherey, Wolfgang and Bapna, Ankur},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09553},
  eprint = {arXiv:2212.09553},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.09553},
  urldate = {2023-01-10},
  abstract = {We present Mu\$\^\{2\}\$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu\$\^\{2\}\$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu\$\^\{2\}\$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-T decoder, despite using a relatively weaker sequence-to-sequence architecture. On text understanding tasks, our model improves by more than 6\textbackslash\% over mSLAM on XNLI, getting closer to the performance of mT5 models of comparable capacity on XNLI and TydiQA, paving the way towards a single model for all speech and text understanding tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/xav/Zotero/storage/MGC9PBAY/Cheng et al. - 2022 - Mu$^ 2 $SLAM Multitask, Multilingual Speech and L.pdf}
}

@misc{chenImprovedBaselinesMomentum2020,
  title = {Improved {{Baselines}} with {{Momentum Contrastive Learning}}},
  author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  year = {2020},
  month = mar,
  number = {arXiv:2003.04297},
  eprint = {arXiv:2003.04297},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.04297},
  urldate = {2023-02-02},
  abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo---namely, using an MLP projection head and more data augmentation---we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/83PDHET5/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf;/home/xav/Zotero/storage/5HC2T64R/2003.html}
}

@misc{chenLargeLanguageModels2023,
  title = {Large {{Language Models}} Are Few(1)-Shot {{Table Reasoners}}},
  author = {Chen, Wenhu},
  year = {2023},
  month = jan,
  number = {arXiv:2210.06710},
  eprint = {arXiv:2210.06710},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.06710},
  urldate = {2023-02-07},
  abstract = {Recent literature has shown that large language models (LLMs) are generally excellent few-shot reasoners to solve text reasoning tasks. However, the capability of LLMs on table reasoning tasks is yet to be explored. In this paper, we aim at understanding how well LLMs can perform table-related tasks with few-shot in-context learning. Specifically, we evaluated LLMs on popular table QA and fact verification datasets like WikiTableQuestion, FetaQA, TabFact, and FEVEROUS and found that LLMs are competent at complex reasoning over table structures, though these models are not pre-trained on any table corpus. When combined with `chain of thoughts' prompting, LLMs can achieve very strong performance with only a 1-shot demonstration, even on par with some SoTA models. We show that LLMs are even more competent at generating comprehensive long-form answers on FetaQA than tuned T5-large. We further manually studied the reasoning chains elicited from LLMs and found that these reasoning chains are highly consistent with the underlying semantic form. We believe that LLMs can serve as a simple yet generic baseline for future research. The code and data are released in https://github.com/wenhuchen/TableCoT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/Q8VLCW5K/Chen - 2023 - Large Language Models are few(1)-shot Table Reason.pdf;/home/xav/Zotero/storage/MZASPLKP/2210.html}
}

@article{chenLORENLogicRegularizedReasoning2022,
  title = {{{LOREN}}: {{Logic-Regularized Reasoning}} for {{Interpretable Fact Verification}}},
  shorttitle = {{{LOREN}}},
  author = {Chen, Jiangjie and Bao, Qiaoben and Sun, Changzhi and Zhang, Xinbo and Chen, Jiaze and Zhou, Hao and Xiao, Yanghua and Li, Lei},
  year = {2022},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {36},
  number = {10},
  eprint = {2012.13577},
  primaryclass = {cs},
  pages = {10482--10491},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i10.21291},
  urldate = {2023-02-08},
  abstract = {Given a natural language statement, how to verify its veracity against a large-scale textual knowledge source like Wikipedia? Most existing neural models make predictions without giving clues about which part of a false claim goes wrong. In this paper, we propose LOREN, an approach for interpretable fact verification. We decompose the verification of the whole claim at phrase-level, where the veracity of the phrases serves as explanations and can be aggregated into the final verdict according to logical rules. The key insight of LOREN is to represent claim phrase veracity as three-valued latent variables, which are regularized by aggregation logical rules. The final claim verification is based on all latent variables. Thus, LOREN enjoys the additional benefit of interpretability -- it is easy to explain how it reaches certain results with claim phrase veracity. Experiments on a public fact verification benchmark show that LOREN is competitive against previous approaches while enjoying the merit of faithful and accurate interpretability. The resources of LOREN are available at: https://github.com/jiangjiechen/LOREN.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/MIKXYFFA/Chen et al. - 2022 - LOREN Logic-Regularized Reasoning for Interpretab.pdf;/home/xav/Zotero/storage/MGQ7F4J8/2012.html}
}

@article{chenLowresourceLearningKnowledge2021,
  title = {Low-Resource {{Learning}} with {{Knowledge Graphs}}: {{A Comprehensive Survey}}},
  shorttitle = {Low-Resource {{Learning}} with {{Knowledge Graphs}}},
  author = {Chen, Jiaoyan and Geng, Yuxia and Chen, Zhuo and Pan, Jeff Z. and He, Yuan and Zhang, Wen and Horrocks, Ian and Chen, Huajun},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.10006 [cs]},
  eprint = {2112.10006},
  primaryclass = {cs},
  urldate = {2022-05-09},
  abstract = {Machine learning methods especially deep neural networks have achieved great success but many of them often rely on a number of labeled samples for training. In real-world applications, we often need to address sample shortage due to e.g., dynamic contexts with emerging prediction targets and costly sample annotation. Therefore, low-resource learning, which aims to learn robust prediction models with no enough resources (especially training samples), is now being widely investigated. Among all the low-resource learning studies, many prefer to utilize some auxiliary information in the form of Knowledge Graph (KG), which is becoming more and more popular for knowledge representation, to reduce the reliance on labeled samples. In this survey, we very comprehensively reviewed over 90 papers about KG-aware research for two major low-resource learning settings \textemdash{} zero-shot learning (ZSL) where new classes for prediction have never appeared in training, and few-shot learning (FSL) where new classes for prediction have only a small number of labeled samples that are available. We first introduced the KGs used in ZSL and FSL studies as well as the existing and potential KG construction solutions, and then systematically categorized and summarized KG-aware ZSL and FSL methods, dividing them into different paradigms such as the mapping-based, the data augmentation, the propagation-based and the optimization-based. We next presented different applications, including not only KG augmented prediction tasks in Computer Vision and Natural Language Processing (e.g., image classification, visual question answering, text classification and knowledge extraction), but also tasks for KG curation (e.g., inductive KG completion), and some typical evaluation resources for each task. We eventually discussed some challenges and future directions on aspects such as new learning and reasoning paradigms, and the construction of high quality KGs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/SSG2I8NB/Chen et al. - 2021 - Low-resource Learning with Knowledge Graphs A Com.pdf}
}

@misc{chenOPALOntologyAwarePretrained2022,
  title = {{{OPAL}}: {{Ontology-Aware Pretrained Language Model}} for {{End-to-End Task-Oriented Dialogue}}},
  shorttitle = {{{OPAL}}},
  author = {Chen, Zhi and Liu, Yuncong and Chen, Lu and Zhu, Su and Wu, Mengyue and Yu, Kai},
  year = {2022},
  month = sep,
  number = {arXiv:2209.04595},
  eprint = {arXiv:2209.04595},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.04595},
  urldate = {2022-12-08},
  abstract = {This paper presents an ontology-aware pretrained language model (OPAL) for end-to-end task-oriented dialogue (TOD). Unlike chit-chat dialogue models, task-oriented dialogue models fulfill at least two task-specific modules: dialogue state tracker (DST) and response generator (RG). The dialogue state consists of the domain-slot-value triples, which are regarded as the user's constraints to search the domain-related databases. The large-scale task-oriented dialogue data with the annotated structured dialogue state usually are inaccessible. It prevents the development of the pretrained language model for the task-oriented dialogue. We propose a simple yet effective pretraining method to alleviate this problem, which consists of two pretraining phases. The first phase is to pretrain on large-scale contextual text data, where the structured information of the text is extracted by the information extracting tool. To bridge the gap between the pretraining method and downstream tasks, we design two pretraining tasks: ontology-like triple recovery and next-text generation, which simulates the DST and RG, respectively. The second phase is to fine-tune the pretrained model on the TOD data. The experimental results show that our proposed method achieves an exciting boost and get competitive performance even without any TOD data on CamRest676 and MultiWOZ benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BKN338BU/Chen et al. - 2022 - OPAL Ontology-Aware Pretrained Language Model for.pdf;/home/xav/Zotero/storage/W85NZC94/2209.html}
}

@misc{chenPerspectivesIncorporatingExpert2022,
  title = {Perspectives on {{Incorporating Expert Feedback}} into {{Model Updates}}},
  author = {Chen, Valerie and Bhatt, Umang and Heidari, Hoda and Weller, Adrian and Talwalkar, Ameet},
  year = {2022},
  month = jul,
  number = {arXiv:2205.06905},
  eprint = {2205.06905},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-30},
  abstract = {Machine learning (ML) practitioners are increasingly tasked with developing models that are aligned with non-technical experts' values and goals. However, there has been insufficient consideration on how practitioners should translate domain expertise into ML updates. In this paper, we consider how to capture interactions between practitioners and experts systematically. We devise a taxonomy to match expert feedback types with practitioner updates. A practitioner may receive feedback from an expert at the observation- or domain-level, and convert this feedback into updates to the dataset, loss function, or parameter space. We review existing work from ML and human-computer interaction to describe this feedback-update taxonomy, and highlight the insufficient consideration given to incorporating feedback from non-technical experts. We end with a set of open questions that naturally arise from our proposed taxonomy and subsequent survey.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/GDZCJL8G/Chen et al. - 2022 - Perspectives on Incorporating Expert Feedback into.pdf}
}

@misc{chenProgramThoughtsPrompting2022,
  title = {Program of {{Thoughts Prompting}}: {{Disentangling Computation}} from {{Reasoning}} for {{Numerical Reasoning Tasks}}},
  shorttitle = {Program of {{Thoughts Prompting}}},
  author = {Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W.},
  year = {2022},
  month = nov,
  number = {arXiv:2211.12588},
  eprint = {arXiv:2211.12588},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.12588},
  urldate = {2023-02-07},
  abstract = {Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\textbackslash\% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on financial datasets. All of our data and code are released in Github\textbackslash footnote\{\textbackslash url\{https://github.com/wenhuchen/Program-of-Thoughts\}\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/H3AWZBKI/Chen et al. - 2022 - Program of Thoughts Prompting Disentangling Compu.pdf;/home/xav/Zotero/storage/23PEK2EW/2211.html}
}

@misc{chenReadingWikipediaAnswer2017,
  title = {Reading {{Wikipedia}} to {{Answer Open-Domain Questions}}},
  author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  year = {2017},
  month = apr,
  number = {arXiv:1704.00051},
  eprint = {arXiv:1704.00051},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.00051},
  urldate = {2023-02-10},
  abstract = {This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/YDCHJPA6/Chen et al. - 2017 - Reading Wikipedia to Answer Open-Domain Questions.pdf;/home/xav/Zotero/storage/4DBENRMH/1704.html}
}

@inproceedings{chenRelationExtractionOpenbook2022,
  title = {Relation {{Extraction}} as {{Open-book Examination}}: {{Retrieval-enhanced Prompt Tuning}}},
  shorttitle = {Relation {{Extraction}} as {{Open-book Examination}}},
  booktitle = {Proceedings of the 45th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Chen, Xiang and Li, Lei and Zhang, Ningyu and Tan, Chuanqi and Huang, Fei and Si, Luo and Chen, Huajun},
  year = {2022},
  month = jul,
  eprint = {2205.02355},
  primaryclass = {cs},
  pages = {2443--2448},
  doi = {10.1145/3477495.3531746},
  urldate = {2022-08-25},
  abstract = {Pre-trained language models have contributed significantly to relation extraction by demonstrating remarkable few-shot learning abilities. However, prompt tuning methods for relation extraction may still fail to generalize to those rare or hard patterns. Note that the previous parametric learning paradigm can be viewed as memorization regarding training data as a book and inference as the close-book test. Those long-tailed or hard patterns can hardly be memorized in parameters given few-shot instances. To this end, we regard RE as an open-book examination and propose a new semiparametric paradigm of retrieval-enhanced prompt tuning for relation extraction. We construct an open-book datastore for retrieval regarding prompt-based instance representations and corresponding relation labels as memorized key-value pairs. During inference, the model can infer relations by linearly interpolating the base output of PLM with the non-parametric nearest neighbor distribution over the datastore. In this way, our model not only infers relation through knowledge stored in the weights during training but also assists decision-making by unwinding and querying examples in the open-book datastore. Extensive experiments on benchmark datasets show that our method can achieve state-of-the-art in both standard supervised and few-shot settings. Code are available in https://github.com/zjunlp/PromptKG/tree/main/research/RetrievalRE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/NYJWJRKF/Chen et al. - 2022 - Relation Extraction as Open-book Examination Retr.pdf}
}

@inproceedings{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = nov,
  pages = {1597--1607},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-02-02},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  langid = {english},
  file = {/home/xav/Zotero/storage/B8Y3XX6Z/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;/home/xav/Zotero/storage/HVTMDI6K/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf}
}

@misc{chenTHEXPrivacyPreservingTransformer2022,
  title = {{{THE-X}}: {{Privacy-Preserving Transformer Inference}} with {{Homomorphic Encryption}}},
  shorttitle = {{{THE-X}}},
  author = {Chen, Tianyu and Bao, Hangbo and Huang, Shaohan and Dong, Li and Jiao, Binxing and Jiang, Daxin and Zhou, Haoyi and Li, Jianxin and Wei, Furu},
  year = {2022},
  month = jun,
  number = {arXiv:2206.00216},
  eprint = {arXiv:2206.00216},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.00216},
  urldate = {2023-02-08},
  abstract = {As more and more pre-trained language models adopt on-cloud deployment, the privacy issues grow quickly, mainly for the exposure of plain-text user data (e.g., search history, medical record, bank account). Privacy-preserving inference of transformer models is on the demand of cloud service users. To protect privacy, it is an attractive choice to compute only with ciphertext in homomorphic encryption (HE). However, enabling pre-trained models inference on ciphertext data is difficult due to the complex computations in transformer blocks, which are not supported by current HE tools yet. In this work, we introduce \$\textbackslash textit\{THE-X\}\$, an approximation approach for transformers, which enables privacy-preserving inference of pre-trained models developed by popular frameworks. \$\textbackslash textit\{THE-X\}\$ proposes a workflow to deal with complex computation in transformer networks, including all the non-polynomial functions like GELU, softmax, and LayerNorm. Experiments reveal our proposed \$\textbackslash textit\{THE-X\}\$ can enable transformer inference on encrypted data for different downstream tasks, all with negligible performance drop but enjoying the theory-guaranteed privacy-preserving advantage.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security},
  file = {/home/xav/Zotero/storage/N3FP6NRH/Chen et al. - 2022 - THE-X Privacy-Preserving Transformer Inference wi.pdf;/home/xav/Zotero/storage/EWMUQFWT/2206.html}
}

@misc{chiInfoXLMInformationTheoreticFramework2021,
  title = {{{InfoXLM}}: {{An Information-Theoretic Framework}} for {{Cross-Lingual Language Model Pre-Training}}},
  shorttitle = {{{InfoXLM}}},
  author = {Chi, Zewen and Dong, Li and Wei, Furu and Yang, Nan and Singhal, Saksham and Wang, Wenhui and Song, Xia and Mao, Xian-Ling and Huang, Heyan and Zhou, Ming},
  year = {2021},
  month = apr,
  number = {arXiv:2007.07834},
  eprint = {arXiv:2007.07834},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {In this work, we present an informationtheoretic framework that formulates crosslingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pretraining task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/8LNTSI9A/Chi et al. - 2021 - InfoXLM An Information-Theoretic Framework for Cr.pdf}
}

@misc{chiuKnowledgeGroundedReinforcementLearning2022,
  title = {Knowledge-{{Grounded Reinforcement Learning}}},
  author = {Chiu, Zih-Yun and Tuan, Yi-Lin and Wang, William Yang and Yip, Michael C.},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03729},
  eprint = {arXiv:2210.03729},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03729},
  urldate = {2022-10-27},
  abstract = {Receiving knowledge, abiding by laws, and being aware of regulations are common behaviors in human society. Bearing in mind that reinforcement learning (RL) algorithms benefit from mimicking humanity, in this work, we propose that an RL agent can act on external guidance in both its learning process and model deployment, making the agent more socially acceptable. We introduce the concept, Knowledge-Grounded RL (KGRL), with a formal definition that an agent learns to follow external guidelines and develop its own policy. Moving towards the goal of KGRL, we propose a novel actor model with an embedding-based attention mechanism that can attend to either a learnable internal policy or external knowledge. The proposed method is orthogonal to training algorithms, and the external knowledge can be flexibly recomposed, rearranged, and reused in both training and inference stages. Through experiments on tasks with discrete and continuous action space, our KGRL agent is shown to be more sample efficient and generalizable, and it has flexibly rearrangeable knowledge embeddings and interpretable behaviors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/xav/Zotero/storage/GBPRDHIH/Chiu et al. - 2022 - Knowledge-Grounded Reinforcement Learning.pdf}
}

@misc{chiusanoCollection57Articles2022,
  title = {A Collection of 57 Articles with {{NLP}} Tips},
  author = {Chiusano, Fabio},
  year = {2022},
  month = mar,
  journal = {NLPlanet},
  urldate = {2022-05-18},
  abstract = {Embeddings, language models, knowledge graphs, chatbots, semantic search, etc.},
  langid = {english},
  file = {/home/xav/Zotero/storage/JQCESRRU/a-collection-of-57-articles-with-nlp-tips-4dbe80e4d473.html}
}

@misc{chiusanoHowDeepMindRETRO2021,
  title = {How the {{DeepMind RETRO}} Model Decouples Reasoning and Memorization},
  author = {Chiusano, Fabio},
  year = {2021},
  month = dec,
  journal = {NLPlanet},
  urldate = {2022-05-04},
  abstract = {In recent years, significant performance gains in language modeling have been achieved by increasing the number of parameters in Transformer models. This has led to a huge increase in training energy costs and resulted in a generation of large Language Models with 100+ billion parameters. At the same time, large datasets containing trillions of words have been collected to train these models. The benefits of increasing the number of parameters come from two factors:     More reasoning capabilities in the form of computations at training and inference time.     More memorization of the training data. DeepMind is exploring how to decouple these aspects, i.e. how to efficiently augment language models with a massive-scale memory without significantly increasing computations. Specifically, DeepMind suggests retrieval from a large text database as a complementary path to scaling language models. With this goal in mind, DeepMind introduced the Retrieval-Enhanced Transformer (RETRO) model: a language model that predicts the next words by conditioning on document chunks retrieved from a large corpus. How RETRO works Consider the sample query ``The 2021 Women's US Open was won''. A standard language model would predict a plausible continuation with the knowledge stored in the network parameters. RETRO, instead, looks for similar sequences in the Retrieval Database, withdraws their continuations, and conditions on them to predict a new plausible continuation.},
  langid = {english},
  file = {/home/xav/Zotero/storage/HMAEZ6R4/Chiusano - 2021 - Two minutes NLP ‚Äî How the DeepMind RETRO model dec.pdf;/home/xav/Zotero/storage/C6TMJYHX/two-minutes-nlp-how-the-deepmind-retro-model-decouples-reasoning-and-memorization-b393b6a8d07c.html}
}

@misc{chiusanoOpenAIInstructGPTBrings2022,
  title = {{{OpenAI InstructGPT}} Brings {{Reward Models}} to {{GPT-3}}},
  author = {Chiusano, Fabio},
  year = {2022},
  month = jan,
  journal = {NLPlanet},
  urldate = {2022-05-18},
  abstract = {OpenAI trained a new language model, called InstructGPT, that is better at following user intentions than GPT-3 while also being more truthful and less toxic, using techniques developed through their alignment research. Here are some example outputs from InstructGPT and GPT-3 from the same prompts. You can find more examples here. Example of prompt with GPT-3 and InstructGPT completions. Image from https://openai.com/blog/instruction-following. The procedure for training InstructGPT is the following:     OpenAI collected a dataset of prompts and labeler demonstrations of the desired model behavior and used it to fine-tune GPT-3.     Then, they collected a dataset of rankings of model outputs for these prompts, which are used to train a reward model.     Finally, the reward model is used to do further fine-tuning of the model from step 1 using the PPO algorithm. This method is called reinforcement learning from human feedback (RLHF), a method produced by OpenAI's alignment research. Finetuning models with reward models is important as the alignment problems are complex and subjective, and aren't fully captured by simple automatic metrics. Scheme of the training procedure of InstructGPT. Image from https://openai.com/blog/instruction-following. One way of thinking about this training process is that it ``unlocks'' capabilities that GPT-3 already had, but were difficult to elicit through prompt engineering alone. A limitation of this approach is that aligning the models only on specific tasks can make their performance worse on some other academic NLP tasks. However, this can be managed with a simple algorithmic change, that is by mixing in during RL fine-tuning a small fraction of the original data used to train GPT-3. In human evaluations, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Here are quality ratings of the models' outputs on a scale from one to seven. InstructGPT outputs are given much higher scores by the labelers than outputs from GPT-3 with a few-shot prompt and without, as well as models fine-tuned with supervised learning. Quality ratings of the models' outputs on a scale from one to seven. InstructGPT outputs are given much higher scores by the labelers than outputs from GPT-3 with a few-shot prompt and without, as well as models fine-tuned with supervised learning. Image from https://openai.com/blog/instruction-following. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. This can be proven using a suite of existing metrics on publicly available datasets. Compared to GPT-3, InstructGPT produces fewer imitative falsehoods and less toxic outputs. Moreover, InstructGPT makes up facts (``hallucinates'') less often. InstructGPT performance on toxic output generation, truthfulness, and hallucinations. Image from https://openai.com/blog/instruction-following. Fine-tuning language models with humans in the loops is proving to be able to create more safe and reliable language models with fewer parameters. OpenAI strongly believes in this research direction and plans to continue on its alignment research.},
  langid = {english},
  file = {/home/xav/Zotero/storage/I7NT9TGE/two-minutes-nlp-openai-instructgpt-brings-reward-models-to-gpt-3-b964ea8704ef.html}
}

@misc{chiusanoOpenAIWebGPTModel2021,
  title = {The {{OpenAI WebGPT}} Model That Answers Questions Browsing the Web},
  author = {Chiusano, Fabio},
  year = {2021},
  month = dec,
  journal = {NLPlanet},
  urldate = {2022-05-04},
  abstract = {The OpenAI team just presented WebGPT, a fine-tuned GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. The prototype copies how humans research answers to questions online: it submits search queries, follows links, and scrolls up and down web pages. It is trained to cite its sources, which makes it easier to give feedback to improve factual accuracy. WebGPT is part of the field of long-form question-answering (LFQA), in which a paragraph-length answer is generated in response to an open-ended question. LFQA systems have the potential to become one of the main ways people learn about the world, but currently lag behind human performance. Existing work tends to focus on two core components of the task: Information Retrieval and Text Synthesis. Instead of trying to improve these ingredients, OpenAI focused on combining them using more faithful training objectives. Answers are judged with human feedback for their factual accuracy, coherence, and overall usefulness. The WebGPT model WebGPT is made of several components. First, there is an Information Retrieval Model which accepts a question as input and outputs commands to issue to the text-based browser and their references, with the goal of extracting relevant text paragraphs that answer the question. It is trained on human demonstrations with Imitation Learning. The Information Retrieval model trained for WebGPT. Image by the author. Second, a Text Synthesis Model is trained to put together several text paragraphs into a single coherent text. GPT3 is able to do this with some finetuning. The Text Synthesis model trained for WebGPT. Image by the author. Third, we have a reward modeling step with a Reward Model that predicts a quality score from a question and an answer. It is trained with human feedbacks, where humans were helped by the references extracted by the Information Retrieval Model and the browser. The Reward model trained for WebGPT. Image by the author. The last step is to put together the three models and improve the whole pipeline leveraging the Reward Model. This can be done in two ways:     Finetuning the Information Retrieval Model against the Reward Model scores with Reinforcement Learning (specifically PPO) and some mitigations to avoid over-optimization on the Reward Model.     Predicting with Rejection Sampling (best-of-n), i.e. predicting multiple answers and selecting the one with the highest reward score.},
  langid = {english},
  file = {/home/xav/Zotero/storage/KYBG2CB6/Chiusano - 2021 - Two minutes NLP ‚Äî The OpenAI WebGPT model that ans.pdf;/home/xav/Zotero/storage/K8C64WAL/two-minutes-nlp-the-openai-webgpt-model-that-answers-questions-browsing-the-web-35f690884c25.html}
}

@misc{chiusanoSimpleTaxonomyKnowledge2021,
  title = {A Simple Taxonomy of {{Knowledge Graph}} Research},
  author = {Chiusano, Fabio},
  year = {2021},
  month = dec,
  journal = {NLPlanet},
  urldate = {2022-05-04},
  abstract = {Knowledge Representation Learning, Knowledge Acquisition, and Temporal Knowledge Graphs},
  langid = {english},
  file = {/home/xav/Zotero/storage/3YF4EWQY/two-minutes-nlp-a-simple-taxonomy-of-knowledge-graph-research-d28bb959486b.html}
}

@article{choiAppropriateQueryKey2021,
  title = {Towards an {{Appropriate Query}}, {{Key}}, and {{Value Computation}} for {{Knowledge Tracing}}},
  author = {Choi, Youngduck and Lee, Youngnam and Cho, Junghyun and Baek, Jineon and Kim, Byungsoo and Cha, Yeongmin and Shin, Dongmin and Bae, Chan and Heo, Jaewe},
  year = {2021},
  month = apr,
  journal = {LAK21: 11th International Learning Analytics and Knowledge Conference},
  eprint = {2002.07033},
  pages = {490--496},
  doi = {10.1145/3448139.3448188},
  urldate = {2022-05-04},
  abstract = {Knowledge tracing, the act of modeling a student's knowledge through learning activities, is an extensively studied problem in the field of computer-aided education. Although models with attention mechanism have outperformed traditional approaches such as Bayesian knowledge tracing and collaborative filtering, they share two limitations. Firstly, the models rely on shallow attention layers and fail to capture complex relations among exercises and responses over time. Secondly, different combinations of queries, keys and values for the self-attention layer for knowledge tracing were not extensively explored. Usual practice of using exercises and interactions (exercise-response pairs) as queries and keys/values respectively lacks empirical support. In this paper, we propose a novel Transformer based model for knowledge tracing, SAINT: Separated Self-AttentIve Neural Knowledge Tracing. SAINT has an encoder-decoder structure where exercise and response embedding sequence separately enter the encoder and the decoder respectively, which allows to stack attention layers multiple times. To the best of our knowledge, this is the first work to suggest an encoder-decoder model for knowledge tracing that applies deep self-attentive layers to exercises and responses separately. The empirical evaluations on a large-scale knowledge tracing dataset show that SAINT achieves the state-of-the-art performance in knowledge tracing with the improvement of AUC by 1.8\% compared to the current state-of-the-art models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/6PQAHWWC/Choi et al. - 2021 - Towards an Appropriate Query, Key, and Value Compu.pdf;/home/xav/Zotero/storage/7Q7KJGUW/2002.html}
}

@misc{choiQuACQuestionAnswering2018,
  title = {{{QuAC}} : {{Question Answering}} in {{Context}}},
  shorttitle = {{{QuAC}}},
  author = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
  year = {2018},
  month = aug,
  number = {arXiv:1808.07036},
  eprint = {arXiv:1808.07036},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.07036},
  urldate = {2023-01-23},
  abstract = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/J5SRXAG5/Choi et al. - 2018 - QuAC  Question Answering in Context.pdf;/home/xav/Zotero/storage/P645D92H/1808.html}
}

@misc{choPromptAugmentedLinearProbing2022,
  title = {Prompt-{{Augmented Linear Probing}}: {{Scaling Beyond The Limit}} of {{Few-shot In-Context Learners}}},
  shorttitle = {Prompt-{{Augmented Linear Probing}}},
  author = {Cho, Hyunsoo and Kim, Hyuhng Joon and Kim, Junyeob and Lee, Sang-Woo and Lee, Sang-goo and Yoo, Kang Min and Kim, Taeuk},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10873},
  eprint = {arXiv:2212.10873},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10873},
  urldate = {2023-02-14},
  abstract = {Through in-context learning (ICL), large-scale language models are effective few-shot learners without additional model fine-tuning. However, the ICL performance does not scale well with the number of available training samples as it is limited by the inherent input length constraint of the underlying language model. Meanwhile, many studies have revealed that language models are also powerful feature extractors, allowing them to be utilized in a black-box manner and enabling the linear probing paradigm, where lightweight discriminators are trained on top of the pre-extracted input representations. This paper proposes prompt-augmented linear probing (PALP), a hybrid of linear probing and ICL, which leverages the best of both worlds. PALP inherits the scalability of linear probing and the capability of enforcing language models to derive more meaningful representations via tailoring input into a more conceivable form. Throughout in-depth investigations on various datasets, we verified that PALP significantly enhances the input representations closing the gap between ICL in the data-hungry scenario and fine-tuning in the data-abundant scenario with little training overhead, potentially making PALP a strong alternative in a black-box scenario.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/IQUR3DTP/Cho et al. - 2022 - Prompt-Augmented Linear Probing Scaling Beyond Th.pdf;/home/xav/Zotero/storage/GYNZYJP5/2212.html}
}

@misc{choubeyCaPEContrastiveParameter2022,
  title = {{{CaPE}}: {{Contrastive Parameter Ensembling}} for {{Reducing Hallucination}} in {{Abstractive Summarization}}},
  shorttitle = {{{CaPE}}},
  author = {Choubey, Prafulla Kumar and Fabbri, Alexander R. and Vig, Jesse and Wu, Chien-Sheng and Liu, Wenhao and Rajani, Nazneen Fatema},
  year = {2022},
  month = may,
  number = {arXiv:2110.07166},
  eprint = {arXiv:2110.07166},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.07166},
  urldate = {2023-02-13},
  abstract = {Hallucination is a known issue for neural abstractive summarization models. Recent work suggests that the degree of hallucination may depend on errors in the training data. In this work, we propose a new method called Contrastive Parameter Ensembling (CaPE) to use training data more effectively, utilizing variations in noise in training samples to reduce hallucination. We first select clean and noisy subsets from the training data using different automatic factual metrics. Then, we fine-tune a base summarization model, which is trained on all training samples, on the clean (noisy) subset to obtain an \textbackslash textit\{expert\} (\textbackslash textit\{anti-expert\}) model. Finally, we adjust the parameters of base model by the difference between parameters of the \textbackslash textit\{expert\} and \textbackslash textit\{anti-expert\} models, steering the base model towards the \textbackslash textit\{expert\} model and away from the \textbackslash textit\{anti-expert\} model. Experimental results show that CaPE improves performance across different automatic factual metrics and human evaluation, with the maximum improvement of 16.69\textbackslash\% and 15.78\textbackslash\% on summary-level dependency-arc entailment accuracy for the XSUM and CNN/DM datasets. The improvement in factual performance does not degrade the performance on other metrics of informativeness such as ROUGE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/QGEP2ZKP/Choubey et al. - 2022 - CaPE Contrastive Parameter Ensembling for Reducin.pdf;/home/xav/Zotero/storage/ZCZ3RZ5F/2110.html}
}

@article{choubeyMoFEMixtureFactual2021,
  title = {{{MoFE}}: {{Mixture}} of {{Factual Experts}} for {{Controlling Hallucinations}} in {{Abstractive Summarization}}},
  shorttitle = {{{MoFE}}},
  author = {Choubey, Prafulla Kumar and Vig, Jesse and Liu, Wenhao and Rajani, Nazneen},
  year = {2021},
  month = oct,
  urldate = {2023-02-13},
  abstract = {Neural abstractive summarization models are susceptible to generating factually inconsistent content, a phenomenon known as hallucination. This limits the usability and adoption of these systems in real-world applications. To reduce the presence of hallucination, we propose the Mixture of Factual Experts (MoFE) model, which combines multiple summarization experts that each target a specific type of error. We train our experts using reinforcement learning (RL) to minimize the error defined by two factual consistency metrics: entity overlap and dependency arc entailment. We construct MoFE by combining the experts using two ensembling strategies (weights and logits) and evaluate them on two summarization datasets (XSUM and CNN/DM). Our experiments on BART models show that the MoFE improves performance according to both entity overlap and dependency arc entailment, without a significant performance drop on standard ROUGE metrics. The performance improvement also transfers to unseen factual consistency metrics, such as question answer-based factuality evaluation metric and BERTScore precision with respect to the source document.},
  langid = {english},
  file = {/home/xav/Zotero/storage/6WYN2BJB/Choubey et al. - 2021 - MoFE Mixture of Factual Experts for Controlling H.pdf}
}

@article{chowdheryPaLMScalingLanguage2022,
  title = {{{PaLM}}: {{Scaling Language Modeling}} with {{Pathways}}},
  shorttitle = {{{PaLM}}},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and {Gur-Ari}, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and {Meier-Hellstern}, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.02311 [cs]},
  eprint = {2204.02311},
  primaryclass = {cs},
  urldate = {2022-05-10},
  abstract = {Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model (PaLM).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {QID: Q111971083},
  file = {/home/xav/Zotero/storage/Z3HFMNN5/Chowdhery et al. - 2022 - PaLM Scaling Language Modeling with Pathways.pdf}
}

@misc{chowdhuryCQASUMMBuildingReferences2018,
  title = {{{CQASUMM}}: {{Building References}} for {{Community Question Answering Summarization Corpora}}},
  shorttitle = {{{CQASUMM}}},
  author = {Chowdhury, Tanya and Chakraborty, Tanmoy},
  year = {2018},
  month = nov,
  number = {arXiv:1811.04884},
  eprint = {arXiv:1811.04884},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.04884},
  urldate = {2022-09-06},
  abstract = {Community Question Answering forums such as Quora, Stackoverflow are rich knowledge resources, often catering to information on topics overlooked by major search engines. Answers submitted to these forums are often elaborated, contain spam, are marred by slurs and business promotions. It is difficult for a reader to go through numerous such answers to gauge community opinion. As a result summarization becomes a prioritized task for CQA forums. While a number of efforts have been made to summarize factoid CQA, little work exists in summarizing non-factoid CQA. We believe this is due to the lack of a considerably large, annotated dataset for CQA summarization. We create CQASUMM, the first huge annotated CQA summarization dataset by filtering the 4.4 million Yahoo! Answers L6 dataset. We sample threads where the best answer can double up as a reference summary and build hundred word summaries from them. We treat other answers as candidates documents for summarization. We provide a script to generate the dataset and introduce the new task of Community Question Answering Summarization. Multi document summarization has been widely studied with news article datasets, especially in the DUC and TAC challenges using news corpora. However documents in CQA have higher variance, contradicting opinion and lesser amount of overlap. We compare the popular multi document summarization techniques and evaluate their performance on our CQA corpora. We look into the state-of-the-art and understand the cases where existing multi document summarizers (MDS) fail. We find that most MDS workflows are built for the entirely factual news corpora, whereas our corpus has a fair share of opinion based instances too. We therefore introduce OpinioSumm, a new MDS which outperforms the best baseline by 4.6\% w.r.t ROUGE-1 score.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/CGL7X5TN/Chowdhury et Chakraborty - 2018 - CQASUMM Building References for Community Questio.pdf;/home/xav/Zotero/storage/A2CH2WM9/1811.html}
}

@article{chowdhuryNoveltyControlledParaphrase2022,
  title = {Novelty {{Controlled Paraphrase Generation}} with {{Retrieval Augmented Conditional Prompt Tuning}}},
  author = {Chowdhury, Jishnu Ray and Zhuang, Yong and Wang, Shuyi},
  year = {2022},
  month = mar,
  journal = {arXiv:2202.00535 [cs]},
  eprint = {2202.00535},
  primaryclass = {cs},
  urldate = {2022-05-14},
  abstract = {Paraphrase generation is a fundamental and long-standing task in natural language processing. In this paper, we concentrate on two contributions to the task: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a parameterefficient method to adapt large pre-trained language models for paraphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a simple model-agnostic method of using specialized prompt tokens for controlled paraphrase generation with varying levels of lexical novelty. By conducting extensive experiments on four datasets, we demonstrate the effectiveness of the proposed approaches for retaining the semantic content of the original text while inducing lexical novelty in the generation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/XWCQVLSX/Chowdhury et al. - 2022 - Novelty Controlled Paraphrase Generation with Retr.pdf}
}

@misc{christmannConversationalQuestionAnswering2022,
  title = {Conversational {{Question Answering}} on {{Heterogeneous Sources}}},
  author = {Christmann, Philipp and Roy, Rishiraj Saha and Weikum, Gerhard},
  year = {2022},
  month = apr,
  number = {arXiv:2204.11677},
  eprint = {arXiv:2204.11677},
  publisher = {{arXiv}},
  urldate = {2022-06-24},
  abstract = {Table 1: Question understanding approaches for ConvQA.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/HQ49C7YB/Christmann et al. - 2022 - Conversational Question Answering on Heterogeneous.pdf}
}

@inproceedings{christmannNEDFastEffective2022,
  title = {Beyond {{NED}}: {{Fast}} and {{Effective Search Space Reduction}} for {{Complex Question Answering}} over {{Knowledge Bases}}},
  shorttitle = {Beyond {{NED}}},
  booktitle = {Proceedings of the {{Fifteenth ACM International Conference}} on {{Web Search}} and {{Data Mining}}},
  author = {Christmann, Philipp and Saha Roy, Rishiraj and Weikum, Gerhard},
  year = {2022},
  month = feb,
  series = {{{WSDM}} '22},
  pages = {172--180},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3488560.3498488},
  urldate = {2022-05-04},
  abstract = {Answering complex questions over knowledge bases (KB-QA) faces huge input data with billions of facts, involving millions of entities and thousands of predicates. For efficiency, QA systems first reduce the answer search space by identifying a set of facts that is likely to contain all answers and relevant cues. The most common technique for doing this is to apply named entity disambiguation (NED) on the question, and retrieve KB facts for the disambiguated entities. This work presents CLOCQ, an efficient method that prunes irrelevant parts of the search space using KB-aware signals. CLOCQ uses a top-k query processor over score-ordered lists of KB items that combine signals about lexical matching, relevance to the question, coherence among candidate items, and connectivity in the KB graph. Experiments with two recent QA benchmarks for complex questions demonstrate the superiority of CLOCQ over state-of-the-art baselines with respect to answer presence, size of the search space, and runtimes.},
  isbn = {978-1-4503-9132-0},
  keywords = {entity linking,knowledge bases,question answering},
  file = {/home/xav/Zotero/storage/B53QQ3VK/Christmann et al. - 2022 - Beyond NED Fast and Effective Search Space Reduct.pdf}
}

@article{chronopoulouEfficientHierarchicalDomain2022,
  title = {Efficient {{Hierarchical Domain Adaptation}} for {{Pretrained Language Models}}},
  author = {Chronopoulou, Alexandra and Peters, Matthew E. and Dodge, Jesse},
  year = {2022},
  month = may,
  journal = {arXiv:2112.08786 [cs]},
  eprint = {2112.08786},
  primaryclass = {cs},
  urldate = {2022-05-17},
  abstract = {The remarkable success of large language models has been driven by dense models trained on massive unlabeled, unstructured corpora. These corpora typically contain text from diverse, heterogeneous sources, but information about the source of the text is rarely used during training. Transferring their knowledge to a target domain is typically done by continuing training in-domain. In this paper, we introduce a method to permit domain adaptation to many diverse domains using a computationally efficient adapter approach. Our method is based on the observation that textual domains are partially overlapping, and we represent domains as a hierarchical tree structure where each node in the tree is associated with a set of adapter weights. When combined with a frozen pretrained language model, this approach enables parameter sharing among related domains, while avoiding negative interference between unrelated ones. Experimental results with GPT-2 and a large fraction of the 100 most represented websites in C4 show across-the-board improvements in-domain. We additionally provide an inference time algorithm for a held-out domain and show that averaging over multiple paths through the tree enables further gains in generalization, while adding only a marginal cost to inference.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BAUZVWJU/Chronopoulou et al. - 2022 - Efficient Hierarchical Domain Adaptation for Pretr.pdf;/home/xav/Zotero/storage/PBQVM5VC/2112.html}
}

@article{chungScalingInstructionFinetunedLanguage2022,
  title = {Scaling {{Instruction-Finetuned Language Models}}},
  author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
  year = {2022},
  month = oct,
  doi = {10.48550/arXiv.2210.11416},
  urldate = {2022-11-06},
  abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For instance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot MMLU. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
  langid = {english},
  file = {/home/xav/Zotero/storage/5ZY3WVAU/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf}
}

@misc{CIKMConferenceInformation,
  title = {{{CIKM}} | {{Conference}} on {{Information}} and {{Knowledge Management}}},
  urldate = {2023-02-15},
  howpublished = {https://www.cikm2022.org/},
  file = {/home/xav/Zotero/storage/DGY4QMCT/www.cikm2022.org.html}
}

@inproceedings{clarkTransformersSoftReasoners2020,
  title = {Transformers as {{Soft Reasoners}} over {{Language}}},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
  year = {2020},
  month = jul,
  pages = {3882--3890},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/537},
  urldate = {2022-06-25},
  abstract = {Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99\%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95\%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.},
  isbn = {978-0-9992411-6-5},
  langid = {english},
  file = {/home/xav/Zotero/storage/FKB25XXY/Clark et al. - 2020 - Transformers as Soft Reasoners over Language.pdf}
}

@misc{clarkTyDiQABenchmark2020,
  title = {{{TyDi QA}}: {{A Benchmark}} for {{Information-Seeking Question Answering}} in {{Typologically Diverse Languages}}},
  shorttitle = {{{TyDi QA}}},
  author = {Clark, Jonathan H. and Choi, Eunsol and Collins, Michael and Garrette, Dan and Kwiatkowski, Tom and Nikolaev, Vitaly and Palomaki, Jennimaria},
  year = {2020},
  month = mar,
  number = {arXiv:2003.05002},
  eprint = {arXiv:2003.05002},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.05002},
  urldate = {2023-03-07},
  abstract = {Confidently making progress on multilingual modeling requires challenging, trustworthy evaluations. We present TyDi QA---a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs. The languages of TyDi QA are diverse with regard to their typology---the set of linguistic features each language expresses---such that we expect models performing well on this set to generalize across a large number of the world's languages. We present a quantitative analysis of the data quality and example-level qualitative linguistic analyses of observed language phenomena that would not be found in English-only corpora. To provide a realistic information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but don't know the answer yet, and the data is collected directly in each language without the use of translation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/HUHCCBZY/Clark et al. - 2020 - TyDi QA A Benchmark for Information-Seeking Quest.pdf;/home/xav/Zotero/storage/57QTNLJU/2003.html}
}

@misc{CLEFQATrack,
  title = {{{CLEF QA}} Track},
  urldate = {2023-02-15},
  howpublished = {http://nlp.uned.es/clef-qa/},
  file = {/home/xav/Zotero/storage/YT4TBAGH/clef-qa.html}
}

@misc{cobbeTrainingVerifiersSolve2021,
  title = {Training {{Verifiers}} to {{Solve Math Word Problems}}},
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  year = {2021},
  month = nov,
  number = {arXiv:2110.14168},
  eprint = {arXiv:2110.14168},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.14168},
  urldate = {2023-01-16},
  abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/2WKS5KTX/Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf;/home/xav/Zotero/storage/V97L74NG/2110.html}
}

@misc{coenenWordcraftHumanAICollaborative2021,
  title = {Wordcraft: A {{Human-AI Collaborative Editor}} for {{Story Writing}}},
  shorttitle = {Wordcraft},
  author = {Coenen, Andy and Davis, Luke and Ippolito, Daphne and Reif, Emily and Yuan, Ann},
  year = {2021},
  month = jul,
  number = {arXiv:2107.07430},
  eprint = {2107.07430},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2107.07430},
  urldate = {2022-06-02},
  abstract = {As neural language models grow in effectiveness, they are increasingly being applied in real-world settings. However these applications tend to be limited in the modes of interaction they support. In this extended abstract, we propose Wordcraft, an AI-assisted editor for story writing in which a writer and a dialog system collaborate to write a story. Our novel interface uses few-shot learning and the natural affordances of conversation to support a variety of interactions. Our editor provides a sandbox for writers to probe the boundaries of transformer-based language models and paves the way for future human-in-the-loop training pipelines and novel evaluation methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/RZY8SJ9D/Coenen et al. - 2021 - Wordcraft a Human-AI Collaborative Editor for Sto.pdf}
}

@inproceedings{colasTutorialVQAQuestionAnswering2020,
  title = {{{TutorialVQA}}: {{Question Answering Dataset}} for {{Tutorial Videos}}},
  shorttitle = {{{TutorialVQA}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Colas, Anthony and Kim, Seokhwan and Dernoncourt, Franck and Gupte, Siddhesh and Wang, Zhe and Kim, Doo Soon},
  year = {2020},
  month = may,
  pages = {5450--5455},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  urldate = {2023-01-24},
  abstract = {Despite the number of currently available datasets on video-question answering, there still remains a need for a dataset involving multi-step and non-factoid answers. Moreover, relying on video transcripts remains an under-explored topic. To adequately address this, we propose a new question answering task on instructional videos, because of their verbose and narrative nature. While previous studies on video question answering have focused on generating a short text as an answer, given a question and video clip, our task aims to identify a span of a video segment as an answer which contains instructional details with various granularities. This work focuses on screencast tutorial videos pertaining to an image editing program. We introduce a dataset, TutorialVQA, consisting of about 6,000 manually collected triples of (video, question, answer span). We also provide experimental results with several baseline algorithms using the video transcripts. The results indicate that the task is challenging and call for the investigation of new algorithms.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  file = {/home/xav/Zotero/storage/9CXW36EP/Colas et al. - 2020 - TutorialVQA Question Answering Dataset for Tutori.pdf}
}

@misc{ConceptNet,
  title = {{{ConceptNet}}},
  urldate = {2022-05-04},
  abstract = {ConceptNet is a semantic network based on the information in the OMCS database. ConceptNet is expressed as a directed graph whose nodes are concepts, and whose edges are assertions of common sense about these concepts. Concepts represent sets of closely related natural language phrases, which could be noun phrases, verb phrases, adjective phrases, or clauses.[5] ConceptNet is created from the natural-language assertions in OMCS by matching them against patterns using a shallow parser. Assertions are expressed as relations between two concepts, selected from a limited set of possible relations. The various relations represent common sentence patterns found in the OMCS corpus, and in particular, every "fill-in-the-blanks" template used on the knowledge-collection Web site is associated with a particular relation.[5] The data structures that make up ConceptNet were significantly reorganized in 2007, and published as ConceptNet 3.[5] The Software Agents group currently distributes a database and API for the new version 4.0.[6] In 2010, OMCS co-founder and director Catherine Havasi, with Robyn Speer, Dennis Clark and Jason Alonso, created Luminoso, a text analytics software company that builds on ConceptNet.[7][8][9][10] It uses ConceptNet as its primary lexical resource in order to help businesses make sense of and derive insight from vast amounts of qualitative data, including surveys, product reviews and social media.[7][11][12] Open Mind Common Sense (OMCS) is an artificial intelligence project based at the Massachusetts Institute of Technology (MIT) Media Lab whose goal is to build and utilize a large commonsense knowledge base from the contributions of many thousands of people across the Web. It has been active from 1999 to 2016. Since its founding, it has accumulated more than a million English facts from over 15,000 contributors in addition to knowledge bases in other languages. Much of OMCS's software is built on three interconnected representations: the natural language corpus that people interact with directly, a semantic network built from this corpus called ConceptNet, and a matrix-based representation of ConceptNet called AnalogySpace that can infer new knowledge using dimensionality reduction.[1] The knowledge collected by Open Mind Common Sense has enabled research projects at MIT and elsewhere.},
  howpublished = {https://conceptnet.io/},
  file = {/home/xav/Zotero/storage/LIVHXLQE/conceptnet.io.html}
}

@phdthesis{cortesroblesManagementInnovationTechnologique2006,
  title = {{Management de l'innovation technologique et des connaissances : synergie entre la th\'eorie TRIZ et le Raisonnement \`a Partir de Cas. Application en g\'enie des proc\'ed\'es et syst\`emes industriels.}},
  shorttitle = {{Management de l'innovation technologique et des connaissances}},
  author = {Cortes Robles, Guillermo},
  year = {2006},
  month = jul,
  urldate = {2022-05-14},
  abstract = {Les m\'ethodologies traditionnelles de r\'esolution de probl\`emes comme le brainstorming, la synectique, la m\'ethode d'essais-erreurs, etc. trouvent leurs limites lorsqu'elles sont confront\'ees \`a un probl\`eme inventif ou probl\`eme contenant une contradiction (conditions sous laquelle deux exigences ou besoins d'un syst\`eme sont mutuellement exclusives, mais doivent \^etre associ\'es afin d'atteindre un m\^eme objectif). Ce type de probl\`emes est g\'en\'eralement r\'esolu en acceptant un important degr\'e de compromis et avec une direction al\'eatoire de recherche d'une solution, qui se traduit par une efficacit\'e inferieure \`a celle atteignable par d'autres m\'ethodes. L'arriv\'ee de la th\'eorie de r\'esolution des probl\`emes inventifs ou th\'eorie TRIZ a \'elimin\'e ces inconv\'enients. Cette approche, bas\'ee sur l'\'evolution historique des syst\`emes techniques, a produit une vision o\`u l'innovation est consid\'er\'ee comme une ressource ma\^itrisable pouvant \^etre appliqu\'ee syst\'ematiquement. Paradoxalement, cette approche bas\'ee sur la connaissance et applicable \`a n'importe quelle discipline ou domaine, ne poss\`ede pas la capacit\'e \`a m\'emoriser, ce qui s'av\`ere indispensable pour l'apprentissage. De ce fait, les connaissances qui ont servi et qui ont \'et\'e cr\'e\'ees lors de la r\'esolution d'une contradiction, ne peuvent pas \^etre r\'eutilis\'ees. Cet inconv\'enient est un facteur n\'egatif pour la performance de TRIZ lors de la r\'esolution d'un probl\`eme inventif. Parall\`element, une autre approche a d\'evelopp\'e la capacit\'e \`a identifier, stocker et r\'eutiliser la connaissance : la gestion des connaissances. Cette capacit\'e est mise en \oe uvre dans le raisonnement \`a partir de cas (R\`aPC). Cette approche, dont l'efficacit\'e est bas\'ee sur l'aspect sp\'ecifique au domaine d'application, utilise les exp\'eriences acquises pendant la r\'esolution des probl\`emes pass\'es, afin d'aborder la r\'esolution des probl\`emes nouveaux. Toutefois, le R\`aPC, de par sa nature, ne prend pas en consid\'eration les solutions qui ont \'et\'e d\'evelopp\'ees dans d'autres domaines. Cela limite fortement la capacit\'e du R\`aPC \`a proposer une solution nouvelle ou innovante. De plus, la m\'emoire du syst\`eme ne peut pas apporter une solution pour une situation qui n'a pas \'et\'e identifi\'ee et r\'esolue auparavant. Les limites et la compl\'ementarit\'e constat\'ees dans les deux approches, ont servi de base pour la proposition d'un nouveau mod\`ele. Ce mod\`ele offre une approche qui combine la vision technologique de la th\'eorie TRIZ et la capacit\'e \`a m\'emoriser et r\'eutiliser la connaissance d\'evelopp\'ee par le raisonnement \`a partir de cas. La synergie entre ces deux approches permet d'un c\^ot\'e, de diriger les efforts cr\'eatifs lors de la r\'esolution d'un probl\`eme inventif et de l'autre, de r\'eutiliser la connaissance d\'eploy\'ee lors de ce processus. La capacit\'e de cette nouvelle approche est illustr\'ee au travers d'\'etudes de cas issues du g\'enie des proc\'ed\'es et du g\'enie industriel.},
  copyright = {info:eu-repo/semantics/openAccess},
  langid = {french},
  file = {/home/xav/Zotero/storage/EZ9KBW9P/Cortes Robles - 2006 - Management de l'innovation technologique et des co.pdf;/home/xav/Zotero/storage/PCX5PG7A/7523.html}
}

@misc{creswellFaithfulReasoningUsing2022,
  title = {Faithful {{Reasoning Using Large Language Models}}},
  author = {Creswell, Antonia and Shanahan, Murray},
  year = {2022},
  month = aug,
  number = {arXiv:2208.14271},
  eprint = {arXiv:2208.14271},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.14271},
  urldate = {2022-09-08},
  abstract = {Although contemporary large language models (LMs) demonstrate impressive question-answering capabilities, their answers are typically the product of a single call to the model. This entails an unwelcome degree of opacity and compromises performance, especially on problems that are inherently multi-step. To address these limitations, we show how LMs can be made to perform faithful multi-step reasoning via a process whose causal structure mirrors the underlying logical structure of the problem. Our approach works by chaining together reasoning steps, where each step results from calls to two fine-tuned LMs, one for selection and one for inference, to produce a valid reasoning trace. Our method carries out a beam search through the space of reasoning traces to improve reasoning quality. We demonstrate the effectiveness of our model on multi-step logical deduction and scientific question-answering, showing that it outperforms baselines on final answer accuracy, and generates humanly interpretable reasoning traces whose validity can be checked by the user.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BICZVVMZ/Creswell et Shanahan - 2022 - Faithful Reasoning Using Large Language Models.pdf}
}

@misc{creswellSelectionInferenceExploitingLarge2022,
  title = {Selection-{{Inference}}: {{Exploiting Large Language Models}} for {{Interpretable Logical Reasoning}}},
  shorttitle = {Selection-{{Inference}}},
  author = {Creswell, Antonia and Shanahan, Murray and Higgins, Irina},
  year = {2022},
  month = may,
  number = {arXiv:2205.09712},
  eprint = {arXiv:2205.09712},
  publisher = {{arXiv}},
  urldate = {2023-01-29},
  abstract = {Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100\% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/WMRGEVGZ/Creswell et al. - 2022 - Selection-Inference Exploiting Large Language Mod.pdf}
}

@article{crubezyConfiguringOnlineProblemSolving,
  title = {Configuring {{Online Problem-Solving Resources}} with the {{Internet Reasoning Service}}},
  author = {Crub{\'e}zy, Monica and Motta, Enrico and Lu, Wenjin and Musen, Mark A},
  pages = {15},
  abstract = {Many resources on the World-Wide Web are now dynamic services that provide some functionality for end users. Existing service-providing resources, however, tend to be ``integral.'' For instance, online services for data analysis are available, but usually it is neither possible to modify the underlying reasoning system, nor to configure it for a different domain, nor to integrate different services to produce new functionalities. Our research goal is to develop the technological framework needed to provide sophisticated online problem-solving resources, configurable for different applications. We describe the design and initial implementation of the Internet Reasoning Service\textemdash a Web-based front-end enabling developers to prototype knowledge-based applications quickly out of reusable problemsolving resources in distributed libraries. Our aim is to make reliable problem-solving technology available to a wider audience and to provide the level of intelligent support that allows rapid generation of Web-based knowledge applications.},
  langid = {english},
  annotation = {QID: Q57983381},
  file = {/home/xav/Zotero/storage/NPWULHBI/Crub√©zy et al. - Configuring Online Problem-Solving Resources with .pdf}
}

@article{crubezyPSMLibrarianConfiguring,
  title = {The {{PSM Librarian}}: {{Configuring Problem-solving Applications}} with {{Prot\'eg\'e}}},
  author = {Crub{\'e}zy, Monica},
  pages = {33},
  langid = {english},
  file = {/home/xav/Zotero/storage/8DFYG6RC/Crub√©zy - The PSM Librarian Configuring Problem-solving App.pdf}
}

@article{cuadraOPENINGBLACKBOX1967,
  title = {{{OPENING THE BLACK BOX OF}} `{{RELEVANCE}}'},
  author = {CUADRA, CARLOS A. and KATTER, ROBERT V.},
  year = {1967},
  month = jan,
  journal = {Journal of Documentation},
  volume = {23},
  number = {4},
  pages = {291--303},
  publisher = {{MCB UP Ltd}},
  issn = {0022-0418},
  doi = {10.1108/eb026436},
  urldate = {2023-02-13},
  abstract = {The purpose of this project was to identify variables thought to affect relevance judgments and conduct a series of laboratory studies to determine the effects of these variables on relevance judgments. This paper discusses the variable of `implicit use orientations'\textemdash the particular attitude taken by a subject (judge) about the intended use of a document. One hundred and forty judges rated each of nine abstracts for relevance to several short information requirement statements. The same judges then repeated the ratings, each adopting (assuming) one of fourteen use orientations described to them. It was found that the particular use orientation assumed by the judge has a marked effect on relevance judgments. It was also found that implicit use orientations can be analysed in terms of their underlying structure and that their study offers the possibility of discovering the conditions under which one person can accurately simulate and use the implicit use orientations of another. Relevance judgments have been used as a basis for measures designed to evaluate the effectiveness of information retrieval systems. These judgments have usually been accepted at face value and have not been subjected to critical scrutiny. There is reason to believe, however, that as ordinarily obtained, they may be unreliable and sensitive to a number of conditions of measurement that have not been carefully controlled in previous evaluation studies.},
  annotation = {QID: Q58630292},
  file = {/home/xav/Zotero/storage/EV3DBIWR/html.html}
}

@article{cuiDocumentAIBenchmarks2021,
  title = {Document {{AI}}: {{Benchmarks}}, {{Models}} and {{Applications}}},
  shorttitle = {Document {{AI}}},
  author = {Cui, Lei and Xu, Yiheng and Lv, Tengchao and Wei, Furu},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.08609 [cs]},
  eprint = {2111.08609},
  primaryclass = {cs},
  urldate = {2022-05-17},
  abstract = {Document AI, or Document Intelligence, is a relatively new research topic that refers to the techniques for automatically reading, understanding, and analyzing business documents. It is an important research direction for natural language processing and computer vision. In recent years, the popularity of deep learning technology has greatly advanced the development of Document AI, such as document layout analysis, visual information extraction, document visual question answering, document image classification, etc. This paper briefly reviews some of the representative models, tasks, and benchmark datasets. Furthermore, we also introduce early-stage heuristic rule-based document analysis, statistical machine learning algorithms, and deep learning approaches especially pre-training methods. Finally, we look into future directions for Document AI research.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/UGKE7XZ4/Cui et al. - 2021 - Document AI Benchmarks, Models and Applications.pdf}
}

@misc{daiDialogInpaintingTurning2022,
  title = {Dialog {{Inpainting}}: {{Turning Documents}} into {{Dialogs}}},
  shorttitle = {Dialog {{Inpainting}}},
  author = {Dai, Zhuyun and Chaganty, Arun Tejasvi and Zhao, Vincent and Amini, Aida and Rashid, Qazi Mamunur and Green, Mike and Guu, Kelvin},
  year = {2022},
  month = may,
  number = {arXiv:2205.09073},
  eprint = {arXiv:2205.09073},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.09073},
  urldate = {2022-09-02},
  abstract = {Many important questions (e.g. "How to eat healthier?") require conversation to establish context and explore in depth. However, conversational question answering (ConvQA) systems have long been stymied by scarce training data that is expensive to collect. To address this problem, we propose a new technique for synthetically generating diverse and high-quality dialog data: dialog inpainting. Our approach takes the text of any document and transforms it into a two-person dialog between the writer and an imagined reader: we treat sentences from the article as utterances spoken by the writer, and then use a dialog inpainter to predict what the imagined reader asked or said in between each of the writer's utterances. By applying this approach to passages from Wikipedia and the web, we produce WikiDialog and WebDialog, two datasets totalling 19 million diverse information-seeking dialogs -- 1,000x larger than the largest existing ConvQA dataset. Furthermore, human raters judge the answer adequacy and conversationality of WikiDialog to be as good or better than existing manually-collected datasets. Using our inpainted data to pre-train ConvQA retrieval systems, we significantly advance state-of-the-art across three benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40\% relative gains on standard evaluation metrics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/XJVJFVNK/Dai et al. - 2022 - Dialog Inpainting Turning Documents into Dialogs.pdf}
}

@misc{daiPromptagatorFewshotDense2022,
  title = {Promptagator: {{Few-shot Dense Retrieval From}} 8 {{Examples}}},
  shorttitle = {Promptagator},
  author = {Dai, Zhuyun and Zhao, Vincent Y. and Ma, Ji and Luan, Yi and Ni, Jianmo and Lu, Jing and Bakalov, Anton and Guu, Kelvin and Hall, Keith B. and Chang, Ming-Wei},
  year = {2022},
  month = sep,
  number = {arXiv:2209.11755},
  eprint = {arXiv:2209.11755},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.11755},
  urldate = {2022-09-28},
  abstract = {Much recent research on information retrieval has focused on how to transfer from one task (typically with abundant supervised data) to various other tasks where supervision is limited, with the implicit assumption that it is possible to generalize from one task to all the rest. However, this overlooks the fact that there are many diverse and unique retrieval tasks, each targeting different search intents, queries, and search domains. In this paper, we suggest to work on Few-shot Dense Retrieval, a setting where each task comes with a short description and a few examples. To amplify the power of a few examples, we propose Prompt-base Query Generation for Retriever (Promptagator), which leverages large language models (LLM) as a few-shot query generator, and creates task-specific retrievers based on the generated data. Powered by LLM's generalization ability, Promptagator makes it possible to create task-specific end-to-end retrievers solely based on a few examples \{without\} using Natural Questions or MS MARCO to train \%question generators or dual encoders. Surprisingly, LLM prompting with no more than 8 examples allows dual encoders to outperform heavily engineered models trained on MS MARCO like ColBERT v2 by more than 1.2 nDCG on average on 11 retrieval sets. Further training standard-size re-rankers using the same generated data yields another 5.0 point nDCG improvement. Our studies determine that query generation can be far more effective than previously observed, especially when a small amount of task-specific knowledge is given.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/IAKULGFS/Dai et al. - 2022 - Promptagator Few-shot Dense Retrieval From 8 Exam.pdf}
}

@misc{daniels-kochExpertiseProblemLearning2022,
  title = {The {{Expertise Problem}}: {{Learning}} from {{Specialized Feedback}}},
  shorttitle = {The {{Expertise Problem}}},
  author = {{Daniels-Koch}, Oliver and Freedman, Rachel},
  year = {2022},
  month = nov,
  number = {arXiv:2211.06519},
  eprint = {arXiv:2211.06519},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.06519},
  urldate = {2022-12-07},
  abstract = {Reinforcement learning from human feedback (RLHF) is a powerful technique for training agents to perform difficult-to-specify tasks. However, human feedback can be noisy, particularly when human teachers lack relevant knowledge or experience. Levels of expertise vary across teachers, and a given teacher may have differing levels of expertise for different components of a task. RLHF algorithms that learn from multiple teachers therefore face an expertise problem: the reliability of a given piece of feedback depends both on the teacher that it comes from and how specialized that teacher is on relevant components of the task. Existing state-of-the-art RLHF algorithms assume that all evaluations come from the same distribution, obscuring this inter- and intra-human variance, and preventing them from accounting for or taking advantage of variations in expertise. We formalize this problem, implement it as an extension of an existing RLHF benchmark, evaluate the performance of a state-of-the-art RLHF algorithm, and explore techniques to improve query and teacher selection. Our key contribution is to demonstrate and characterize the expertise problem, and to provide an open-source implementation for testing future solutions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/TFCE8S2Q/Daniels-Koch et Freedman - 2022 - The Expertise Problem Learning from Specialized F.pdf;/home/xav/Zotero/storage/ZAQ3MIXD/2211.html}
}

@misc{dasCasebasedReasoningNatural2021,
  title = {Case-Based {{Reasoning}} for {{Natural Language Queries}} over {{Knowledge Bases}}},
  author = {Das, Rajarshi and Zaheer, Manzil and Thai, Dung and Godbole, Ameya and Perez, Ethan and Lee, Jay-Yoon and Tan, Lizhen and Polymenakos, Lazaros and McCallum, Andrew},
  year = {2021},
  month = nov,
  number = {arXiv:2104.08762},
  eprint = {arXiv:2104.08762},
  publisher = {{arXiv}},
  urldate = {2022-09-01},
  abstract = {It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions -- a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the ComplexWebQuestions dataset, CBR-KBQA outperforms the current state of the art by 11\textbackslash\% on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases \textbackslash emph\{without\} any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/FUDWH5IT/Das et al. - 2021 - Case-based Reasoning for Natural Language Queries .pdf;/home/xav/Zotero/storage/6LV2FUAH/2104.html}
}

@misc{dasguptaLanguageModelsShow2022,
  title = {Language Models Show Human-like Content Effects on Reasoning},
  author = {Dasgupta, Ishita and Lampinen, Andrew K. and Chan, Stephanie C. Y. and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L. and Hill, Felix},
  year = {2022},
  month = jul,
  number = {arXiv:2207.07051},
  eprint = {arXiv:2207.07051},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.07051},
  urldate = {2022-10-29},
  abstract = {Abstract reasoning is a key ability for an intelligent system. Large language models achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect, and depends on our knowledge and beliefs about the content of the reasoning problem. For example, humans reason much more reliably about logical rules that are grounded in everyday situations than arbitrary rules about abstract attributes. The training experiences of language models similarly endow them with prior expectations that reflect human knowledge and beliefs. We therefore hypothesized that language models would show human-like content effects on abstract reasoning problems. We explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (Wason, 1968). We find that state of the art large language models (with 7 or 70 billion parameters; Hoffman et al., 2022) reflect many of the same patterns observed in humans across these tasks -- like humans, models reason more effectively about believable situations than unrealistic or abstract ones. Our findings have implications for understanding both these cognitive effects, and the factors that contribute to language model performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/VT5ELR9Q/Dasgupta et al. - 2022 - Language models show human-like content effects on.pdf;/home/xav/Zotero/storage/FEG9N4FH/2207.html}
}

@misc{dasigiDatasetInformationSeekingQuestions2021,
  title = {A {{Dataset}} of {{Information-Seeking Questions}} and {{Answers Anchored}} in {{Research Papers}}},
  author = {Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A. and Gardner, Matt},
  year = {2021},
  month = may,
  number = {arXiv:2105.03011},
  eprint = {arXiv:2105.03011},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.03011},
  urldate = {2023-02-09},
  abstract = {Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present QASPER, a dataset of 5,049 questions over 1,585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/HE3ZRKG3/Dasigi et al. - 2021 - A Dataset of Information-Seeking Questions and Ans.pdf;/home/xav/Zotero/storage/EDYJQXAD/2105.html}
}

@inproceedings{dasigiQuorefReadingComprehension2019,
  title = {Quoref: {{A Reading Comprehension Dataset}} with {{Questions Requiring Coreferential Reasoning}}},
  shorttitle = {Quoref},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Dasigi, Pradeep and Liu, Nelson F. and Marasovi{\'c}, Ana and Smith, Noah A. and Gardner, Matt},
  year = {2019},
  month = nov,
  pages = {5925--5932},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1606},
  urldate = {2023-02-01},
  abstract = {Machine comprehension of texts longer than a single sentence often requires coreference resolution. However, most current reading comprehension benchmarks do not contain complex coreferential phenomena and hence fail to evaluate the ability of models to resolve coreference. We present a new crowdsourced dataset containing more than 24K span-selection questions that require resolving coreference among entities in over 4.7K English paragraphs from Wikipedia. Obtaining questions focused on such phenomena is challenging, because it is hard to avoid lexical cues that shortcut complex reasoning. We deal with this issue by using a strong baseline model as an adversary in the crowdsourcing loop, which helps crowdworkers avoid writing questions with exploitable surface cues. We show that state-of-the-art reading comprehension models perform significantly worse than humans on this benchmark\textemdash the best model performance is 70.5 F1, while the estimated human performance is 93.4 F1.},
  file = {/home/xav/Zotero/storage/LWFRGKQI/Dasigi et al. - 2019 - Quoref A Reading Comprehension Dataset with Quest.pdf}
}

@misc{DataDrivenDesignbyAnalogyState,
  title = {Data-{{Driven Design-by-Analogy}}: {{State}} of the {{Art}} and {{Future Directions}}},
  shorttitle = {Papers with {{Code}} - {{Paper}} Tables with Annotated Results for {{Data-Driven Design-by-Analogy}}},
  urldate = {2022-05-14},
  abstract = {Paper tables with annotated results for Data-Driven Design-by-Analogy: State of the Art and Future Directions},
  howpublished = {https://paperswithcode.com/paper/data-driven-design-by-analogy-state-of-the/review/},
  langid = {english},
  file = {/home/xav/Zotero/storage/7RH5YYIX/Data-Driven Design-by-Analogy State of the Art an.pdf;/home/xav/Zotero/storage/A9TGT2GA/review.html}
}

@misc{debBoostingNaturalLanguage2022,
  title = {Boosting {{Natural Language Generation}} from {{Instructions}} with {{Meta-Learning}}},
  author = {Deb, Budhaditya and Zheng, Guoqing and Awadallah, Ahmed Hassan},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11617},
  eprint = {arXiv:2210.11617},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11617},
  urldate = {2022-12-25},
  abstract = {Recent work has shown that language models (LMs) trained with multi-task \textbackslash textit\{instructional learning\} (MTIL) can solve diverse NLP tasks in zero- and few-shot settings with improved performance compared to prompt tuning. MTIL illustrates that LMs can extract and use information about the task from instructions beyond the surface patterns of the inputs and outputs. This suggests that meta-learning may further enhance the utilization of instructions for effective task transfer. In this paper we investigate whether meta-learning applied to MTIL can further improve generalization to unseen tasks in a zero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in three directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network (HNet) based adaptation to generate task specific parameters conditioned on instructions, and 3) an approach combining HNet and MAML. Through extensive experiments on the large scale Natural Instructions V2 dataset, we show that our proposed approaches significantly improve over strong baselines in zero-shot settings. In particular, meta-learning improves the effectiveness of instructions and is most impactful when the test tasks are strictly zero-shot (i.e. no similar tasks in the training set) and are "hard" for LMs, illustrating the potential of meta-learning for MTIL for out-of-distribution tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/2HAPGFJM/Deb et al. - 2022 - Boosting Natural Language Generation from Instruct.pdf}
}

@misc{DeepmindResearch,
  title = {Deepmind {{Research}}},
  urldate = {2023-02-15},
  abstract = {Our pioneering research includes Deep Learning, Reinforcement Learning, Theory \& Foundations, Neuroscience, Unsupervised Learning \& Generative Models, Control \& Robotics, and Safety.},
  howpublished = {https://www.deepmind.com/research},
  langid = {english},
  file = {/home/xav/Zotero/storage/CZ2QULUA/research.html}
}

@misc{dejongFiDOFusioninDecoderOptimized2022,
  title = {{{FiDO}}: {{Fusion-in-Decoder}} Optimized for Stronger Performance and Faster Inference},
  shorttitle = {{{FiDO}}},
  author = {{de Jong}, Michiel and Zemlyanskiy, Yury and Ainslie, Joshua and FitzGerald, Nicholas and Sanghai, Sumit and Sha, Fei and Cohen, William},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08153},
  eprint = {arXiv:2212.08153},
  publisher = {{arXiv}},
  urldate = {2022-12-21},
  abstract = {Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, FiD suffers from very expensive inference. We show that the majority of inference time results from memory bandwidth constraints in the decoder, and propose two simple changes to the FiD architecture to speed up inference by 7x. The faster decoder inference then allows for a much larger decoder. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ECYJ4QS2/de Jong et al. - 2022 - FiDO Fusion-in-Decoder optimized for stronger per.pdf}
}

@misc{dejongPrecomputedMemoryOnthefly2023,
  title = {Pre-Computed Memory or on-the-Fly Encoding? {{A}} Hybrid Approach to Retrieval Augmentation Makes the Most of Your Compute},
  shorttitle = {Pre-Computed Memory or on-the-Fly Encoding?},
  author = {{de Jong}, Michiel and Zemlyanskiy, Yury and FitzGerald, Nicholas and Ainslie, Joshua and Sanghai, Sumit and Sha, Fei and Cohen, William},
  year = {2023},
  month = jan,
  number = {arXiv:2301.10448},
  eprint = {arXiv:2301.10448},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.10448},
  urldate = {2023-02-01},
  abstract = {Retrieval-augmented language models such as Fusion-in-Decoder are powerful, setting the state of the art on a variety of knowledge-intensive tasks. However, they are also expensive, due to the need to encode a large number of retrieved passages. Some work avoids this cost by pre-encoding a text corpus into a memory and retrieving dense representations directly. However, pre-encoding memory incurs a severe quality penalty as the memory representations are not conditioned on the current input. We propose LUMEN, a hybrid between these two extremes, pre-computing the majority of the retrieval representation and completing the encoding on the fly using a live encoder that is conditioned on the question and fine-tuned for the task. We show that LUMEN significantly outperforms pure memory on multiple question-answering tasks while being much cheaper than FiD, and outperforms both for any given compute budget. Moreover, the advantage of LUMEN over FiD increases with model size.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/LSJFPMBZ/de Jong et al. - 2023 - Pre-computed memory or on-the-fly encoding A hybr.pdf}
}

@article{dengMultihopInferenceQuestiondriven2020,
  title = {Multi-Hop {{Inference}} for {{Question-driven Summarization}}},
  author = {Deng, Yang and Zhang, Wenxuan and Lam, Wai},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.03738 [cs]},
  eprint = {2010.03738},
  primaryclass = {cs},
  urldate = {2022-04-06},
  abstract = {Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions. In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries. Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer. A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives. Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/J9IIUQGE/Deng et al. - 2020 - Multi-hop Inference for Question-driven Summarizat.pdf;/home/xav/Zotero/storage/QXJ8UJTE/2010.html}
}

@misc{dengRLPromptOptimizingDiscrete2022,
  title = {{{RLPrompt}}: {{Optimizing Discrete Text Prompts}} with {{Reinforcement Learning}}},
  shorttitle = {{{RLPrompt}}},
  author = {Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric P. and Hu, Zhiting},
  year = {2022},
  month = oct,
  number = {arXiv:2205.12548},
  eprint = {arXiv:2205.12548},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.12548},
  urldate = {2022-12-07},
  abstract = {Prompting has shown impressive success in enabling large pretrained language models (LMs) to perform diverse NLP tasks, especially when only few downstream data are available. Automatically finding the optimal prompt for each task, however, is challenging. Most existing work resorts to tuning soft prompt (e.g., embeddings) which falls short of interpretability, reusability across LMs, and applicability when gradients are not accessible. Discrete prompt, on the other hand, is difficult to optimize, and is often created by "enumeration (e.g., paraphrasing)-then-selection" heuristics that do not explore the prompt space systematically. This paper proposes RLPrompt, an efficient discrete prompt optimization approach with reinforcement learning (RL). RLPrompt formulates a parameter-efficient policy network that generates the desired discrete prompt after training with reward. To overcome the complexity and stochasticity of reward signals by the large LM environment, we incorporate effective reward stabilization that substantially enhances the training efficiency. RLPrompt is flexibly applicable to different types of LMs, such as masked (e.g., BERT) and left-to-right models (e.g., GPTs), for both classification and generation tasks. Experiments on few-shot classification and unsupervised text style transfer show superior performance over a wide range of existing finetuning or prompting methods. Interestingly, the resulting optimized prompts are often ungrammatical gibberish text; and surprisingly, those gibberish prompts are transferrable between different LMs to retain significant performance, indicating LM prompting may not follow human language patterns.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/8U4IZMDD/Deng et al. - 2022 - RLPrompt Optimizing Discrete Text Prompts with Re.pdf;/home/xav/Zotero/storage/2SN3IJBS/2205.html}
}

@article{departmentofcomputerscienceandengineeringsrminstituteofscienceandtechnologykattankulathurchennaiindiaTopicModelingBased2020,
  title = {Topic {{Modeling Based Extractive Text Summarization}}},
  author = {{Department of Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur, Chennai, India} and Issam, Kalliath Abdul Rasheed and Patel*, Shivam and {Department of Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur, Chennai, India.} and N., Subalalitha C. and {Department of Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur, Chennai, India.}},
  year = {2020},
  month = apr,
  journal = {International Journal of Innovative Technology and Exploring Engineering},
  volume = {9},
  number = {6},
  pages = {1710--1719},
  issn = {22783075},
  doi = {10.35940/ijitee.F4611.049620},
  urldate = {2022-05-13},
  abstract = {Text summarization is an approach for identifying important information present within text documents. This computational technique aims to generate shorter versions of the source text, by including only the relevant and salient information present within the source text. In this paper, we propose a novel method to summarize a text document by clustering its contents based on latent topics produced using topic modeling techniques and by generating extractive summaries for each of the identified text clusters. All extractive sub-summaries are later combined to generate a summary for any given source document. We utilize the lesser used and challenging WikiHow dataset in our approach to text summarization. This dataset is unlike the commonly used news datasets which are available for text summarization. The well-known news datasets present their most important information in the first few lines of their source texts, which make their summarization a lesser challenging task when compared to summarizing the WikiHow dataset. Contrary to these news datasets, the documents in the WikiHow dataset are written using a generalized approach and have lesser abstractedness and higher compression ratio, thus proposing a greater challenge to generate summaries. A lot of the current state-of-the-art text summarization techniques tend to eliminate important information present in source documents in the favor of brevity. Our proposed technique aims to capture all the varied information present in source documents. Although the dataset proved challenging, after performing extensive tests within our experimental setup, we have discovered that our model produces encouraging ROUGE results and summaries when compared to the other published extractive and abstractive text summarization models.},
  langid = {english},
  file = {/home/xav/Zotero/storage/XGY47M2J/Department of Computer Science and Engineering, SRM Institute of Science and Technology, Kattankulathur, Chennai, India et al. - 2020 - Topic Modeling Based Extractive Text Summarization.pdf}
}

@misc{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {arXiv:1810.04805},
  publisher = {{arXiv}},
  urldate = {2022-07-19},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/EMABZNDG/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@misc{dholeNLAugmenterFrameworkTaskSensitive2022,
  title = {{{NL-Augmenter}}: {{A Framework}} for {{Task-Sensitive Natural Language Augmentation}}},
  shorttitle = {{{NL-Augmenter}}},
  author = {Dhole, Kaustubh D. and Gangal, Varun and Gehrmann, Sebastian and Gupta, Aadesh and Li, Zhenhao and Mahamood, Saad and Mahendiran, Abinaya and Mille, Simon and Shrivastava, Ashish and Tan, Samson and Wu, Tongshuang and {Sohl-Dickstein}, Jascha and Choi, Jinho D. and Hovy, Eduard and Dusek, Ondrej and Ruder, Sebastian and Anand, Sajant and Aneja, Nagender and Banjade, Rabin and Barthe, Lisa and Behnke, Hanna and {Berlot-Attwell}, Ian and Boyle, Connor and Brun, Caroline and Cabezudo, Marco Antonio Sobrevilla and Cahyawijaya, Samuel and Chapuis, Emile and Che, Wanxiang and Choudhary, Mukund and Clauss, Christian and Colombo, Pierre and Cornell, Filip and Dagan, Gautier and Das, Mayukh and Dixit, Tanay and Dopierre, Thomas and Dray, Paul-Alexis and Dubey, Suchitra and Ekeinhor, Tatiana and Di Giovanni, Marco and Goyal, Tanya and Gupta, Rishabh and Gupta, Rishabh and Hamla, Louanes and Han, Sang and {Harel-Canada}, Fabrice and Honore, Antoine and Jindal, Ishan and Joniak, Przemyslaw K. and Kleyko, Denis and Kovatchev, Venelin and Krishna, Kalpesh and Kumar, Ashutosh and Langer, Stefan and Lee, Seungjae Ryan and Levinson, Corey James and Liang, Hualou and Liang, Kaizhao and Liu, Zhexiong and Lukyanenko, Andrey and Marivate, Vukosi and {de Melo}, Gerard and Meoni, Simon and Meyer, Maxime and Mir, Afnan and Moosavi, Nafise Sadat and Muennighoff, Niklas and Mun, Timothy Sum Hon and Murray, Kenton and Namysl, Marcin and Obedkova, Maria and Oli, Priti and Pasricha, Nivranshu and Pfister, Jan and Plant, Richard and Prabhu, Vinay and Pais, Vasile and Qin, Libo and Raji, Shahab and Rajpoot, Pawan Kumar and Raunak, Vikas and Rinberg, Roy and Roberts, Nicolas and Rodriguez, Juan Diego and Roux, Claude and S., Vasconcellos P. H. and Sai, Ananya B. and Schmidt, Robin M. and Scialom, Thomas and Sefara, Tshephisho and Shamsi, Saqib N. and Shen, Xudong and Shi, Haoyue and Shi, Yiwen and Shvets, Anna and Siegel, Nick and Sileo, Damien and Simon, Jamie and Singh, Chandan and Sitelew, Roman and Soni, Priyank and Sorensen, Taylor and Soto, William and Srivastava, Aman and Srivatsa, KV Aditya and Sun, Tony and T, Mukund Varma and Tabassum, A. and Tan, Fiona Anting and Teehan, Ryan and Tiwari, Mo and Tolkiehn, Marie and Wang, Athena and Wang, Zijian and Wang, Gloria and Wang, Zijie J. and Wei, Fuxuan and Wilie, Bryan and Winata, Genta Indra and Wu, Xinyi and Wydma{\'n}ski, Witold and Xie, Tianbao and Yaseen, Usama and Yee, Michael A. and Zhang, Jing and Zhang, Yue},
  year = {2022},
  month = oct,
  number = {arXiv:2112.02721},
  eprint = {arXiv:2112.02721},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.02721},
  urldate = {2022-11-21},
  abstract = {Data augmentation is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of the data they are trained on. In this paper, we present NL-Augmenter, a new participatory Python-based natural language augmentation framework which supports the creation of both transformations (modifications to the data) and filters (data splits according to specific features). We describe the framework and an initial set of 117 transformations and 23 filters for a variety of natural language tasks. We demonstrate the efficacy of NL-Augmenter by using several of its transformations to analyze the robustness of popular natural language models. The infrastructure, datacards and robustness analysis results are available publicly on the NL-Augmenter repository (https://github.com/GEM-benchmark/NL-Augmenter).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/W6E2EAHR/Dhole et al. - 2022 - NL-Augmenter A Framework for Task-Sensitive Natur.pdf;/home/xav/Zotero/storage/BJ974I29/2112.html}
}

@misc{diaoActivePromptingChainofThought2023,
  title = {Active {{Prompting}} with {{Chain-of-Thought}} for {{Large Language Models}}},
  author = {Diao, Shizhe and Wang, Pengcheng and Lin, Yong and Zhang, Tong},
  year = {2023},
  month = feb,
  number = {arXiv:2302.12246},
  eprint = {arXiv:2302.12246},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.12246},
  urldate = {2023-02-24},
  abstract = {The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-cot.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/89HLANQH/Diao et al. - 2023 - Active Prompting with Chain-of-Thought for Large L.pdf}
}

@misc{dingDeltaTuningComprehensive2022,
  title = {Delta {{Tuning}}: {{A Comprehensive Study}} of {{Parameter Efficient Methods}} for {{Pre-trained Language Models}}},
  shorttitle = {Delta {{Tuning}}},
  author = {Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and Yi, Jing and Zhao, Weilin and Wang, Xiaozhi and Liu, Zhiyuan and Zheng, Hai-Tao and Chen, Jianfei and Liu, Yang and Tang, Jie and Li, Juanzi and Sun, Maosong},
  year = {2022},
  month = mar,
  number = {arXiv:2203.06904},
  eprint = {arXiv:2203.06904},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.06904},
  urldate = {2022-12-07},
  abstract = {Despite the success, the process of fine-tuning large-scale PLMs brings prohibitive adaptation costs. In fact, fine-tuning all the parameters of a colossal model and retaining separate instances for different tasks are practically infeasible. This necessitates a new branch of research focusing on the parameter-efficient adaptation of PLMs, dubbed as delta tuning in this paper. In contrast with the standard fine-tuning, delta tuning only fine-tunes a small portion of the model parameters while keeping the rest untouched, largely reducing both the computation and storage costs. Recent studies have demonstrated that a series of delta tuning methods with distinct tuned parameter selection could achieve performance on a par with full-parameter fine-tuning, suggesting a new promising way of stimulating large-scale PLMs. In this paper, we first formally describe the problem of delta tuning and then comprehensively review recent delta tuning approaches. We also propose a unified categorization criterion that divide existing delta tuning methods into three groups: addition-based, specification-based, and reparameterization-based methods. Though initially proposed as an efficient method to steer large models, we believe that some of the fascinating evidence discovered along with delta tuning could help further reveal the mechanisms of PLMs and even deep neural networks. To this end, we discuss the theoretical principles underlying the effectiveness of delta tuning and propose frameworks to interpret delta tuning from the perspective of optimization and optimal control, respectively. Furthermore, we provide a holistic empirical study of representative methods, where results on over 100 NLP tasks demonstrate a comprehensive performance comparison of different approaches. The experimental results also cover the analysis of combinatorial, scaling and transferable properties of delta tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/FZ9RFPUJ/Ding et al. - 2022 - Delta Tuning A Comprehensive Study of Parameter E.pdf;/home/xav/Zotero/storage/IL9I8XSA/2203.html}
}

@misc{dohanLanguageModelCascades2022,
  title = {Language {{Model Cascades}}},
  author = {Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A. and {Sohl-dickstein}, Jascha and Murphy, Kevin and Sutton, Charles},
  year = {2022},
  month = jul,
  number = {arXiv:2207.10342},
  eprint = {arXiv:2207.10342},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.10342},
  urldate = {2022-08-22},
  abstract = {Prompted models have demonstrated impressive few-shot learning abilities. Repeated interactions at test-time with a single model, or the composition of multiple models together, further expands capabilities. These compositions are probabilistic models, and may be expressed in the language of graphical models with random variables whose values are complex data types such as strings. Cases with control flow and dynamic structure require techniques from probabilistic programming, which allow implementing disparate model structures and inference strategies in a unified language. We formalize several existing techniques from this perspective, including scratchpads / chain of thought, verifiers, STaR, selection-inference, and tool use. We refer to the resulting programs as language model cascades.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/FS94V3JI/Dohan et al. - 2022 - Language Model Cascades.pdf}
}

@article{doMethodOntologyIntegration2019,
  title = {A {{Method}} of {{Ontology Integration}} for {{Designing Intelligent Problem Solvers}}},
  author = {Do, Nhon V. and Nguyen, Hien D. and Mai, Thanh T.},
  year = {2019},
  month = jan,
  journal = {Applied Sciences},
  volume = {9},
  number = {18},
  pages = {3793},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app9183793},
  urldate = {2022-05-04},
  abstract = {Nowadays, designing knowledge-based systems which involve knowledge from different domains requires deep research of methods and techniques for knowledge integration, and ontology integration has become the foundation for many recent knowledge integration methods. To meet the requirements of real-world applications, methods of ontology integration need to be studied and developed. In this paper, an ontology model used as the knowledge kernel is presented, consisting of concepts, relationships between concepts, and inference rules. Additionally, this kernel is also added to other knowledge, such as knowledge of operators and functions, to form an integrated knowledge-based system. The mechanism of this integration method works upon the integration of the knowledge components in the ontology structure. Besides this, problems and the reasoning method to solve them on the integrated knowledge domain are also studied. Many related problems in the integrated knowledge domain and the reasoning method for solving them are also studied. Such an integrated model can represent the real-world knowledge domain about operators and functions with high accuracy and effectiveness. The ontology model can also be applied to build knowledge bases for intelligent problem solvers (IPS) in many mathematical courses in college, such as linear algebra and graph theory. These IPSs have great potential in helping students perform better in those college courses.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {intelligent problems solver,intelligent software,knowledge engineering,knowledge integration,knowledge-based system,ontology integration},
  file = {/home/xav/Zotero/storage/2PVJ2AH8/Do et al. - 2019 - A Method of Ontology Integration for Designing Int.pdf;/home/xav/Zotero/storage/6NRNDIPR/3793.html}
}

@article{domingueIRSIIIBrokerbasedApproach2008,
  title = {{{IRS-III}}: {{A}} Broker-Based Approach to Semantic {{Web}} Services},
  shorttitle = {{{IRS-III}}},
  author = {Domingue, John and Cabral, Liliana and Galizia, Stefania and Tanasescu, Vlad and Gugliotta, Alessio and Norton, Barry and Pedrinaci, Carlos},
  year = {2008},
  month = apr,
  journal = {Journal of Web Semantics},
  volume = {6},
  number = {2},
  pages = {109--132},
  issn = {15708268},
  doi = {10.1016/j.websem.2008.01.001},
  urldate = {2022-05-14},
  langid = {english},
  file = {/home/xav/Zotero/storage/B6T7TZW6/Domingue et al. - 2008 - IRS-III A broker-based approach to semantic Web s.pdf}
}

@article{domingueProblemSolvingMethods2009b,
  title = {Problem Solving Methods in a Global Networked Age},
  author = {Domingue, John and Fensel, Dieter},
  year = {2009},
  month = nov,
  journal = {AI EDAM},
  volume = {23},
  number = {4},
  pages = {373--390},
  issn = {1469-1760, 0890-0604},
  doi = {10.1017/S0890060409990060},
  urldate = {2022-11-07},
  abstract = {We believe that the future for problem solving method (PSM) derived work is very promising. In short, PSMs provide a solid foundation for creating a semantic layer supporting planetary-scale networks. Moreover, within a world-scale network where billions services are used and created by billions of parties in ad hoc dynamic fashion we believe that PSM-based mechanisms provide the only viable approach to dealing the sheer scale systematically. Our current experiments in this area are based upon a generic ontology for describing Web services derived from earlier work on PSMs. We outline how platforms based on our ontology can support large-scale networked interactivity in three main areas. Within a large European project we are able to map business level process descriptions to semantic Web service descriptions, to enable business experts to manage and use enterprise processes running in corporate information technology systems. Although highly successful, Web service-based applications predominately run behind corporate firewalls and are far less pervasive on the general Web. Within a second large European project we are extending our semantic service work using the principles underlying the Web and Web 2.0 to transform the Web from a Web of data to one where services are managed and used at large scale. Significant initiatives are now underway in North America, Asia, and Europe to design a new Internet using a ``clean-slate'' approach to fulfill the demands created by new modes of use and the additional 3 billion users linked to mobile phones. Our investigations within the European-based Future Internet program indicate that a significant opportunity exists for our PSM-derived work to address the key challenges currently identified: scalability, trust, interoperability, pervasive usability, and mobility. We outline one PSM-derived approach as an exemplar.},
  langid = {english},
  keywords = {Future Internet,Large-Scale Interoperability,Semantic Web,Semantic Web Services,Web Services Modeling Ontology},
  file = {/home/xav/Zotero/storage/SIQMUNAN/Problem solving methods in a global networked age.pdf}
}

@article{dornerComplexProblemSolving2017,
  title = {Complex {{Problem Solving}}: {{What It Is}} and {{What It Is Not}}},
  shorttitle = {Complex {{Problem Solving}}},
  author = {D{\"o}rner, Dietrich and Funke, Joachim},
  year = {2017},
  month = jul,
  journal = {Frontiers in Psychology},
  volume = {8},
  pages = {1--11},
  doi = {10.3389/fpsyg.2017.01153},
  abstract = {Computer-simulated scenarios have been part of psychological research on problem solving for more than 40 years. The shift in emphasis from simple toy problems to complex, more real-life oriented problems has been accompanied by discussions about the best ways to assess the process of solving complex problems. Psychometric issues such as reliable assessments and addressing correlations with other instruments have been in the foreground of these discussions and have left the content validity of complex problem solving in the background. In this paper, we return the focus to content issues and address the important features that define complex problems.},
  annotation = {QID: Q40976653},
  file = {/home/xav/Zotero/storage/L52QSEYF/D√∂rner et Funke - 2017 - Complex Problem Solving What It Is and What It Is.pdf}
}

@article{dornerHumanErrorComplex2022,
  title = {Human Error in Complex Problem Solving and Dynamic Decision Making: {{A}} Taxonomy of 24 Errors and a Theory},
  shorttitle = {Human Error in Complex Problem Solving and Dynamic Decision Making},
  author = {D{\"o}rner, Dietrich and G{\"u}ss, C. Dominik},
  year = {2022},
  month = aug,
  journal = {Computers in Human Behavior Reports},
  volume = {7},
  pages = {100222},
  issn = {2451-9588},
  doi = {10.1016/j.chbr.2022.100222},
  urldate = {2022-09-06},
  abstract = {The current study extends existing research on human error by investigating human error in complex, dynamic, and uncertain microworlds. The main goal was to develop a taxonomy of errors for such situations. Several tasks with differing characteristics and demands on the problem solver were used: the simulation of a chocolate producing company (CHOCO FINE), the simulation of a tribe of semi-nomads in the Sahel zone (MORO), and the simulation of wildfires engaging fire-fighting units (WINFIRE). Observing participants and teams working on these simulations, we refer to specific cases and describe 24 errors related to the steps of human problem solving and dynamic decision making. A theory is then presented which attempts to explain the causes of these errors occurring in microworld contexts. Whereas most existing theories focus on cognitive explanations, the theory presented explains human error as a result of the interaction of emotions, motivations, and cognition. The current study not only extends the taxonomy and list of human errors, but especially addresses the lack of theoretical explanations in the literature on human error. The findings provide a starting point for further theory development and empirical testing and could be applied in training programs; hopefully contributing to sensitivity and awareness of the demands of complex, dynamic, and uncertain problems, and ultimately contributing to fewer human errors.},
  langid = {english},
  keywords = {Cognitive bias,Complex problem solving,Dynamic decision making,Human error,Microworld,Virtual environment},
  annotation = {QID: Q114749542},
  file = {/home/xav/Zotero/storage/I5BEKSNN/D√∂rner et G√ºss - 2022 - Human error in complex problem solving and dynamic.pdf;/home/xav/Zotero/storage/EMSXCDD9/S2451958822000562.html}
}

@misc{droriNeuralNetworkSolves2022,
  title = {A {{Neural Network Solves}}, {{Explains}}, and {{Generates University Math Problems}} by {{Program Synthesis}} and {{Few-Shot Learning}} at {{Human Level}}},
  author = {Drori, Iddo and Zhang, Sarah and Shuttleworth, Reece and Tang, Leonard and Lu, Albert and Ke, Elizabeth and Liu, Kevin and Chen, Linda and Tran, Sunny and Cheng, Newman and Wang, Roman and Singh, Nikhil and Patti, Taylor L. and Lynch, Jayson and Shporer, Avi and Verma, Nakul and Wu, Eugene and Strang, Gilbert},
  year = {2022},
  month = may,
  number = {arXiv:2112.15594},
  eprint = {2112.15594},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-07},
  abstract = {We demonstrate that a neural network pre-trained on text and fine-tuned on code solves mathematics course problems, explains solutions, and generates new questions at a human level. We automatically synthesize programs using few-shot learning and OpenAI's Codex transformer and execute them to solve course problems at 81\% automatic accuracy. We curate a new dataset of questions from MIT's largest mathematics courses (Single Variable and Multivariable Calculus, Differential Equations, Introduction to Probability and Statistics, Linear Algebra, and Mathematics for Computer Science) and Columbia University's Computational Linear Algebra. We solve questions from a MATH dataset (on Prealgebra, Algebra, Counting and Probability, Intermediate Algebra, Number Theory, and Precalculus), the latest benchmark of advanced mathematics problems designed to assess mathematical reasoning. We randomly sample questions and generate solutions with multiple modalities, including numbers, equations, and plots. The latest GPT-3 language model pre-trained on text automatically solves only 18.8\% of these university questions using zero-shot learning and 30.8\% using few-shot learning and the most recent chain of thought prompting. In contrast, program synthesis with few-shot learning using Codex fine-tuned on code generates programs that automatically solve 81\% of these questions. Our approach improves the previous state-of-the-art automatic solution accuracy on the benchmark topics from 8.8\% to 81.1\%. We perform a survey to evaluate the quality and difficulty of generated questions. This work is the first to automatically solve university-level mathematics course questions at a human level and the first work to explain and generate university-level mathematics course questions at scale, a milestone for higher education.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/YW6QRVHV/Drori et al. - 2022 - A Neural Network Solves, Explains, and Generates U.pdf}
}

@misc{drozdovCompositionalSemanticParsing2022,
  title = {Compositional {{Semantic Parsing}} with {{Large Language Models}}},
  author = {Drozdov, Andrew and Sch{\"a}rli, Nathanael and Aky{\"u}rek, Ekin and Scales, Nathan and Song, Xinying and Chen, Xinyun and Bousquet, Olivier and Zhou, Denny},
  year = {2022},
  month = sep,
  number = {arXiv:2209.15003},
  eprint = {arXiv:2209.15003},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.15003},
  urldate = {2022-10-05},
  abstract = {Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1\% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/LRZZRPRW/Drozdov et al. - 2022 - Compositional Semantic Parsing with Large Language.pdf}
}

@misc{duaSuccessivePromptingDecomposing2022,
  title = {Successive {{Prompting}} for {{Decomposing Complex Questions}}},
  author = {Dua, Dheeru and Gupta, Shivanshu and Singh, Sameer and Gardner, Matt},
  year = {2022},
  month = dec,
  number = {arXiv:2212.04092},
  eprint = {arXiv:2212.04092},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.04092},
  urldate = {2023-01-29},
  abstract = {Answering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce ``Successive Prompting'', where we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate a synthetic dataset which can be used to bootstrap a model's ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement of \textasciitilde 5\% absolute F1 on a few-shot version of the DROP dataset when compared with a state-of-the-art model with the same supervision.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/PHYFQ9PM/Dua et al. - 2022 - Successive Prompting for Decomposing Complex Quest.pdf;/home/xav/Zotero/storage/NB4AGMGD/2212.html}
}

@inproceedings{dulceanuPhotoshopQuiACorpusNonFactoid2018,
  title = {{{PhotoshopQuiA}}: {{A Corpus}} of {{Non-Factoid Questions}} and {{Answers}} for {{Why-Question Answering}}},
  shorttitle = {{{PhotoshopQuiA}}},
  booktitle = {Proceedings of the {{Eleventh International Conference}} on {{Language Resources}} and {{Evaluation}} ({{LREC}} 2018)},
  author = {Dulceanu, Andrei and Le Dinh, Thang and Chang, Walter and Bui, Trung and Kim, Doo Soon and Vu, Manh Chien and Kim, Seokhwan},
  year = {2018},
  month = may,
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Miyazaki, Japan}},
  urldate = {2023-01-03},
  file = {/home/xav/Zotero/storage/8C238NVM/Dulceanu et al. - 2018 - PhotoshopQuiA A Corpus of Non-Factoid Questions a.pdf}
}

@misc{duLearningIterativeReasoning2022,
  title = {Learning {{Iterative Reasoning}} through {{Energy Minimization}}},
  author = {Du, Yilun and Li, Shuang and Tenenbaum, Joshua B. and Mordatch, Igor},
  year = {2022},
  month = jun,
  number = {arXiv:2206.15448},
  eprint = {arXiv:2206.15448},
  publisher = {{arXiv}},
  urldate = {2022-11-13},
  abstract = {Deep learning has excelled on complex pattern recognition tasks such as image classification and object recognition. However, it struggles with tasks requiring nontrivial reasoning, such as algorithmic computation. Humans are able to solve such tasks through iterative reasoning -- spending more time thinking about harder tasks. Most existing neural networks, however, exhibit a fixed computational budget controlled by the neural network architecture, preventing additional computational processing on harder tasks. In this work, we present a new framework for iterative reasoning with neural networks. We train a neural network to parameterize an energy landscape over all outputs, and implement each step of the iterative reasoning as an energy minimization step to find a minimal energy solution. By formulating reasoning as an energy minimization problem, for harder problems that lead to more complex energy landscapes, we may then adjust our underlying computational budget by running a more complex optimization procedure. We empirically illustrate that our iterative reasoning approach can solve more accurate and generalizable algorithmic reasoning tasks in both graph and continuous domains. Finally, we illustrate that our approach can recursively solve algorithmic problems requiring nested reasoning},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/Q5F3WHJB/Du et al. - 2022 - Learning Iterative Reasoning through Energy Minimi.pdf}
}

@misc{duongLearningGenerateQuestions2022,
  title = {Learning to {{Generate Questions}} by {{Enhancing Text Generation}} with {{Sentence Selection}}},
  author = {Duong, Do Hoang Thai and Son, Nguyen Hong and Le, Hung and Nguyen, Minh-Tien},
  year = {2022},
  month = dec,
  number = {arXiv:2212.12192},
  eprint = {arXiv:2212.12192},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.12192},
  urldate = {2023-01-03},
  abstract = {We introduce an approach for the answer-aware question generation problem. Instead of only relying on the capability of strong pre-trained language models, we observe that the information of answers and questions can be found in some relevant sentences in the context. Based on that, we design a model which includes two modules: a selector and a generator. The selector forces the model to more focus on relevant sentences regarding an answer to provide implicit local information. The generator generates questions by implicitly combining local information from the selector and global information from the whole context encoded by the encoder. The model is trained jointly to take advantage of latent interactions between the two modules. Experimental results on two benchmark datasets show that our model is better than strong pre-trained models for the question generation task. The code is also available (shorturl.at/lV567).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/B65TA2XT/Duong et al. - 2022 - Learning to Generate Questions by Enhancing Text G.pdf;/home/xav/Zotero/storage/NL3LXISN/2212.html}
}

@inproceedings{durmusFEQAQuestionAnswering2020,
  title = {{{FEQA}}: {{A Question Answering Evaluation Framework}} for {{Faithfulness Assessment}} in {{Abstractive Summarization}}},
  shorttitle = {{{FEQA}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Durmus, Esin and He, He and Diab, Mona},
  year = {2020},
  pages = {5055--5070},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.454},
  urldate = {2023-01-24},
  abstract = {Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA,1 which leverages recent advances in reading comprehension. Given questionanswer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.},
  langid = {english},
  file = {/home/xav/Zotero/storage/EZNNPMLM/Durmus et al. - 2020 - FEQA A Question Answering Evaluation Framework fo.pdf}
}

@misc{dwivedi-yuEditEvalInstructionBasedBenchmark2022,
  title = {{{EditEval}}: {{An Instruction-Based Benchmark}} for {{Text Improvements}}},
  shorttitle = {{{EditEval}}},
  author = {{Dwivedi-Yu}, Jane and Schick, Timo and Jiang, Zhengbao and Lomeli, Maria and Lewis, Patrick and Izacard, Gautier and Grave, Edouard and Riedel, Sebastian and Petroni, Fabio},
  year = {2022},
  month = sep,
  number = {arXiv:2209.13331},
  eprint = {arXiv:2209.13331},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.13331},
  urldate = {2022-10-04},
  abstract = {Evaluation of text generation to date has primarily focused on content created sequentially, rather than improvements on a piece of text. Writing, however, is naturally an iterative and incremental process that requires expertise in different modular skills such as fixing outdated information or making the style more consistent. Even so, comprehensive evaluation of a model's capacity to perform these skills and the ability to edit remains sparse. This work presents EditEval: An instruction-based, benchmark and evaluation suite that leverages high-quality existing and new datasets for automatic evaluation of editing capabilities such as making text more cohesive and paraphrasing. We evaluate several pre-trained models, which shows that InstructGPT and PEER perform the best, but that most baselines fall below the supervised SOTA, particularly when neutralizing and updating information. Our analysis also shows that commonly used metrics for editing tasks do not always correlate well, and that optimization for prompts with the highest performance does not necessarily entail the strongest robustness to different models. Through the release of this benchmark and a publicly available leaderboard challenge, we hope to unlock future research in developing models capable of iterative and more controllable editing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/VRQRC4I2/Dwivedi-Yu et al. - 2022 - EditEval An Instruction-Based Benchmark for Text .pdf}
}

@article{emmanouilidisEnablingHumanLoop2019,
  title = {Enabling the Human in the Loop: {{Linked}} Data and Knowledge in Industrial Cyber-Physical Systems},
  shorttitle = {Enabling the Human in the Loop},
  author = {Emmanouilidis, Christos and Pistofidis, Petros and Bertoncelj, Luka and Katsouros, Vassilis and Fournaris, Apostolos and Koulamas, Christos and {Ruiz-Carcel}, Cristobal},
  year = {2019},
  month = jan,
  journal = {Annual Reviews in Control},
  volume = {47},
  pages = {249--265},
  issn = {1367-5788},
  doi = {10.1016/j.arcontrol.2019.03.004},
  urldate = {2022-11-20},
  abstract = {Industrial Cyber-Physical Systems have benefitted substantially from the introduction of a range of technology enablers. These include web-based and semantic computing, ubiquitous sensing, internet of things (IoT) with multi-connectivity, advanced computing architectures and digital platforms, coupled with edge or cloud side data management and analytics, and have contributed to shaping up enhanced or new data value chains in manufacturing. While parts of such data flows are increasingly automated, there is now a greater demand for more effectively integrating, rather than eliminating, human cognitive capabilities in the loop of production related processes. Human integration in Cyber-Physical environments can already be digitally supported in various ways. However, incorporating human skills and tangible knowledge requires approaches and technological solutions that facilitate the engagement of personnel within technical systems in ways that take advantage or amplify their cognitive capabilities to achieve more effective sociotechnical systems. After analysing related research, this paper introduces a novel viewpoint for enabling human in the loop engagement linked to cognitive capabilities and highlighting the role of context information management in industrial systems. Furthermore, it presents examples of technology enablers for placing the human in the loop at selected application cases relevant to production environments. Such placement benefits from the joint management of linked maintenance data and knowledge, expands the power of machine learning for asset awareness with embedded event detection, and facilitates IoT-driven analytics for product lifecycle management.},
  langid = {english},
  keywords = {Asset lifecycle management,Context information management,Cyber-physical systems,Human in the loop,Internet of things,Maintenance,Product lifecycle management},
  annotation = {QID: Q97480285},
  file = {/home/xav/Zotero/storage/PG9LXDY3/Emmanouilidis et al. - 2019 - Enabling the human in the loop Linked data and kn.pdf;/home/xav/Zotero/storage/MEPEER55/S1367578818301366.html}
}

@misc{EMNLPConferenceEmpirical,
  title = {{{EMNLP}} | {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  journal = {emnlp 2022},
  urldate = {2023-02-15},
  abstract = {Abu DhabiDecember 7\textendash 11, 2022},
  howpublished = {https://2022.emnlp.org/},
  langid = {english},
  file = {/home/xav/Zotero/storage/XA8GTFIZ/2022.emnlp.org.html}
}

@article{erdemNeuralNaturalLanguage2022,
  title = {Neural {{Natural Language Generation}}: {{A Survey}} on {{Multilinguality}}, {{Multimodality}}, {{Controllability}} and {{Learning}}},
  shorttitle = {Neural {{Natural Language Generation}}},
  author = {Erdem, Erkut and Kuyu, Menekse and Yagcioglu, Semih and Frank, A. and Parcalabescu, Letitia and Plank, Barbara and Babii, A. and Turuta, O. and Erdem, Aykut and Calixto, Iacer and Lloret, Elena and Apostol, E. and Truic{\u a}, Ciprian-Octavian and {\v S}andrih, Branislava and {Martin{\v c}i{\'c}-Ip{\v s}i{\'c}}, Sanda and Berend, G{\'a}bor and Gatt, Albert and Korvel, G.},
  year = {2022},
  journal = {J. Artif. Intell. Res.},
  doi = {10.1613/jair.1.12918},
  abstract = {This state-of-the-art report investigates the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions.},
  annotation = {QID: Q114923271},
  file = {/home/xav/Zotero/storage/AF5CYPV2/Erdem et al. - 2022 - Neural Natural Language Generation A Survey on Mu.pdf}
}

@article{fajcikR2D2ModularBaseline2021,
  title = {R2-{{D2}}: {{A Modular Baseline}} for {{Open-Domain Question Answering}}},
  shorttitle = {R2-{{D2}}},
  author = {Fajcik, Martin and Docekal, Martin and Ondrej, Karel and Smrz, Pavel},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.03502 [cs]},
  eprint = {2109.03502},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {This work presents a novel four-stage opendomain QA pipeline R2-D2 (RANK TWICE, READ TWICE). The pipeline is composed of a retriever, passage reranker, extractive reader, generative reader and a mechanism that aggregates the final prediction from all system's components. We demonstrate its strength across three open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA, surpassing state-of-the-art on the first two. Our analysis demonstrates that: (i) combining extractive and generative reader yields absolute improvements up to 5 exact match and it is at least twice as effective as the posterior averaging ensemble of the same models with different parameters, (ii) the extractive reader with fewer parameters can match the performance of the generative reader on extractive QA datasets1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/4KPY3VX3/Fajcik et al. - 2021 - R2-D2 A Modular Baseline for Open-Domain Question.pdf}
}

@misc{fanAugmentingTransformersKNNBased2020,
  title = {Augmenting {{Transformers}} with {{KNN-Based Composite Memory}} for {{Dialogue}}},
  author = {Fan, Angela and Gardent, Claire and Braud, Chloe and Bordes, Antoine},
  year = {2020},
  month = nov,
  number = {arXiv:2004.12744},
  eprint = {arXiv:2004.12744},
  publisher = {{arXiv}},
  urldate = {2022-06-25},
  abstract = {Various machine learning tasks can benefit from access to external information of different modalities, such as text and images. Recent work has focused on learning architectures with large memories capable of storing this knowledge. We propose augmenting generative Transformer neural networks with KNN-based Information Fetching (KIF) modules. Each KIF module learns a read operation to access fixed external knowledge. We apply these modules to generative dialog modeling, a challenging task where information must be flexibly retrieved and incorporated to maintain the topic and flow of conversation. We demonstrate the effectiveness of our approach by identifying relevant knowledge required for knowledgeable but engaging dialog from Wikipedia, images, and human-written dialog utterances, and show that leveraging this retrieved information improves model performance, measured by automatic and human evaluation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/AXTZHG2T/Fan et al. - 2020 - Augmenting Transformers with KNN-Based Composite M.pdf}
}

@misc{fanAutomatedRepairPrograms2023,
  title = {Automated {{Repair}} of {{Programs}} from {{Large Language Models}}},
  author = {Fan, Zhiyu and Gao, Xiang and Mirchev, Martin and Roychoudhury, Abhik and Tan, Shin Hwei},
  year = {2023},
  month = jan,
  number = {arXiv:2205.10583},
  eprint = {arXiv:2205.10583},
  publisher = {{arXiv}},
  urldate = {2023-01-05},
  abstract = {Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Software Engineering},
  file = {/home/xav/Zotero/storage/DN7EJLY7/Fan et al. - 2023 - Automated Repair of Programs from Large Language M.pdf}
}

@inproceedings{fangBenchmarkingCommonsenseKnowledge2021,
  title = {Benchmarking {{Commonsense Knowledge Base Population}} with an {{Effective Evaluation Dataset}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Fang, Tianqing and Wang, Weiqi and Choi, Sehyun and Hao, Shibo and Zhang, Hongming and Song, Yangqiu and He, Bin},
  year = {2021},
  pages = {8949--8964},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.705},
  urldate = {2022-05-04},
  langid = {english},
  file = {/home/xav/Zotero/storage/PERX9F57/Fang et al. - 2021 - Benchmarking Commonsense Knowledge Base Population.pdf}
}

@article{fangHierarchicalGraphNetwork2020,
  title = {Hierarchical {{Graph Network}} for {{Multi-hop Question Answering}}},
  author = {Fang, Yuwei and Sun, Siqi and Gan, Zhe and Pillai, Rohit and Wang, Shuohang and Liu, Jingjing},
  year = {2020},
  month = oct,
  journal = {arXiv:1911.03631 [cs]},
  eprint = {1911.03631},
  primaryclass = {cs},
  urldate = {2022-03-28},
  abstract = {In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {QID: Q85123932},
  file = {/home/xav/Zotero/storage/J5AP5VXD/Fang et al. - 2020 - Hierarchical Graph Network for Multi-hop Question .pdf;/home/xav/Zotero/storage/344JUW32/1911.html}
}

@misc{fanMineDojoBuildingOpenEnded2022,
  title = {{{MineDojo}}: {{Building Open-Ended Embodied Agents}} with {{Internet-Scale Knowledge}}},
  shorttitle = {{{MineDojo}}},
  author = {Fan, Linxi and Wang, Guanzhi and Jiang, Yunfan and Mandlekar, Ajay and Yang, Yuncong and Zhu, Haoyi and Tang, Andrew and Huang, De-An and Zhu, Yuke and Anandkumar, Anima},
  year = {2022},
  month = nov,
  number = {arXiv:2206.08853},
  eprint = {arXiv:2206.08853},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.08853},
  urldate = {2022-11-27},
  abstract = {Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/Q69PPX3G/Fan et al. - 2022 - MineDojo Building Open-Ended Embodied Agents with.pdf}
}

@misc{fayekTemporalReasoningAudio2019,
  title = {Temporal {{Reasoning}} via {{Audio Question Answering}}},
  author = {Fayek, Haytham M. and Johnson, Justin},
  year = {2019},
  month = nov,
  number = {arXiv:1911.09655},
  eprint = {arXiv:1911.09655},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.09655},
  urldate = {2023-01-24},
  abstract = {Multimodal question answering tasks can be used as proxy tasks to study systems that can perceive and reason about the world. Answering questions about different types of input modalities stresses different aspects of reasoning such as visual reasoning, reading comprehension, story understanding, or navigation. In this paper, we use the task of Audio Question Answering (AQA) to study the temporal reasoning abilities of machine learning models. To this end, we introduce the Diagnostic Audio Question Answering (DAQA) dataset comprising audio sequences of natural sound events and programmatically generated questions and answers that probe various aspects of temporal reasoning. We adapt several recent state-of-the-art methods for visual question answering to the AQA task, and use DAQA to demonstrate that they perform poorly on questions that require in-depth temporal reasoning. Finally, we propose a new model, Multiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the recent Feature-wise Linear Modulation (FiLM) model and significantly improves its temporal reasoning capabilities. We envisage DAQA to foster research on AQA and temporal reasoning and MALiMo a step towards models for AQA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/xav/Zotero/storage/4UFUTBWY/Fayek et Johnson - 2019 - Temporal Reasoning via Audio Question Answering.pdf;/home/xav/Zotero/storage/G7XHYPFW/1911.html}
}

@article{fayekTemporalReasoningAudio2020,
  title = {Temporal {{Reasoning}} via {{Audio Question Answering}}},
  author = {Fayek, Haytham M. and Johnson, Justin},
  year = {2020},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume = {28},
  pages = {2283--2294},
  issn = {2329-9304},
  doi = {10.1109/TASLP.2020.3010650},
  abstract = {Multimodal question answering tasks can be used as proxy tasks to study systems that can perceive and reason about the world. Answering questions about different types of input modalities stresses different aspects of reasoning such as visual reasoning, reading comprehension, story understanding, or navigation. In this article, we use the task of Audio Question Answering (AQA) to study the temporal reasoning abilities of machine learning models. To this end, we introduce the Diagnostic Audio Question Answering (DAQA) dataset comprising audio sequences of natural sound events and programmatically generated questions and answers that probe various aspects of temporal reasoning. We adapt several recent state-of-the-art methods for visual question answering to the AQA task, and use DAQA to demonstrate that they perform poorly on questions that require in-depth temporal reasoning. Finally, we propose a new model, Multiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the recent Feature-wise Linear Modulation (FiLM) model and significantly improves its temporal reasoning capabilities. We envisage DAQA to foster research on AQA and temporal reasoning and MALiMo a step towards models for AQA.},
  keywords = {Adaptation models,Audio,Benchmark testing,Cognition,Knowledge discovery,question answering,reasoning,Speech processing,Task analysis,temporal reasoning,Visualization},
  file = {/home/xav/Zotero/storage/54YZ9WGE/Fayek et Johnson - 2020 - Temporal Reasoning via Audio Question Answering.pdf;/home/xav/Zotero/storage/3KGWF5J9/9145807.html}
}

@book{fayemiBioinspiredDesignCharacterisation2014,
  title = {Bio-Inspired Design Characterisation and Its Links with Problem Solving Tools},
  author = {Fayemi, Pierre-Emmanuel and Maranzana, Nicolas and Aoussat, A. and Bersano, Giacomo},
  year = {2014},
  month = may,
  abstract = {Although bio-inspiration is a well-known instrument for innovation, the problem-solving process that leads to the solution has not been fully investigated yet. The purpose of this article is to understand what bio-inspiration is, by defining its relative concepts and boundaries. After the outlining of a generic biomimetic process, a direct correspondence with TRIZ tools is presented. Each phase of the proposed process has been classified according to the type of tool that is needed. For the two first class, an ideal set of features has been defined.},
  file = {/home/xav/Zotero/storage/9TJJBBFF/Fayemi et al. - 2014 - Bio-inspired design characterisation and its links.pdf}
}

@article{federCausaLMCausalModel2021a,
  title = {{{CausaLM}}: {{Causal Model Explanation Through Counterfactual Language Models}}},
  shorttitle = {{{CausaLM}}},
  author = {Feder, Amir and Oved, Nadav and Shalit, Uri and Reichart, Roi},
  year = {2021},
  month = may,
  journal = {Computational Linguistics},
  eprint = {2005.13407},
  primaryclass = {cs},
  pages = {1--54},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00404},
  urldate = {2022-10-12},
  abstract = {Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/C5S22FHW/Feder et al. - 2021 - CausaLM Causal Model Explanation Through Counterf.pdf}
}

@misc{fedusSwitchTransformersScaling2022,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year = {2022},
  month = jun,
  number = {arXiv:2101.03961},
  eprint = {arXiv:2101.03961},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.03961},
  urldate = {2023-02-09},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/AQFPHKF2/Fedus et al. - 2022 - Switch Transformers Scaling to Trillion Parameter.pdf;/home/xav/Zotero/storage/KKSAIV2B/2101.html}
}

@misc{fengGenericTemporalReasoning2022,
  title = {Generic {{Temporal Reasoning}} with {{Differential Analysis}} and {{Explanation}}},
  author = {Feng, Yu and Zhou, Ben and Wang, Haoyu and Jin, Helen and Roth, Dan},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10467},
  eprint = {arXiv:2212.10467},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10467},
  urldate = {2023-01-30},
  abstract = {Temporal reasoning is the task of predicting temporal relations of event pairs with corresponding contexts. While some temporal reasoning models perform reasonably well on in-domain benchmarks, we have little idea of the systems' generalizability due to existing datasets' limitations. In this work, we introduce a novel task named TODAY that bridges this gap with temporal differential analysis, which as the name suggests, evaluates if systems can correctly understand the effect of incremental changes. Specifically, TODAY makes slight context changes for given event pairs, and systems need to tell how this subtle contextual change will affect temporal relation distributions. To facilitate learning, TODAY also annotates human explanations. We show that existing models, including GPT-3, drop to random guessing on TODAY, suggesting that they heavily rely on spurious information rather than proper reasoning for temporal predictions. On the other hand, we show that TODAY's supervision style and explanation annotations can be used in joint learning and encourage models to use more appropriate signals during training and outperform across several benchmarks. TODAY can also be used to train models to solicit incidental supervision from noisy sources such as GPT-3 and moves farther towards generic temporal reasoning systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BEQ6TY9I/Feng et al. - 2022 - Generic Temporal Reasoning with Differential Analy.pdf;/home/xav/Zotero/storage/DZKITLHH/2212.html}
}

@article{fenselUnifiedProblemSolvingMethod2003,
  title = {The {{Unified Problem-Solving Method Development Language UPML}}},
  author = {Fensel, Dieter and Motta, Enrico and {van Harmelen}, Frank and Benjamins, V. Richard and Crubezy, Monica and Decker, Stefan and Gaspari, Mauro and Groenboom, Rix and Grosso, William and Musen, Mark and Plaza, Enric and Schreiber, Guus and Studer, Rudi and Wielinga, Bob},
  year = {2003},
  month = feb,
  journal = {Knowledge and Information Systems},
  volume = {5},
  number = {1},
  pages = {83--131},
  issn = {0219-1377},
  doi = {10.1007/s10115-002-0074-5},
  urldate = {2022-03-24},
  abstract = {Problem-solving methods provide reusable architectures and components for implementing the reasoning part of knowledge-based systems. The Unified Problem-Solving Method Description Language (UPML) has been developed to describe and implement such architectures and components to facilitate their semi-automatic reuse and adaptation. In a nutshell, UPML is a framework for developing knowledge-intensive reasoning systems based on libraries ofg eneric problem-solving components. The paper describes the components and adapters, architectural constraints, development guidelines, and tools provided by UPML. UPML is developed as part of the IBROW project, which provides an Internet-based brokering service for reusing problem-solving methods.},
  langid = {english},
  annotation = {QID: Q56840234},
  file = {/home/xav/Zotero/storage/EWNWWIDC/UPML A Framework for Knowledge System Reuse.pdf;/home/xav/Zotero/storage/LDA9M9ET/Fensel et al. - 2003 - The Unified Problem-Solving Method Development Lan.pdf}
}

@incollection{fenselUsingOntologiesDefining2006,
  title = {Using {{Ontologies For Defining Tasks}}, {{Problem-Solving Methods}} and {{Their Mappings}}},
  author = {Fensel, D. and Motta, Enrico and Decker, S. and Zdr{\'a}hal, Zdenek},
  year = {2006},
  month = apr,
  pages = {113--128},
  doi = {10.1007/BFb0026781},
  abstract = {In recent years two main technologies for knowledge sharing and reuse have emerged: ontologies and problem solving methods (PSMs). Ontologies specify reusable conceptualizations which can be shared by multiple reasoning components communicating during a problem solving process. PSMs describe in a domain-independent way the generic reasoning steps and knowledge types needed to perform a task. Typically PSMs are specified in a task-specific fashion, using modelling frameworks which describe their control and inference structures as well as their knowledge requirements and competence. In this paper we discuss a novel approach to PSM specification, which is based on the use of formal ontologies. In particular our specifications abstract from control, data flow and other dynamic aspects of PSMs to focus on the logical theory associated with a PSM (method ontology). This approach concentrates on the competence and knowledge requirements of a PSM, rather than internal control details, thus enabling black-box-style reuse. In the paper we also look at the nature of PSM specifications and we show that these can be characterised in a task-independent style as generic search strategies. The resulting modelling gap between method-independent task specifications and task-independent method ontologies can be bridged by constructing the relevant adapter ontology, which reformulates the method ontology in task-specific terms. An important aspect of the ontology-centred approach described here is that, in contrast with other characterisations of task-independent PSMs, it does away with the simple, binary distinction between weak and strong methods. We argue that any method can be defined in either task-independent or task-dependent style and therefore such distinction is of limited utility in PSM reuse. The differences between PSMs which affect reuse concern the ontological commitments which they make with respect to domain knowledge and goal specifications.},
  isbn = {978-3-540-63592-5},
  file = {/home/xav/Zotero/storage/56C6ERSA/Fensel et al. - 2006 - Using Ontologies For Defining Tasks, Problem-Solvi.pdf}
}

@article{ferrucciBuildingWatsonOverview2010,
  title = {Building {{Watson}}: {{An Overview}} of the {{DeepQA Project}}},
  shorttitle = {Building {{Watson}}},
  author = {Ferrucci, David and Brown, Eric and {Chu-Carroll}, Jennifer and Fan, James and Gondek, David and Kalyanpur, Aditya and Lally, Adam and Murdock, J William and Nyberg, Eric and Prager, John and Schlaefer, Nico and Welty, Christopher},
  year = {2010},
  month = sep,
  journal = {AI Magazine},
  volume = {31},
  pages = {59--79},
  doi = {10.1609/aimag.v31i3.2303},
  abstract = {IBM Research undertook a challenge to build a computer system that could compete at the human champion level in real time on the American TV quiz show, Jeopardy. The extent of the challenge includes fielding a real-time automatic contestant on the show, not merely a laboratory exercise. The Jeopardy Challenge helped us address requirements that led to the design of the DeepQA architecture and the implementation of Watson. After three years of intense research and development by a core team of about 20 researchers, Watson is performing at human expert levels in terms of precision, confidence, and speed at the Jeopardy quiz show. Our results strongly suggest that DeepQA is an effective and extensible architecture that can be used as a foundation for combining, deploying, evaluating, and advancing a wide range of algorithmic techniques to rapidly advance the field of question answering (QA).},
  annotation = {QID: Q61772844},
  file = {/home/xav/Zotero/storage/G2PLCZ9F/Ferrucci et al. - 2010 - Building Watson An Overview of the DeepQA Project.pdf}
}

@inproceedings{fishchevaArgumentativeTextGeneration2022,
  title = {Argumentative {{Text Generation}} in {{Economic Domain}}},
  booktitle = {Computational {{Linguistics}} and {{Intellectual Technologies}}},
  author = {Fishcheva, Irina and Osadchiy, Dmitriy and Bochenina, Klavdiya and Kotelnikov, Evgeny},
  year = {2022},
  month = jun,
  eprint = {2206.09251},
  primaryclass = {cs},
  pages = {211--222},
  doi = {10.28995/2075-7182-2022-21-211-222},
  urldate = {2023-01-03},
  abstract = {The development of large and super-large language models, such as GPT-3, T5, Switch Transformer, ERNIE, etc., has significantly improved the performance of text generation. One of the important research directions in this area is the generation of texts with arguments. The solution of this problem can be used in business meetings, political debates, dialogue systems, for preparation of student essays. One of the main domains for these applications is the economic sphere. The key problem of the argument text generation for the Russian language is the lack of annotated argumentation corpora. In this paper, we use translated versions of the Argumentative Microtext, Persuasive Essays and UKP Sentential corpora to fine-tune RuBERT model. Further, this model is used to annotate the corpus of economic news by argumentation. Then the annotated corpus is employed to fine-tune the ruGPT-3 model, which generates argument texts. The results show that this approach improves the accuracy of the argument generation by more than 20 percentage points (63.2\textbackslash\% vs. 42.5\textbackslash\%) compared to the original ruGPT-3 model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/TNWC382E/Fishcheva et al. - 2022 - Argumentative Text Generation in Economic Domain.pdf;/home/xav/Zotero/storage/PI3U2UR8/2206.html}
}

@article{flennerhagBootstrappedMetaLearning2022,
  title = {Bootstrapped {{Meta-Learning}}},
  author = {Flennerhag, Sebastian and Schroecker, Yannick and Zahavy, Tom and {van Hasselt}, Hado and Silver, David and Singh, Satinder},
  year = {2022},
  month = mar,
  journal = {arXiv:2109.04504 [cs, stat]},
  eprint = {2109.04504},
  primaryclass = {cs, stat},
  urldate = {2022-05-04},
  abstract = {Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent, without backpropagating through the update rule.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/6EN6XWFN/Flennerhag et al. - 2022 - Bootstrapped Meta-Learning.pdf;/home/xav/Zotero/storage/NGCUQFFA/2109.html}
}

@article{floresUsingCollectiveIntelligence2015,
  title = {Using the {{Collective Intelligence}} for Inventive Problem Solving: {{A}} Contribution for {{Open Computer Aided Innovation}}},
  shorttitle = {Using the {{Collective Intelligence}} for Inventive Problem Solving},
  author = {Flores, Rene Lopez and Belaud, J. and Lann, J. L. and N{\'e}gny, St{\'e}phane},
  year = {2015},
  journal = {Expert Syst. Appl.},
  doi = {10.1016/j.eswa.2015.08.024},
  abstract = {In the industrial context, an interest exists in the collective resolution of creative problems during the conceptual design phase. In this work we introduce an information-based software framework useful to collaborate for inventive problems solving. This framework proposes the implementation of techniques from the Collective Intelligence (CI) research field in combination with the systematic methods provided by the TRIZ theory. Both approaches are centered in the human aspect of the innovation process, and are complementary. While CI focuses on the intelligent behavior that emerges in collaborative work, the TRIZ theory is centered in the individual capacity to solve problems systematically. The framework's objective is to improve the individual creativity provided by the TRIZ method and tools, with the value created by the collective contributions. This work aims to contribute formulating the basis to extend the research field of Computer Aided Innovation, to the next evolutionary step called ``Open CAI 2.0''.},
  file = {/home/xav/Zotero/storage/MZH56E2V/Flores et al. - 2015 - Using the Collective Intelligence for inventive pr.pdf}
}

@misc{foltz-smithCausalInferenceGround2021,
  title = {Causal {{Inference}} and {{Ground Truth}} with {{GPT3}}},
  author = {{Foltz-Smith}, Russell},
  year = {2021},
  month = jul,
  journal = {Your Virtual Self},
  urldate = {2022-05-04},
  abstract = {Overview},
  langid = {english},
  file = {/home/xav/Zotero/storage/95NEP896/causal-inference-and-ground-truth-with-gpt3-2f1dc3e8f692.html}
}

@misc{FrameworkUnifyingProblemSolving,
  title = {A {{Framework}} for {{Unifying Problem-Solving Knowledge}} and {{Workflow Modeling}}},
  urldate = {2022-11-07},
  abstract = {AAAI advances the understanding of the mechanisms underlying thought and intelligent behavior and their embodiment in machines.},
  howpublished = {https://www.aaai.org/Library/Symposia/Spring/2008/ss08-01-014.php},
  file = {/home/xav/Zotero/storage/2ESF6XZD/SS08-01-014.pdf}
}

@misc{frantarSparseGPTMassiveLanguage2023,
  title = {{{SparseGPT}}: {{Massive Language Models Can Be Accurately Pruned}} in {{One-Shot}}},
  shorttitle = {{{SparseGPT}}},
  author = {Frantar, Elias and Alistarh, Dan},
  year = {2023},
  month = jan,
  number = {arXiv:2301.00774},
  eprint = {arXiv:2301.00774},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.00774},
  urldate = {2023-02-03},
  abstract = {We show for the first time that large-scale generative pretrained transformer (GPT) family models can be pruned to at least 50\% sparsity in one-shot, without any retraining, at minimal loss of accuracy. This is achieved via a new pruning method called SparseGPT, specifically designed to work efficiently and accurately on massive GPT-family models. When executing SparseGPT on the largest available open-source models, OPT-175B and BLOOM-176B, we can reach 60\% sparsity with negligible increase in perplexity: remarkably, more than 100 billion weights from these models can be ignored at inference time. SparseGPT generalizes to semi-structured (2:4 and 4:8) patterns, and is compatible with weight quantization approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/3SY5MSDY/Frantar et Alistarh - 2023 - SparseGPT Massive Language Models Can Be Accurate.pdf;/home/xav/Zotero/storage/DMU6ZMKY/2301.html}
}

@book{frenschComplexProblemSolving1995,
  title = {Complex {{Problem Solving}}\textemdash{{The European Perspective}}},
  author = {Frensch, Peter and Funke, Joachim},
  year = {1995},
  month = jan,
  journal = {Learning to Solve Complex Scientific Problems},
  file = {/home/xav/Zotero/storage/GCWGVJNC/Frensch et Funke - 1995 - Complex Problem Solving‚ÄîThe European Perspective.pdf}
}

@article{frisoniNLGMetricverseEndtoEndLibrary2022,
  title = {{{NLG-Metricverse}}: {{An End-to-End Library}} for {{Evaluating Natural Language Generation}}},
  author = {Frisoni, Giacomo and Carbonaro, Antonella and Moro, Gianluca and Zammarchi, Andrea and Avagnano, Marco},
  year = {2022},
  langid = {english},
  file = {/home/xav/Zotero/storage/IRFC9KXD/Frisoni et al. - NLG-Metricverse An End-to-End Library for Evaluat.pdf}
}

@misc{fuComplexityBasedPromptingMultiStep2023,
  title = {Complexity-{{Based Prompting}} for {{Multi-Step Reasoning}}},
  author = {Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
  year = {2023},
  month = jan,
  number = {arXiv:2210.00720},
  eprint = {arXiv:2210.00720},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.00720},
  urldate = {2023-02-07},
  abstract = {We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/UAZ2VE38/Fu et al. - 2023 - Complexity-Based Prompting for Multi-Step Reasonin.pdf;/home/xav/Zotero/storage/3CZHYCB2/2210.html}
}

@misc{fuSpecializingSmallerLanguage2023,
  title = {Specializing {{Smaller Language Models}} towards {{Multi-Step Reasoning}}},
  author = {Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and Khot, Tushar},
  year = {2023},
  month = jan,
  number = {arXiv:2301.12726},
  eprint = {arXiv:2301.12726},
  publisher = {{arXiv}},
  urldate = {2023-02-07},
  abstract = {The surprising ability of Large Language Models (LLMs) to perform well on complex reasoning with only few-shot chain-of-thought prompts is believed to emerge only in very large-scale models (100+ billion parameters). We show that such abilities can, in fact, be distilled down from GPT3.5 ({$\geq$} 175B) to T5 variants ({$\leq$} 11B). We propose model specialization, to specialize the model's ability towards a target task. The hypothesis is that large models (commonly viewed as larger than 100B) have strong modeling power, but are spread on a large spectrum of tasks. Small models (commonly viewed as smaller than 10B) have limited model capacity, but if we concentrate their capacity on a specific target task, the model can achieve a decent improved performance. We use multi-step math reasoning as our testbed because it is a very typical emergent ability. We show two important aspects of model abilities: (1). there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities; (2). by paying the price of decreased generic ability, we can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability. We further give comprehensive discussions about important design choices for better generalization, including the tuning data format, the start model checkpoint, and a new model selection method. We hope our practice and discoveries can serve as an important attempt towards specialized smaller models in the new research paradigm set by LLMs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/CFNVZVEH/Fu et al. - 2023 - Specializing Smaller Language Models towards Multi.pdf}
}

@misc{galicSimilaritySearchEngine2022,
  title = {Similarity {{Search Engine}} for {{Millions}} of {{Math Problems}}},
  author = {Gali{\'c}, Vlado},
  year = {2022},
  month = may,
  journal = {Photomath Engineering},
  urldate = {2022-05-17},
  abstract = {For all developers, business people, tech companies who are curious how to lower costs in tasks where the business process requires manual\ldots},
  langid = {english},
  file = {/home/xav/Zotero/storage/7GQ2A8HG/similarity-search-engine-for-millions-of-math-problems-5040c98deb48.html}
}

@article{ganapiniThinkingFastSlow2021,
  title = {Thinking {{Fast}} and {{Slow}} in {{AI}}: The {{Role}} of {{Metacognition}}},
  shorttitle = {Thinking {{Fast}} and {{Slow}} in {{AI}}},
  author = {Ganapini, Marianna Bergamaschi and Campbell, Murray and Fabiano, Francesco and Horesh, Lior and Lenchner, Jon and Loreggia, Andrea and Mattei, Nicholas and Rossi, Francesca and Srivastava, Biplav and Venable, Kristen Brent},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.01834 [cs]},
  eprint = {2110.01834},
  primaryclass = {cs},
  urldate = {2022-04-23},
  abstract = {AI systems have seen dramatic advancement in recent years, bringing many applications that pervade our everyday life. However, we are still mostly seeing instances of narrow AI: many of these recent developments are typically focused on a very limited set of competencies and goals, e.g., image interpretation, natural language processing, classification, prediction, and many others. Moreover, while these successes can be accredited to improved algorithms and techniques, they are also tightly linked to the availability of huge datasets and computational power. State-of-the-art AI still lacks many capabilities that would naturally be included in a notion of (human) intelligence. We argue that a better study of the mechanisms that allow humans to have these capabilities can help us understand how to imbue AI systems with these competencies. We focus especially on D. Kahneman's theory of thinking fast and slow, and we propose a multi-agent AI architecture where incoming problems are solved by either system 1 (or "fast") agents, that react by exploiting only past experience, or by system 2 (or "slow") agents, that are deliberately activated when there is the need to reason and search for optimal solutions beyond what is expected from the system 1 agent. Both kinds of agents are supported by a model of the world, containing domain knowledge about the environment, and a model of "self", containing information about past actions of the system and solvers' skills.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/xav/Zotero/storage/3TTCDELC/Ganapini et al. - 2021 - Thinking Fast and Slow in AI the Role of Metacogn.pdf;/home/xav/Zotero/storage/CVXWA9U6/2110.html}
}

@misc{ganguliRedTeamingLanguage2022,
  title = {Red {{Teaming Language Models}} to {{Reduce Harms}}: {{Methods}}, {{Scaling Behaviors}}, and {{Lessons Learned}}},
  shorttitle = {Red {{Teaming Language Models}} to {{Reduce Harms}}},
  author = {Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and Jones, Andy and Bowman, Sam and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Elhage, Nelson and {El-Showk}, Sheer and Fort, Stanislav and {Hatfield-Dodds}, Zac and Henighan, Tom and Hernandez, Danny and Hume, Tristan and Jacobson, Josh and Johnston, Scott and Kravec, Shauna and Olsson, Catherine and Ringer, Sam and {Tran-Johnson}, Eli and Amodei, Dario and Brown, Tom and Joseph, Nicholas and McCandlish, Sam and Olah, Chris and Kaplan, Jared and Clark, Jack},
  year = {2022},
  month = nov,
  number = {arXiv:2209.07858},
  eprint = {arXiv:2209.07858},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.07858},
  urldate = {2023-02-08},
  abstract = {We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/home/xav/Zotero/storage/XLLNQ5FG/Ganguli et al. - 2022 - Red Teaming Language Models to Reduce Harms Metho.pdf;/home/xav/Zotero/storage/CDNI39JX/2209.html}
}

@misc{gaoPALProgramaidedLanguage2023,
  title = {{{PAL}}: {{Program-aided Language Models}}},
  shorttitle = {{{PAL}}},
  author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  year = {2023},
  month = jan,
  number = {arXiv:2211.10435},
  eprint = {arXiv:2211.10435},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.10435},
  urldate = {2023-02-07},
  abstract = {Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time ("few-shot prompting"). Much of this success can be attributed to prompting methods such as "chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15\% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/XEGFST7R/Gao et al. - 2023 - PAL Program-aided Language Models.pdf;/home/xav/Zotero/storage/Q32Q32I9/2211.html}
}

@misc{gaoScalingLawsReward2022,
  title = {Scaling {{Laws}} for {{Reward Model Overoptimization}}},
  author = {Gao, Leo and Schulman, John and Hilton, Jacob},
  year = {2022},
  month = oct,
  number = {arXiv:2210.10760},
  eprint = {arXiv:2210.10760},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.10760},
  urldate = {2022-12-21},
  abstract = {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed "gold-standard" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-\$n\$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/EJ6EJT42/Gao et al. - 2022 - Scaling Laws for Reward Model Overoptimization.pdf}
}

@misc{gaoSimCSESimpleContrastive2022,
  title = {{{SimCSE}}: {{Simple Contrastive Learning}} of {{Sentence Embeddings}}},
  shorttitle = {{{SimCSE}}},
  author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
  year = {2022},
  month = may,
  number = {arXiv:2104.08821},
  eprint = {arXiv:2104.08821},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08821},
  urldate = {2023-02-02},
  abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/TTAQPZ9J/Gao et al. - 2022 - SimCSE Simple Contrastive Learning of Sentence Em.pdf;/home/xav/Zotero/storage/KYDZI57M/2104.html}
}

@misc{gaoUnifiedContinualLearning2023,
  title = {A {{Unified Continual Learning Framework}} with {{General Parameter-Efficient Tuning}}},
  author = {Gao, Qiankun and Zhao, Chen and Sun, Yifan and Xi, Teng and Zhang, Gang and Ghanem, Bernard and Zhang, Jian},
  year = {2023},
  month = mar,
  number = {arXiv:2303.10070},
  eprint = {arXiv:2303.10070},
  publisher = {{arXiv}},
  urldate = {2023-03-24},
  abstract = {The ``pre-training \textrightarrow{} downstream adaptation'' presents both new opportunities and challenges for Continual Learning (CL). Although the recent state-of-the-art in CL is achieved through Parameter-Efficient-Tuning (PET) adaptation paradigm, only prompt has been explored, limiting its application to Transformers only. In this paper, we position prompting as one instantiation of PET, and propose a unified CL framework with general PET, dubbed as LearningAccumulation-Ensemble (LAE). PET, e.g., using Adapter, LoRA, or Prefix, can adapt a pre-trained model to downstream tasks with fewer parameters and resources. Given a PET method, our LAE framework incorporates it for CL with three novel designs. 1) Learning: the pre-trained model adapts to the new task by tuning an online PET module, along with our adaptation speed calibration to align different PET modules, 2) Accumulation: the task-specific knowledge learned by the online PET module is accumulated into an offline PET module through momentum update, 3) Ensemble: During inference, we respectively construct two experts with online/offline PET modules (which are favored by the novel/historical tasks) for prediction ensemble. We show that LAE is compatible with a battery of PET methods and gains strong CL capability. For example, LAE with Adaptor PET surpasses the prior state-ofthe-art by 1.3\% and 3.6\% in last-incremental accuracy on CIFAR100 and ImageNet-R datasets, respectively.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/ARC5QX9L/Gao et al. - 2023 - A Unified Continual Learning Framework with Genera.pdf}
}

@misc{garbaceaWhyConstrainedNeural2022,
  title = {Why Is Constrained Neural Language Generation Particularly Challenging?},
  author = {Garbacea, Cristina and Mei, Qiaozhu},
  year = {2022},
  month = jun,
  number = {arXiv:2206.05395},
  eprint = {arXiv:2206.05395},
  publisher = {{arXiv}},
  urldate = {2022-07-19},
  abstract = {Recent advances in deep neural language models combined with the capacity of large scale datasets have accelerated the development of natural language generation systems that produce fluent and coherent texts (to various degrees of success) in a multitude of tasks and application contexts. However, controlling the output of these models for desired user and task needs is still an open challenge. This is crucial not only to customizing the content and style of the generated language, but also to their safe and reliable deployment in the real world. We present an extensive survey on the emerging topic of constrained neural language generation in which we formally define and categorize the problems of natural language generation by distinguishing between conditions and constraints (the latter being testable conditions on the output text instead of the input), present constrained text generation tasks, and review existing methods and evaluation metrics for constrained text generation. Our aim is to highlight recent progress and trends in this emerging field, informing on the most promising directions and limitations towards advancing the state-of-the-art of constrained neural language generation research.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/TBBBVU4T/Garbacea et Mei - 2022 - Why is constrained neural language generation part.pdf}
}

@misc{garciaKnowITVQAAnswering2019,
  title = {{{KnowIT VQA}}: {{Answering Knowledge-Based Questions}} about {{Videos}}},
  shorttitle = {{{KnowIT VQA}}},
  author = {Garcia, Noa and Otani, Mayu and Chu, Chenhui and Nakashima, Yuta},
  year = {2019},
  month = dec,
  number = {arXiv:1910.10706},
  eprint = {arXiv:1910.10706},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.10706},
  urldate = {2023-01-24},
  abstract = {We propose a novel video understanding task by fusing knowledge-based and video question answering. First, we introduce KnowIT VQA, a video dataset with 24,282 human-generated question-answer pairs about a popular sitcom. The dataset combines visual, textual and temporal coherence reasoning together with knowledge-based questions, which need of the experience obtained from the viewing of the series to be answered. Second, we propose a video understanding model by combining the visual and textual video content with specific knowledge about the show. Our main findings are: (i) the incorporation of knowledge produces outstanding improvements for VQA in video, and (ii) the performance on KnowIT VQA still lags well behind human accuracy, indicating its usefulness for studying current video modelling limitations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/LZ5ES466/Garcia et al. - 2019 - KnowIT VQA Answering Knowledge-Based Questions ab.pdf;/home/xav/Zotero/storage/LBZ6GHAM/1910.html}
}

@incollection{gentilePersonalizedKnowledgeGraphs2019,
  title = {Personalized {{Knowledge Graphs}} for the {{Pharmaceutical Domain}}},
  booktitle = {The {{Semantic Web}} \textendash{} {{ISWC}} 2019},
  author = {Gentile, Anna Lisa and Gruhl, Daniel and Ristoski, Petar and Welch, Steve},
  editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Sv{\'a}tek, Vojt{\v e}ch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefran{\c c}ois, Maxime and Gandon, Fabien},
  year = {2019},
  volume = {11779},
  pages = {400--417},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-30796-7_25},
  urldate = {2022-05-04},
  abstract = {A considerable amount of scientific and technical content is still locked behind data formats which are not machine readable, especially PDF files - and this is particularly true in the healthcare domain. While the Semantic Web has nourished the shift to more accessible formats, in business scenarios it is critical to be able to tap into this type of content, both to extract as well as embed machine readable semantic information.},
  isbn = {978-3-030-30795-0 978-3-030-30796-7},
  langid = {english},
  file = {/home/xav/Zotero/storage/CVYBZZDN/Gentile et al. - 2019 - Personalized Knowledge Graphs for the Pharmaceutic.pdf}
}

@inproceedings{gizziCreativeProblemSolving2019,
  title = {Creative {{Problem Solving}} by {{Robots Using Action Primitive Discovery}}},
  booktitle = {2019 {{Joint IEEE}} 9th {{International Conference}} on {{Development}} and {{Learning}} and {{Epigenetic Robotics}} ({{ICDL-EpiRob}})},
  author = {Gizzi, Evana and Castro, Mateo Guaman and Sinapov, Jivko},
  year = {2019},
  month = aug,
  pages = {228--233},
  publisher = {{IEEE}},
  address = {{Oslo, Norway}},
  doi = {10.1109/DEVLRN.2019.8850711},
  urldate = {2022-11-21},
  abstract = {Humans and many other species have the remarkable ability to innovate and creatively problem solve onthe-fly. Inspired by these abilities, we propose a framework for action discovery in problem solving scenarios similar to puzzle-boxes used to evaluate intelligence in animal species. The proposed framework assumes that the robot starts with a knowledge base including predicates and actions, which, however, are insufficient to solve the problem faced by the robot. We describe a method for discovering new action primitives through object exploration and action segmentation, which is able to iteratively update the robot's knowledge base on-thefly until the solution becomes feasible. We implemented and evaluated the framework using a 3D physics-based simulated object retrieval task for the Baxter bi-manual robot. Results suggest that action segmentation is one viable path towards enabling autonomous agents to adapt on-the-fly and in short amounts of time to new situations that were unforeseen by their programmers and engineers.},
  isbn = {978-1-5386-8128-2},
  langid = {english},
  file = {/home/xav/Zotero/storage/LXM4V3E2/Gizzi et al. - 2019 - Creative Problem Solving by Robots Using Action Pr.pdf}
}

@misc{glaeseImprovingAlignmentDialogue2022,
  title = {Improving Alignment of Dialogue Agents via Targeted Human Judgements},
  author = {Glaese, Amelia and McAleese, Nat and Tr{\k{e}}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and {Campbell-Gillingham}, Lucy and Uesato, Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang, Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez and Green, Richard and Mokr{\'a}, So{\v n}a and Fernando, Nicholas and Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel, Iason and Isaac, William and Mellor, John and Hassabis, Demis and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving, Geoffrey},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14375},
  eprint = {arXiv:2209.14375},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14375},
  urldate = {2023-01-08},
  abstract = {We present Sparrow, an information-seeking dialogue agent trained to be more helpful, correct, and harmless compared to prompted language model baselines. We use reinforcement learning from human feedback to train our models with two new additions to help human raters judge agent behaviour. First, to make our agent more helpful and harmless, we break down the requirements for good dialogue into natural language rules the agent should follow, and ask raters about each rule separately. We demonstrate that this breakdown enables us to collect more targeted human judgements of agent behaviour and allows for more efficient rule-conditional reward models. Second, our agent provides evidence from sources supporting factual claims when collecting preference judgements over model statements. For factual questions, evidence provided by Sparrow supports the sampled response 78\% of the time. Sparrow is preferred more often than baselines while being more resilient to adversarial probing by humans, violating our rules only 8\% of the time when probed. Finally, we conduct extensive analyses showing that though our model learns to follow our rules it can exhibit distributional biases.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/246DZ7AE/Glaese et al. - 2022 - Improving alignment of dialogue agents via targete.pdf;/home/xav/Zotero/storage/Z35JV4PL/2209.html}
}

@inproceedings{gomez-perezFrameworkDesignComposition2004,
  title = {A {{Framework}} for {{Design}} and {{Composition}} of {{Semantic Web Services}}},
  booktitle = {Proceedings of the  {{AAAI Spring Symposium Series}} on {{Semantic Web Services}} | {{AAAI Spring Symposium Series}} on {{Semantic Web Services}} | 22-24  {{March}}  2004 | {{Stanford University}},  {{USA}}},
  author = {{G{\'o}mez-P{\'e}rez}, A. and {Gonz{\'a}lez-Cabero}, R. and Lama, M.},
  year = {2004},
  month = mar,
  publisher = {{Facultad de Inform\'atica (UPM)}},
  address = {{Stanford University,  USA}},
  urldate = {2022-11-07},
  abstract = {Semantic Web Services (SWS) are Web Services (WS) whose description is semantically enhanced with markup languages (e.g., OWL-S). This semantic description will enable external agents and programs to discover, compose and invoke SWSs. However, as a previous step to the specification of SWSs in a language, it must be designed at a conceptual level to guarantee its correctness and avoid inconsistencies among its internal components. In this paper, we present a framework for design and (semi) automatic composition of SWSs at a language-independent and knowledge level. This framework is based on a stack of ontologies that (1) describe the different parts of a SWS; and (2) contain a set of axioms that are really design rules to be verified by the ontology instances. Based on these ontologies, design and composition of SWSs can be viewed as the correct instantiation of the ontologies themselves. Once these instances have been created they will be exported to SWS languages such as OWL-S.},
  copyright = {(c) Editor/Autor},
  isbn = {978-1-57735-198-6},
  langid = {english},
  file = {/home/xav/Zotero/storage/8HTZ4J7H/Framew_Des_Comp_SW_Ser.pdf}
}

@inproceedings{goodwinZeroShotConditionalSummarization2020,
  title = {Towards {{Zero-Shot Conditional Summarization}} with {{Adaptive Multi-Task Fine-Tuning}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Goodwin, Travis and Savery, Max and {Demner-Fushman}, Dina},
  year = {2020},
  month = nov,
  pages = {3215--3226},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.findings-emnlp.289},
  urldate = {2023-02-02},
  abstract = {Automatic summarization research has traditionally focused on providing high quality general-purpose summaries of documents. However, there are many applications which require more specific summaries, such as supporting question answering or topic-based literature discovery. In this paper we study the problem of conditional summarization in which content selection and surface realization are explicitly conditioned on an ad-hoc natural language question or topic description. Because of the difficulty in obtaining sufficient reference summaries to support arbitrary conditional summarization, we explore the use of multi-task fine-tuning (MTFT) on twenty-one natural language tasks to enable zero-shot conditional summarization on five tasks. We present four new summarization datasets, two novel ``online'' or adaptive task-mixing strategies, and report zero-shot performance using T5 and BART, demonstrating that MTFT can improve zero-shot summarization quality.},
  file = {/home/xav/Zotero/storage/SYZXLI88/Goodwin et al. - 2020 - Towards Zero-Shot Conditional Summarization with A.pdf}
}

@misc{GoogleResearchPublications,
  title = {Google {{Research}}  {{Publications}}},
  journal = {Google Research},
  urldate = {2023-02-15},
  abstract = {Google publishes hundreds of research papers each year. Publishing our work enables us to collaborate and share ideas with, as well as learn from, the broader scientific community.},
  howpublished = {https://research.google/pubs/},
  langid = {english},
  file = {/home/xav/Zotero/storage/AUKRSR6B/pubs.html}
}

@article{gouKnowledgeDistillationSurvey2021,
  title = {Knowledge {{Distillation}}: {{A Survey}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen John and Tao, Dacheng},
  year = {2021},
  month = jun,
  journal = {International Journal of Computer Vision},
  volume = {129},
  number = {6},
  eprint = {2006.05525},
  pages = {1789--1819},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-021-01453-z},
  urldate = {2022-05-04},
  abstract = {In recent years, deep neural networks have been successful in both industry and academia, especially for computer vision tasks. The great success of deep learning is mainly due to its scalability to encode large-scale data and to maneuver billions of model parameters. However, it is a challenge to deploy these cumbersome deep models on devices with limited resources, e.g., mobile phones and embedded devices, not only because of the high computational complexity but also the large storage requirements. To this end, a variety of model compression and acceleration techniques have been developed. As a representative type of model compression and acceleration, knowledge distillation effectively learns a small student model from a large teacher model. It has received rapid increasing attention from the community. This paper provides a comprehensive survey of knowledge distillation from the perspectives of knowledge categories, training schemes, teacher-student architecture, distillation algorithms, performance comparison and applications. Furthermore, challenges in knowledge distillation are briefly reviewed and comments on future research are discussed and forwarded.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {QID: Q111522576},
  file = {/home/xav/Zotero/storage/LA8RKE46/Gou et al. - 2021 - Knowledge Distillation A Survey.pdf}
}

@article{goyalRetrievalAugmentedReinforcementLearning2022,
  title = {Retrieval-{{Augmented Reinforcement Learning}}},
  author = {Goyal, Anirudh and Friesen, Abram L. and Banino, Andrea and Weber, Theophane and Ke, Nan Rosemary and Badia, Adria Puigdomenech and Guez, Arthur and Mirza, Mehdi and Humphreys, Peter C. and Konyushkova, Ksenia and Sifre, Laurent and Valko, Michal and Osindero, Simon and Lillicrap, Timothy and Heess, Nicolas and Blundell, Charles},
  year = {2022},
  month = mar,
  journal = {arXiv:2202.08417 [cs]},
  eprint = {2202.08417},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Most deep reinforcement learning (RL) algorithms distill experience into parametric behavior policies or value functions via gradient updates. While effective, this approach has several disadvantages: (1) it is computationally expensive, (2) it can take many updates to integrate experiences into the parametric model, (3) experiences that are not fully integrated do not appropriately influence the agent's behavior, and (4) behavior is limited by the capacity of the model. In this paper we explore an alternative paradigm in which we train a network to map a dataset of past experiences to optimal behavior. Specifically, we augment an RL agent with a retrieval process (parameterized as a neural network) that has direct access to a dataset of experiences. This dataset can come from the agent's past experiences, expert demonstrations, or any other relevant source. The retrieval process is trained to retrieve information from the dataset that may be useful in the current context, to help the agent achieve its goal faster and more efficiently. We integrate our method into two different RL agents: an offline DQN agent and an online R2D2 agent. In offline multi-task problems, we show that the retrieval-augmented DQN agent avoids task interference and learns faster than the baseline DQN agent. On Atari, we show that retrieval-augmented R2D2 learns significantly faster than the baseline R2D2 agent and achieves higher scores. We run extensive ablations to measure the contributions of the components of our proposed method.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/285G7JAW/Goyal et al. - 2022 - Retrieval-Augmented Reinforcement Learning.pdf}
}

@misc{gozalo-brizuelaChatGPTNotAll2023,
  title = {{{ChatGPT}} Is Not All You Need. {{A State}} of the {{Art Review}} of Large {{Generative AI}} Models},
  author = {{Gozalo-Brizuela}, Roberto and {Garrido-Merchan}, Eduardo C.},
  year = {2023},
  month = jan,
  number = {arXiv:2301.04655},
  eprint = {arXiv:2301.04655},
  publisher = {{arXiv}},
  urldate = {2023-02-09},
  abstract = {During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/PSF4MFKH/Gozalo-Brizuela et Garrido-Merchan - 2023 - ChatGPT is not all you need. A State of the Art Re.pdf}
}

@article{graesserComplexProblemSolving2017,
  title = {Complex {{Problem Solving}} in {{Assessments}} of {{Collaborative Problem Solving}}},
  author = {Graesser, Arthur and Kuo, Bor-Chen and Liao, Chen-Huei},
  year = {2017},
  month = jun,
  journal = {Journal of Intelligence},
  volume = {5},
  number = {2},
  pages = {10},
  issn = {2079-3200},
  doi = {10.3390/jintelligence5020010},
  urldate = {2023-02-23},
  abstract = {Collaborative problem solving (ColPS) proficiency was developed as a new assessment for the Programme for International Student Assessment (PISA) in the 2015 international evaluation of student skills and knowledge. The assessment framework defined by the PISA ColPS 2015 expert group crossed three major collaboration processes with four problem solving processes that were adopted from the PISA 2012 individual problem solving assessment to form a matrix of 12 specific skills. The three major collaboration processes are (1) establishing and maintaining shared understanding; (2) taking appropriate action; and (3) establishing and maintaining team organization. The four problem solving processes are exploring and understanding the problem, representing and formulating the problem, planning and executing strategies, and monitoring and reflecting on the problem-solving activities. This article discusses how the problem-solving dimension was integrated with the collaboration dimension. We also discuss how computer agents were involved in the PISA ColPS 2015 assessment in order to ensure a satisfactory assessment of collaborative problem solving. Examples of the use of agents to assess ColPS are provided in the context of a released PISA item and a project conducted in Taiwan.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {agents,collaborative problem solving,conversation-based assessment},
  file = {/home/xav/Zotero/storage/V2INVFXZ/Graesser et al. - 2017 - Complex Problem Solving in Assessments of Collabor.pdf}
}

@phdthesis{grailContributionLectureAutomatique2021,
  title = {Contribution \`a La Lecture Automatique \`a l'aide de R\'eseaux Neuronaux Profonds},
  author = {Grail, Quentin},
  year = {2021},
  abstract = {La compr\'ehension automatique du langage naturel est un d\'efi important de l'intelligence artificielle.Dans cette dissertation, nous d\'ecrivons l'ensemble de nos contributions apport\'ees \`a ce domaine.Nous pr\'esentons plusieurs directions que nous pensons cruciales \`a la construction de meilleurs syst\`emes de traitement automatique du langage naturel.La premi\`ere partie de cette dissertation couvre certains concepts essentiels notamment en proposant un historique rapide de la repr\'esentation vectorielle de mots ainsi que des t\^aches de lecture et de r\'esum\'e automatique de texte.Cette partie d\'ecrit certains des principaux objectifs qui ont guid\'es la recherche durant ces derni\`eres ann\'ees jusqu'\`a la r\'ecente r\'evolution de l'apprentissage profond appliqu\'ee au traitement du langage naturel.Le premier th\`eme d\'evelopp\'e dans cette th\`ese concerne la compr\'ehension automatique de texte au travers de la t\^ache de question-r\'eponse.Nos contributions dans ce domaine sont li\'ees \`a trois aspects principaux : les donn\'ees d'\'evaluation, les algorithmes d'apprentissage, la construction de nouveaux mod\`eles.Dans ce premier th\`eme, nous proposons un jeu de donn\'ees de question-r\'eponse permettant d'\'evaluer les comp\'etences de raisonnement relationnel du syst\`eme de lecture.Ensuite, nous proposons un protocole d'apprentissage adversarial ayant pour but de g\'en\'erer automatiquement des exemples bruit\'es afin d'am\'eliorer les performances du mod\`ele de lecture.Finalement, nous d\'ecrivons nos travaux propos\'es dans le cadre de question-r\'eponse multi-hop. La t\^ache de question-r\'eponse est assez g\'en\'erale et de nouveaux types de questions ont \'emerg\'es ces derni\`eres ann\'ees dans le but d'\'evaluer diff\'erentes comp\'etences des mod\`eles de lecture.Les questions multi-hop font partie de ces nouvelles directions et n\'ecessite au lecteur de collecter de l'information dans plusieurs parties de documents afin de r\'epondre correctement \`a une question.Nous pensons que cette t\^ache est un pas de plus vers la construction de meilleurs mod\`eles de compr\'ehension du langage et proposons notre contribution au travers d'un mod\`ele de lecture efficace et interpr\'etable.L'explosion de l'apprentissage profond associ\'e \`a l'augmentation de la puissance de calcul des machines modernes \`a conduit \`a des progr\`es remarquables dans le domaine du traitement du langage naturel.Cependant, les r\'ecentes architectures d\'evelopp\'ees ont tendance \`a \^etre \'evalu\'ees sur des t\^aches n\'ecessitant de lire uniquement des textes de taille relativement mod\'er\'ee.Le deuxi\`eme th\`eme couvert dans cette th\`ese concerne l'apprentissage de repr\'esentations de textes longs en utilisant diff\'erentes architectures d'apprentissage profond \'etat de l'art.Nous d\'ecrivons notre proposition ayant pour but d'am\'eliorer les r\'ecentes approches propos\'ees, en les adaptant pour des t\^aches n\'ecessitant le traitement de documents longs.Nous avons \'evalu\'e cette proposition sur une t\^ache de r\'esum\'e extractif de textes scientifiques et pr\'esentons des r\'esultats encourageants ne n\'ecessitant qu'une adaptation minimale des architectures existantes.},
  school = {Universit\'e Grenoble Alpes},
  file = {/home/xav/Zotero/storage/U9HY8SP6/Grail - 2021 - Contribution √† la lecture automatique √† l‚Äôaide de .pdf}
}

@article{greenwellApproachSemanticIntelligence,
  title = {An {{Approach}} to the {{Semantic Intelligence Cloud}}},
  author = {Greenwell, Richard},
  pages = {244},
  langid = {english},
  file = {/home/xav/Zotero/storage/XYYWCM8B/Greenwell - An Approach to the Semantic Intelligence Cloud.pdf}
}

@misc{grillBootstrapYourOwn2020,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  month = sep,
  number = {arXiv:2006.07733},
  eprint = {arXiv:2006.07733},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.07733},
  urldate = {2023-02-02},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/CY95678Y/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf;/home/xav/Zotero/storage/VDBJZRFF/2006.html}
}

@misc{guanWideningPipelineHumanGuided2021,
  title = {Widening the {{Pipeline}} in {{Human-Guided Reinforcement Learning}} with {{Explanation}} and {{Context-Aware Data Augmentation}}},
  author = {Guan, Lin and Verma, Mudit and Guo, Sihang and Zhang, Ruohan and Kambhampati, Subbarao},
  year = {2021},
  month = oct,
  number = {arXiv:2006.14804},
  eprint = {arXiv:2006.14804},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.14804},
  urldate = {2022-10-28},
  abstract = {Human explanation (e.g., in terms of feature importance) has been recently used to extend the communication channel between human and agent in interactive machine learning. Under this setting, human trainers provide not only the ground truth but also some form of explanation. However, this kind of human guidance was only investigated in supervised learning tasks, and it remains unclear how to best incorporate this type of human knowledge into deep reinforcement learning. In this paper, we present the first study of using human visual explanations in human-in-the-loop reinforcement learning (HRL). We focus on the task of learning from feedback, in which the human trainer not only gives binary evaluative "good" or "bad" feedback for queried state-action pairs, but also provides a visual explanation by annotating relevant features in images. We propose EXPAND (EXPlanation AugmeNted feeDback) to encourage the model to encode task-relevant features through a context-aware data augmentation that only perturbs irrelevant features in human salient information. We choose five tasks, namely Pixel-Taxi and four Atari games, to evaluate the performance and sample efficiency of this approach. We show that our method significantly outperforms methods leveraging human explanation that are adapted from supervised learning, and Human-in-the-loop RL baselines that only utilize evaluative feedback.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/xav/Zotero/storage/GTKKAF47/Guan et al. - 2021 - Widening the Pipeline in Human-Guided Reinforcemen.pdf}
}

@article{guoCycleGTUnsupervisedGraphtoText2020,
  title = {{{CycleGT}}: {{Unsupervised Graph-to-Text}} and {{Text-to-Graph Generation}} via {{Cycle Training}}},
  shorttitle = {{{CycleGT}}},
  author = {Guo, Qipeng and Jin, Zhijing and Qiu, Xipeng and Zhang, Weinan and Wipf, David and Zhang, Zheng},
  year = {2020},
  month = dec,
  journal = {arXiv:2006.04702 [cs]},
  eprint = {2006.04702},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Two important tasks at the intersection of knowledge graphs and natural language processing are graph-to-text (G2T) and text-to-graph (T2G) conversion. Due to the difficulty and high cost of data collection, the supervised data available in the two fields are usually on the magnitude of tens of thousands, for example, 18K in the WebNLG\textasciitilde 2017 dataset after preprocessing, which is far fewer than the millions of data for other tasks such as machine translation. Consequently, deep learning models for G2T and T2G suffer largely from scarce training data. We present CycleGT, an unsupervised training method that can bootstrap from fully non-parallel graph and text data, and iteratively back translate between the two forms. Experiments on WebNLG datasets show that our unsupervised model trained on the same number of data achieves performance on par with several fully supervised models. Further experiments on the non-parallel GenWiki dataset verify that our method performs the best among unsupervised baselines. This validates our framework as an effective approach to overcome the data scarcity problem in the fields of G2T and T2G. Our code is available at https://github.com/QipengGuo/CycleGT.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {QID: Q104835890},
  file = {/home/xav/Zotero/storage/TCCZ6KGL/Guo et al. - 2020 - CycleGT Unsupervised Graph-to-Text and Text-to-Gr.pdf}
}

@misc{guoSurveyAutomatedFactChecking2022,
  title = {A {{Survey}} on {{Automated Fact-Checking}}},
  author = {Guo, Zhijiang and Schlichtkrull, Michael and Vlachos, Andreas},
  year = {2022},
  month = jun,
  number = {arXiv:2108.11896},
  eprint = {arXiv:2108.11896},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.11896},
  urldate = {2023-02-08},
  abstract = {Fact-checking has become increasingly important due to the speed with which both information and misinformation can spread in the modern media ecosystem. Therefore, researchers have been exploring how fact-checking can be automated, using techniques based on natural language processing, machine learning, knowledge representation, and databases to automatically predict the veracity of claims. In this paper, we survey automated fact-checking stemming from natural language processing, and discuss its connections to related tasks and disciplines. In this process, we present an overview of existing datasets and models, aiming to unify the various definitions given and identify common concepts. Finally, we highlight challenges for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/LMBAHMTZ/Guo et al. - 2022 - A Survey on Automated Fact-Checking.pdf;/home/xav/Zotero/storage/MXTFCS78/2108.html}
}

@misc{guptaDisflQABenchmarkDataset2021,
  title = {Disfl-{{QA}}: {{A Benchmark Dataset}} for {{Understanding Disfluencies}} in {{Question Answering}}},
  shorttitle = {Disfl-{{QA}}},
  author = {Gupta, Aditya and Xu, Jiacheng and Upadhyay, Shyam and Yang, Diyi and Faruqui, Manaal},
  year = {2021},
  month = jun,
  number = {arXiv:2106.04016},
  eprint = {arXiv:2106.04016},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.04016},
  urldate = {2023-01-16},
  abstract = {Disfluencies is an under-studied topic in NLP, even though it is ubiquitous in human conversation. This is largely due to the lack of datasets containing disfluencies. In this paper, we present a new challenge question answering dataset, Disfl-QA, a derivative of SQuAD, where humans introduce contextual disfluencies in previously fluent questions. Disfl-QA contains a variety of challenging disfluencies that require a more comprehensive understanding of the text than what was necessary in prior datasets. Experiments show that the performance of existing state-of-the-art question answering models degrades significantly when tested on Disfl-QA in a zero-shot setting.We show data augmentation methods partially recover the loss in performance and also demonstrate the efficacy of using gold data for fine-tuning. We argue that we need large-scale disfluency datasets in order for NLP models to be robust to them. The dataset is publicly available at: https://github.com/google-research-datasets/disfl-qa.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/KFPZ78NU/Gupta et al. - 2021 - Disfl-QA A Benchmark Dataset for Understanding Di.pdf;/home/xav/Zotero/storage/D37D9VXB/2106.html}
}

@misc{gururanganDEMixLayersDisentangling2021,
  title = {{{DEMix Layers}}: {{Disentangling Domains}} for {{Modular Language Modeling}}},
  shorttitle = {{{DEMix Layers}}},
  author = {Gururangan, Suchin and Lewis, Mike and Holtzman, Ari and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2021},
  month = aug,
  number = {arXiv:2108.05036},
  eprint = {arXiv:2108.05036},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.05036},
  urldate = {2023-02-10},
  abstract = {We introduce a new domain expert mixture (DEMix) layer that enables conditioning a language model (LM) on the domain of the input text. A DEMix layer is a collection of expert feedforward networks, each specialized to a domain, that makes the LM modular: experts can be mixed, added or removed after initial training. Extensive experiments with autoregressive transformer LMs (up to 1.3B parameters) show that DEMix layers reduce test-time perplexity, increase training efficiency, and enable rapid adaptation with little overhead. We show that mixing experts during inference, using a parameter-free weighted ensemble, allows the model to better generalize to heterogeneous or unseen domains. We also show that experts can be added to iteratively incorporate new domains without forgetting older ones, and that experts can be removed to restrict access to unwanted domains, without additional training. Overall, these results demonstrate benefits of explicitly conditioning on textual domains during language modeling.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/SMA9HXH3/Gururangan et al. - 2021 - DEMix Layers Disentangling Domains for Modular La.pdf;/home/xav/Zotero/storage/NGTX99Z5/2108.html}
}

@article{guThreeLevelsGeneralization2021,
  title = {Beyond {{I}}.{{I}}.{{D}}.: {{Three Levels}} of {{Generalization}} for {{Question Answering}} on {{Knowledge Bases}}},
  shorttitle = {Beyond {{I}}.{{I}}.{{D}}.},
  author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
  year = {2021},
  month = apr,
  journal = {Proceedings of the Web Conference 2021},
  eprint = {2011.07743},
  pages = {3477--3488},
  doi = {10.1145/3442381.3449992},
  urldate = {2022-05-04},
  abstract = {Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d may be neither reasonably achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sample training examples from the enormous space would be highly data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d, compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  annotation = {QID: Q102154713},
  file = {/home/xav/Zotero/storage/SMNUDFE3/Gu et al. - 2021 - Beyond I.I.D. Three Levels of Generalization for .pdf}
}

@article{guuREALMRetrievalAugmentedLanguage2020,
  title = {{{REALM}}: {{Retrieval-Augmented Language Model Pre-Training}}},
  shorttitle = {{{REALM}}},
  author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.08909 [cs]},
  eprint = {2002.08909},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {QID: Q84876883},
  file = {/home/xav/Zotero/storage/QPJMZRJH/Guu et al. - 2020 - REALM Retrieval-Augmented Language Model Pre-Trai.pdf;/home/xav/Zotero/storage/GCMPSK9I/2002.html}
}

@article{haCollectiveIntelligenceDeep2022,
  title = {Collective {{Intelligence}} for {{Deep Learning}}: {{A Survey}} of {{Recent Developments}}},
  shorttitle = {Collective {{Intelligence}} for {{Deep Learning}}},
  author = {Ha, David and Tang, Yujin},
  year = {2022},
  month = mar,
  journal = {arXiv:2111.14377 [cs]},
  eprint = {2111.14377},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {In the past decade, we have witnessed the rise of deep learning to dominate the field of artificial intelligence. Advances in artificial neural networks alongside corresponding advances in hardware accelerators with large memory capacity, together with the availability of large datasets enabled practitioners to train and deploy sophisticated neural network models that achieve state-of-the-art performance on tasks across several fields spanning computer vision, natural language processing, and reinforcement learning. However, as these neural networks become bigger, more complex, and more widely used, fundamental problems with current deep learning models become more apparent. State-of-the-art deep learning models are known to suffer from issues that range from poor robustness, inability to adapt to novel task settings, to requiring rigid and inflexible configuration assumptions. Collective behavior, commonly observed in nature, tends to produce systems that are robust, adaptable, and have less rigid assumptions about the environment configuration. Collective intelligence, as a field, studies the group intelligence that emerges from the interactions of many individuals. Within this field, ideas such as self-organization, emergent behavior, swarm optimization, and cellular automata were developed to model and explain complex systems. It is therefore natural to see these ideas incorporated into newer deep learning methods. In this review, we will provide a historical context of neural network research's involvement with complex systems, and highlight several active areas in modern deep learning research that incorporate the principles of collective intelligence to advance its current capabilities. We hope this review can serve as a bridge between the complex systems and deep learning communities.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/home/xav/Zotero/storage/Q2ZFL5L6/Ha et Tang - 2022 - Collective Intelligence for Deep Learning A Surve.pdf}
}

@misc{haluptzokLanguageModelsCan2022,
  title = {Language {{Models Can Teach Themselves}} to {{Program Better}}},
  author = {Haluptzok, Patrick and Bowers, Matthew and Kalai, Adam Tauman},
  year = {2022},
  month = sep,
  number = {arXiv:2207.14502},
  eprint = {arXiv:2207.14502},
  publisher = {{arXiv}},
  urldate = {2023-01-05},
  abstract = {Recent Language Models (LMs) achieve breakthrough performance in code generation when trained on human-authored problems, even solving some competitive-programming problems. Self-play has proven useful in games such as Go, and thus it is natural to ask whether LMs can generate their own instructive programming problems to improve their performance. We show that it is possible for an LM to synthesize programming problems and solutions, which are filtered for correctness by a Python interpreter. The LM's performance is then seen to improve when it is fine-tuned on its own synthetic problems and verified solutions; thus the model ``improves itself'' using the Python interpreter. Problems are specified formally as programming puzzles [Schuster et al., 2021], a code-based problem format where solutions can easily be verified for correctness by execution. In experiments on publicly-available LMs, test accuracy more than doubles. This work demonstrates the potential for code LMs, with an interpreter, to generate instructive problems and improve their own performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/NJYFSRHB/Haluptzok et al. - 2022 - Language Models Can Teach Themselves to Program Be.pdf}
}

@inproceedings{hambardzumyanWARPWordlevelAdversarial2021,
  title = {{{WARP}}: {{Word-level Adversarial ReProgramming}}},
  shorttitle = {{{WARP}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Hambardzumyan, Karen and Khachatrian, Hrant and May, Jonathan},
  year = {2021},
  month = aug,
  pages = {4921--4933},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.381},
  urldate = {2022-05-04},
  abstract = {Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.},
  file = {/home/xav/Zotero/storage/YV3NMM4Y/Hambardzumyan et al. - 2021 - WARP Word-level Adversarial ReProgramming.pdf}
}

@article{hamiltonNeuroSymbolicAIMeeting2022,
  title = {Is {{Neuro-Symbolic AI Meeting}} Its {{Promise}} in {{Natural Language Processing}}? {{A Structured Review}}},
  shorttitle = {Is {{Neuro-Symbolic AI Meeting}} Its {{Promise}} in {{Natural Language Processing}}?},
  author = {Hamilton, Kyle and Nayak, Aparna and Bo{\v z}i{\'c}, Bojan and Longo, Luca},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.12205 [cs]},
  eprint = {2202.12205},
  primaryclass = {cs},
  urldate = {2022-05-09},
  abstract = {Advocates for Neuro-Symbolic AI (NeSy) assert that combining deep learning with symbolic reasoning will lead to stronger AI than either paradigm on its own. As successful as deep learning has been, it is generally accepted that even our best deep learning systems are not very good at abstract reasoning. And since reasoning is inextricably linked to language, it makes intuitive sense that Natural Language Processing (NLP), would be a particularly well-suited candidate for NeSy. We conduct a structured review of studies implementing NeSy for NLP, challenges and future directions, and aim to answer the question of whether NeSy is indeed meeting its promises: reasoning, out-of-distribution generalization, interpretability, learning and reasoning from small data, and transferability to new domains. We examine the impact of knowledge representation, such as rules and semantic networks, language structure and relational structure, and whether implicit or explicit reasoning contributes to higher promise scores. We find that knowledge encoded in relational structures and explicit reasoning tend to lead to more NeSy goals being satisfied. We also advocate for a more methodical approach to the application of theories of reasoning, which we hope can reduce some of the friction between the symbolic and sub-symbolic schools of AI.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/5FELVB2D/Hamilton et al. - 2022 - Is Neuro-Symbolic AI Meeting its Promise in Natura.pdf}
}

@inproceedings{hanESTERMachineReading2021,
  title = {{{ESTER}}: {{A Machine Reading Comprehension Dataset}} for {{Reasoning}} about {{Event Semantic Relations}}},
  shorttitle = {{{ESTER}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Han, Rujun and Hsu, I-Hung and Sun, Jiao and Baylon, Julia and Ning, Qiang and Roth, Dan and Peng, Nanyun},
  year = {2021},
  month = nov,
  pages = {7543--7559},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.597},
  urldate = {2023-02-01},
  abstract = {Understanding how events are semantically related to each other is the essence of reading comprehension. Recent event-centric reading comprehension datasets focus mostly on event arguments or temporal relations. While these tasks partially evaluate machines' ability of narrative understanding, human-like reading comprehension requires the capability to process event-based information beyond arguments and temporal reasoning. For example, to understand causality between events, we need to infer motivation or purpose; to establish event hierarchy, we need to understand the composition of events. To facilitate these tasks, we introduce **ESTER**, a comprehensive machine reading comprehension (MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages natural language queries to reason about the five most common event semantic relations, provides more than 6K questions, and captures 10.1K event relation pairs. Experimental results show that the current SOTA systems achieve 22.1\%, 63.3\% and 83.5\% for token-based exact-match (**EM**), **F1** and event-based **HIT@1** scores, which are all significantly below human performances (36.0\%, 79.6\%, 100\% respectively), highlighting our dataset as a challenging benchmark.},
  file = {/home/xav/Zotero/storage/JCD9WNW8/Han et al. - 2021 - ESTER A Machine Reading Comprehension Dataset for.pdf}
}

@inproceedings{hanifiApplicationFMEABased2021,
  title = {Application of an {{FMEA Based Method}} to {{Prioritize}} the {{Initial Problem Choices}} in {{Inventive Design}}},
  booktitle = {Creative {{Solutions}} for a {{Sustainable Development}}},
  author = {Hanifi, Masih and Chibane, Hicham and Houssin, Remy and Cavallucci, Denis},
  editor = {Borgianni, Yuri and Brad, Stelian and Cavallucci, Denis and Livotov, Pavel},
  year = {2021},
  series = {{{IFIP Advances}} in {{Information}} and {{Communication Technology}}},
  pages = {233--244},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-86614-3_19},
  abstract = {Initial Analysis of a complex situation is one of the most vital phase in inventive design. To ensure an exhaustive and formal method to draw a knowledge representation model, a problem-graph is proposed. However, one of the criticisms often leveled is that the application of these methods is time-consuming. For this reason, the Inverse Problem Graph method was introduced to increase the agility of the inventive design process through the beginning of the problem formulation from an initial problem, located in the lower level of a problem situation. Nevertheless, the way designers should select the most important initial problem among the others is not treated. The purpose of this article is to integrate a Failure Mode Effect Analysis (FMEA) based method into the IPG method in order to prioritize the initial problems in the initial analysis phase. The capability of the proposal is finally tested through its application in a case study.},
  isbn = {978-3-030-86614-3},
  langid = {english},
  keywords = {AHP,Complex problem,FMEA,Inventive design,TRIZ}
}

@misc{hanWikiCSSHExtractingEvaluating2021,
  title = {{{WikiCSSH}}: {{Extracting}} and {{Evaluating Computer Science Subject Headings}} from {{Wikipedia}}},
  shorttitle = {{{WikiCSSH}}},
  author = {Han, Kanyao and Yang, Pingjing and Mishra, Shubhanshu and Diesner, Jana},
  year = {2021},
  month = sep,
  number = {arXiv:2109.04945},
  eprint = {arXiv:2109.04945},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.04945},
  urldate = {2022-11-18},
  abstract = {Hierarchical domain-specific classification schemas (or subject heading vocabularies) are often used to identify, classify, and disambiguate concepts that occur in scholarly articles. In this work, we develop, apply, and evaluate a human-in-the-loop workflow that first extracts an initial category tree from crowd-sourced Wikipedia data, and then combines community detection, machine learning, and hand-crafted heuristics or rules to prune the initial tree. This work resulted in WikiCSSH; a large-scale, hierarchically organized vocabulary for the domain of computer science (CS). Our evaluation suggests that WikiCSSH outperforms alternative CS vocabularies in terms of vocabulary size as well as the performance of lexicon-based key-phrase extraction from scholarly data. WikiCSSH can further distinguish between coarse-grained versus fine-grained CS concepts. The outlined workflow can serve as a template for building hierarchically-organized subject heading vocabularies for other domains that are covered in Wikipedia.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Digital Libraries,Computer Science - Social and Information Networks},
  file = {/home/xav/Zotero/storage/AE75JBKC/Han et al. - 2021 - WikiCSSH Extracting and Evaluating Computer Scien.pdf}
}

@inproceedings{haoIronPrivateInference2022,
  title = {Iron : {{Private Inference}} on {{Transformers}}},
  shorttitle = {Iron},
  author = {Hao, Meng and Li, Hongwei and Chen, Hanxiao and Xing, Pengzhi and Xu, Guowen and Zhang, Tianwei},
  year = {2022},
  urldate = {2023-02-08},
  abstract = {We initiate the study of private inference on Transformer-based models in the client-server setting, where clients have private inputs and servers hold proprietary models. Our main contribution is to provide several new secure protocols for matrix multiplication and complex non-linear functions like Softmax, GELU activations, and LayerNorm, which are critical components of Transformers. Specifically, we first propose a customized homomorphic encryption-based protocol for matrix multiplication that crucially relies on a novel compact packing technique. This design achieves {$\surd$} m \texttimes{} less communication ( m is the number of rows of the output matrix) over the most efficient work. Second, we design efficient protocols for three non-linear functions via integrating advanced underlying protocols and specialized optimizations. Compared to the state-of-the-art protocols, our recipes reduce about half of the communication and computation overhead. Furthermore, all protocols are numerically precise, which preserve the model accuracy of plaintext. These techniques together allow us to implement Iron , an efficient Transformer-based private inference framework. Experiments conducted on several real-world datasets and models demonstrate that Iron achieves 3 {$\sim$} 14 \texttimes{} less communication and 3 {$\sim$} 11 \texttimes{} less runtime compared to the prior art.},
  file = {/home/xav/Zotero/storage/93SAHVNC/Hao et al. - 2022 - Iron  Private Inference on Transformers.pdf}
}

@misc{haoLanguageModelsAre2022,
  title = {Language {{Models}} Are {{General-Purpose Interfaces}}},
  author = {Hao, Yaru and Song, Haoyu and Dong, Li and Huang, Shaohan and Chi, Zewen and Wang, Wenhui and Ma, Shuming and Wei, Furu},
  year = {2022},
  month = jun,
  number = {arXiv:2206.06336},
  eprint = {arXiv:2206.06336},
  publisher = {{arXiv}},
  urldate = {2022-07-18},
  abstract = {Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling incontext learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/DE5FYALJ/Hao et al. - 2022 - Language Models are General-Purpose Interfaces.pdf}
}

@misc{haoStructuredPromptingScaling2022,
  title = {Structured {{Prompting}}: {{Scaling In-Context Learning}} to 1,000 {{Examples}}},
  shorttitle = {Structured {{Prompting}}},
  author = {Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},
  year = {2022},
  month = dec,
  number = {arXiv:2212.06713},
  eprint = {arXiv:2212.06713},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.06713},
  urldate = {2023-01-17},
  abstract = {Large language models have exhibited intriguing in-context learning capability, achieving promising zero- and few-shot performance without updating the parameters. However, conventional in-context learning is usually restricted by length constraints, rendering it ineffective to absorb supervision from a large number of examples. In order to go beyond few shots, we introduce structured prompting that breaks the length limit and scales in-context learning to thousands of examples. Specifically, demonstration examples are separately encoded with well-designed position embeddings, and then they are jointly attended by the test example using a rescaled attention mechanism. So we can scale the number of exemplars with linear complexity instead of quadratic complexity with respect to length. Experimental results on a diverse set of tasks show that our approach improves end-task performance and reduces evaluation variance over conventional in-context learning as the number of demonstration examples increases. Code has been released at https://aka.ms/structured-prompting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/76Q7RQYM/Hao et al. - 2022 - Structured Prompting Scaling In-Context Learning .pdf}
}

@misc{haqueThinkThisMost2022,
  title = {"{{I}} Think This Is the Most Disruptive Technology": {{Exploring Sentiments}} of {{ChatGPT Early Adopters}} Using {{Twitter Data}}},
  shorttitle = {"{{I}} Think This Is the Most Disruptive Technology"},
  author = {Haque, Mubin Ul and Dharmadasa, Isuru and Sworna, Zarrin Tasnim and Rajapakse, Roshan Namal and Ahmad, Hussain},
  year = {2022},
  month = dec,
  number = {arXiv:2212.05856},
  eprint = {arXiv:2212.05856},
  publisher = {{arXiv}},
  urldate = {2023-01-22},
  abstract = {Large language models have recently attracted significant attention due to their impressive performance on a variety of tasks. ChatGPT developed by OpenAI is one such implementation of a large, pre-trained language model that has gained immense popularity among early adopters, where certain users go to the extent of characterizing it as a disruptive technology in many domains. Understanding such early adopters' sentiments is important because it can provide insights into the potential success or failure of the technology, as well as its strengths and weaknesses. In this paper, we conduct a mixedmethod study using 10,732 tweets from early ChatGPT users. We first use topic modelling to identify the main topics and then perform an in-depth qualitative sentiment analysis of each topic. Our results show that the majority of the early adopters have expressed overwhelmingly positive sentiments related to topics such as Disruptions to software development, Entertainment and exercising creativity. Only a limited percentage of users expressed concerns about issues such as the potential for misuse of ChatGPT, especially regarding topics such as Impact on educational aspects. We discuss these findings by providing specific examples for each topic and then detail implications related to addressing these concerns for both researchers and users.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/35MRNSGT/Haque et al. - 2022 - I think this is the most disruptive technology .pdf}
}

@inproceedings{hashemiANTIQUENonfactoidQuestion2020,
  title = {{{ANTIQUE}}: {{A Non-factoid Question Answering Benchmark}}},
  shorttitle = {{{ANTIQUE}}},
  booktitle = {Advances in {{Information Retrieval}}},
  author = {Hashemi, Helia and Aliannejadi, Mohammad and Zamani, Hamed and Croft, W. Bruce},
  editor = {Jose, Joemon M. and Yilmaz, Emine and Magalh{\~a}es, Jo{\~a}o and Castells, Pablo and Ferro, Nicola and Silva, M{\'a}rio J. and Martins, Fl{\'a}vio},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {166--173},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-45442-5_21},
  abstract = {Considering the widespread use of mobile and voice search, answer passage retrieval for non-factoid questions plays a critical role in modern information retrieval systems. Despite the importance of the task, the community still feels the significant lack of large-scale non-factoid question answering collections with real questions and comprehensive relevance judgments. In this paper, we develop and release a collection of 2,626 open-domain non-factoid questions from a diverse set of categories. The dataset, called ANTIQUE, contains 34k manual relevance annotations. The questions were asked by real users in a community question answering service, i.e., Yahoo! Answers. Relevance judgments for all the answers to each question were collected through crowdsourcing. To facilitate further research, we also include a brief analysis of the data as well as baseline results on both classical and neural IR models.},
  isbn = {978-3-030-45442-5},
  langid = {english},
  file = {/home/xav/Zotero/storage/K7QTXTNJ/Hashemi et al. - 2020 - ANTIQUE A Non-factoid Question Answering Benchmar.pdf}
}

@misc{heBlindSpotsModelBased2022,
  title = {On the {{Blind Spots}} of {{Model-Based Evaluation Metrics}} for {{Text Generation}}},
  author = {He, Tianxing and Zhang, Jingyu and Wang, Tianle and Kumar, Sachin and Cho, Kyunghyun and Glass, James and Tsvetkov, Yulia},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10020},
  eprint = {arXiv:2212.10020},
  publisher = {{arXiv}},
  urldate = {2023-01-22},
  abstract = {In this work, we explore a useful but often neglected methodology for robustness analysis of text generation evaluation metrics: stress tests with synthetic data. Basically, we design and synthesize a wide range of potential errors and check whether they result in a commensurate drop in the metric scores. We examine a range of recently proposed evaluation metrics based on pretrained language models, for the tasks of open-ended generation, translation, and summarization. Our experiments reveal interesting insensitivities, biases, or even loopholes in existing metrics. For example, we find that BERTScore ignores truncation errors in summarization, and MAUVE (built on top of GPT-2) is insensitive to errors at the beginning of generations. Further, we investigate the reasons behind these blind spots and suggest practical workarounds for a more reliable evaluation of text generation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/CKCK6PND/He et al. - 2022 - On the Blind Spots of Model-Based Evaluation Metri.pdf}
}

@article{heCollectiveIntelligenceTaxonomy2019,
  title = {Collective {{Intelligence}}: {{A Taxonomy}} and {{Survey}}},
  shorttitle = {Collective {{Intelligence}}},
  author = {He, Feijuan and Pan, Yudai and Lin, Qika and Miao, Xianglin and Chen, Zhouguo},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {170213--170225},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2955677},
  abstract = {Collective intelligence (CI) refers to the intelligence that emerges at the macro-level of a collection and transcends that of the individuals. CI is a continuously popular research topic that is studied by researchers in different areas, such as sociology, economics, biology, and artificial intelligence. In this survey, we summarize the works of CI in various fields. First, according to the existence of interactions between individuals and the feedback mechanism in the aggregation process, we establish CI taxonomy that includes three paradigms: isolation, collaboration and feedback. We then conduct statistical literature analysis to explain the differences among three paradigms and their development in recent years. Second, we elaborate the types of CI under each paradigm and discuss the generation mechanism or theoretical basis of the different types of CI. Third, we describe certain CI-related applications in 2019, which can be appropriately categorized by our proposed taxonomy. Finally, we summarize the future research directions of CI under each paradigm. We hope that this survey helps researchers understand the current conditions of CI and clears the directions of future research.},
  keywords = {Collaboration,Collective intelligence,emergence,Forecasting,paradigm,survey,synergetics,Tag clouds,Task analysis,Taxonomy},
  file = {/home/xav/Zotero/storage/DEEPVBYJ/He et al. - 2019 - Collective Intelligence A Taxonomy and Survey.pdf}
}

@inproceedings{heEffectivenessAdapterbasedTuning2021,
  title = {On the {{Effectiveness}} of {{Adapter-based Tuning}} for {{Pretrained Language Model Adaptation}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {He, Ruidan and Liu, Linlin and Ye, Hai and Tan, Qingyu and Ding, Bosheng and Cheng, Liying and Low, Jiawei and Bing, Lidong and Si, Luo},
  year = {2021},
  month = aug,
  pages = {2208--2222},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.172},
  urldate = {2023-02-02},
  abstract = {Adapter-based tuning has recently arisen as an alternative to fine-tuning. It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task. As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing. Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning. However, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness. In this paper, we study the latter. We first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM. We then empirically compare the two tuning methods on several downstream NLP tasks and settings. We demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates.},
  file = {/home/xav/Zotero/storage/AWSPCKBZ/He et al. - 2021 - On the Effectiveness of Adapter-based Tuning for P.pdf}
}

@article{hegdeUnsupervisedParaphraseGeneration2020,
  title = {Unsupervised {{Paraphrase Generation}} Using {{Pre-trained Language Models}}},
  author = {Hegde, Chaitra and Patil, Shrikumar},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.05477 [cs]},
  eprint = {2006.05477},
  primaryclass = {cs},
  urldate = {2022-05-14},
  abstract = {Large scale Pre-trained Language Models have proven to be very powerful approach in various Natural language tasks. OpenAI's GPT-2 \textbackslash cite\{radford2019language\} is notable for its capability to generate fluent, well formulated, grammatically consistent text and for phrase completions. In this paper we leverage this generation capability of GPT-2 to generate paraphrases without any supervision from labelled data. We examine how the results compare with other supervised and unsupervised approaches and the effect of using paraphrases for data augmentation on downstream tasks such as classification. Our experiments show that paraphrases generated with our model are of good quality, are diverse and improves the downstream task performance when used for data augmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/HLLE68Y8/Hegde et Patil - 2020 - Unsupervised Paraphrase Generation using Pre-train.pdf;/home/xav/Zotero/storage/F8ZIA5TZ/2006.html}
}

@article{heImprovingMultihopKnowledge2021,
  title = {Improving {{Multi-hop Knowledge Base Question Answering}} by {{Learning Intermediate Supervision Signals}}},
  author = {He, Gaole and Lan, Yunshi and Jiang, Jing and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2021},
  month = mar,
  journal = {Proceedings of the 14th ACM International Conference on Web Search and Data Mining},
  eprint = {2101.03737},
  pages = {553--561},
  doi = {10.1145/3437963.3441753},
  urldate = {2022-05-04},
  abstract = {Multi-hop Knowledge Base Question Answering (KBQA) aims to find the answer entities that are multiple hops away in the Knowledge Base (KB) from the entities in the question. A major challenge is the lack of supervision signals at intermediate steps. Therefore, multi-hop KBQA algorithms can only receive the feedback from the final answer, which makes the learning unstable or ineffective. To address this challenge, we propose a novel teacher-student approach for the multi-hop KBQA task. In our approach, the student network aims to find the correct answer to the query, while the teacher network tries to learn intermediate supervision signals for improving the reasoning capacity of the student network. The major novelty lies in the design of the teacher network, where we utilize both forward and backward reasoning to enhance the learning of intermediate entity distributions. By considering bidirectional reasoning, the teacher network can produce more reliable intermediate supervision signals, which can alleviate the issue of spurious reasoning. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our approach on the KBQA task. The code to reproduce our analysis is available at https://github.com/RichardHGL/WSDM2021\_NSM.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/4IKX8SU8/He et al. - 2021 - Improving Multi-hop Knowledge Base Question Answer.pdf}
}

@article{helweReasoningTransformerbasedModels,
  title = {Reasoning with {{Transformer-based Models}}: {{Deep Learning}}, but {{Shallow Reasoning}}},
  author = {Helwe, Chadi and Clavel, Chloe and Suchanek, Fabian},
  journal = {Deep Learning},
  pages = {28},
  abstract = {Recent years have seen impressive performance of transformer-based models on different natural language processing tasks. However, it is not clear to what degree the transformers can reason on natural language. To shed light on this question, this survey paper discusses the performance of transformers on different reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning. We point out successes and limitations, of both empirical and theoretical nature.},
  langid = {english},
  file = {/home/xav/Zotero/storage/UNDWDJKU/Helwe et al. - Reasoning with Transformer-based Models Deep Lear.pdf}
}

@misc{hendrycksMeasuringMassiveMultitask2021,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = {2021},
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {arXiv:2009.03300},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.03300},
  urldate = {2022-09-06},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/U8RPCCZW/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf;/home/xav/Zotero/storage/YKM923ZF/2009.html}
}

@misc{heRethinkingRetrievalFaithful2022,
  title = {Rethinking with {{Retrieval}}: {{Faithful Large Language Model Inference}}},
  shorttitle = {Rethinking with {{Retrieval}}},
  author = {He, Hangfeng and Zhang, Hongming and Roth, Dan},
  year = {2022},
  month = dec,
  number = {arXiv:2301.00303},
  eprint = {arXiv:2301.00303},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.00303},
  urldate = {2023-01-08},
  abstract = {Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/8HXNZDQ7/He et al. - 2022 - Rethinking with Retrieval Faithful Large Language.pdf}
}

@misc{hermannTeachingMachinesRead2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  author = {Hermann, Karl Moritz and Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  year = {2015},
  month = nov,
  number = {arXiv:1506.03340},
  eprint = {arXiv:1506.03340},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.03340},
  urldate = {2023-02-01},
  abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/home/xav/Zotero/storage/HSCG9DWS/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf;/home/xav/Zotero/storage/S47MHH3B/1506.html}
}

@misc{heZCodePretrainedLanguage2022,
  title = {Z-{{Code}}++: {{A Pre-trained Language Model Optimized}} for {{Abstractive Summarization}}},
  shorttitle = {Z-{{Code}}++},
  author = {He, Pengcheng and Peng, Baolin and Lu, Liyang and Wang, Song and Mei, Jie and Liu, Yang and Xu, Ruochen and Awadalla, Hany Hassan and Shi, Yu and Zhu, Chenguang and Xiong, Wayne and Zeng, Michael and Gao, Jianfeng and Huang, Xuedong},
  year = {2022},
  month = aug,
  number = {arXiv:2208.09770},
  eprint = {arXiv:2208.09770},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.09770},
  urldate = {2022-08-29},
  abstract = {This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,cs.CL; cs.GL,I.2,I.7},
  file = {/home/xav/Zotero/storage/SQZQ6URM/He et al. - 2022 - Z-Code++ A Pre-trained Language Model Optimized f.pdf}
}

@misc{hillGoldilocksPrincipleReading2016,
  title = {The {{Goldilocks Principle}}: {{Reading Children}}'s {{Books}} with {{Explicit Memory Representations}}},
  shorttitle = {The {{Goldilocks Principle}}},
  author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
  year = {2016},
  month = apr,
  number = {arXiv:1511.02301},
  eprint = {arXiv:1511.02301},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.02301},
  urldate = {2023-02-01},
  abstract = {We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/QQNABXNU/Hill et al. - 2016 - The Goldilocks Principle Reading Children's Books.pdf;/home/xav/Zotero/storage/CZ5XDHAZ/1511.html}
}

@article{hoffmannTrainingComputeOptimalLarge2022,
  title = {Training {{Compute-Optimal Large Language Models}}},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.15556 [cs]},
  eprint = {2203.15556},
  primaryclass = {cs},
  urldate = {2022-04-16},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over \textbackslash nummodels language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, \textbackslash chinchilla, that uses the same compute budget as \textbackslash gopher but with 70B parameters and 4\$\textbackslash times\$ more more data. \textbackslash chinchilla uniformly and significantly outperforms \textbackslash Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that \textbackslash chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, \textbackslash chinchilla reaches a state-of-the-art average accuracy of 67.5\textbackslash\% on the MMLU benchmark, greater than a 7\textbackslash\% improvement over \textbackslash gopher.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/5AG2J8GA/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf;/home/xav/Zotero/storage/XT8JXTGA/2203.html}
}

@article{hoganKnowledgeGraphs2022,
  title = {Knowledge {{Graphs}}},
  author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and {d'Amato}, Claudia and {de Melo}, Gerard and Gutierrez, Claudio and Gayo, Jos{\'e} Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
  year = {2022},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {4},
  eprint = {2003.02320},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3447772},
  urldate = {2022-05-04},
  abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Machine Learning},
  annotation = {QID: Q113482045},
  file = {/home/xav/Zotero/storage/IGWRJMAI/Hogan et al. - 2022 - Knowledge Graphs.pdf;/home/xav/Zotero/storage/2EBCQIEN/2003.html}
}

@misc{hoLargeLanguageModels2022,
  title = {Large {{Language Models Are Reasoning Teachers}}},
  author = {Ho, Namgyu and Schmid, Laura and Yun, Se-Young},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10071},
  eprint = {arXiv:2212.10071},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10071},
  urldate = {2023-01-08},
  abstract = {Language models (LMs) have demonstrated remarkable performance on downstream tasks, using in-context exemplars or human instructions. Recent works have shown that chain-of-thought (CoT) prompting can elicit models to solve complex reasoning tasks, step-by-step. However, the efficacy of prompt-based CoT methods is restricted to very large LMs such as GPT-3 (175B), thus limiting deployability. In this paper, we revisit the fine-tuning approach to enable complex reasoning in smaller LMs, optimized to efficiently perform a specific task. We propose Fine-tune-CoT, a method that leverages the capabilities of very large LMs to generate reasoning samples and teach smaller models via fine-tuning. We evaluate our method on publicly available LMs across a wide range of complex tasks and model sizes. We find that Fine-tune-CoT enables substantial reasoning capability in small models, whereas previous prompt-based baselines exhibit near-random performance. Student models can even outperform the teacher in some tasks while reducing model size requirements by several orders of magnitude. We conduct extensive ablations and sample studies to understand the reasoning capabilities of student models. We also identify several important nuances that have been overlooked in concurrent fine-tuning works on CoT and address them in our analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/HDDJZ8NQ/Ho et al. - 2022 - Large Language Models Are Reasoning Teachers.pdf}
}

@misc{Home,
  title = {Home},
  urldate = {2023-02-15},
  howpublished = {https://www.cikm2022.org/},
  file = {/home/xav/Zotero/storage/TDKKIB54/www.cikm2022.org.html}
}

@inproceedings{hongDFXLowlatencyMultiFPGA2022a,
  title = {{{DFX}}: {{A Low-latency Multi-FPGA Appliance}} for {{Accelerating Transformer-based Text Generation}}},
  shorttitle = {{{DFX}}},
  booktitle = {2022 {{IEEE Hot Chips}} 34 {{Symposium}} ({{HCS}})},
  author = {Hong, Seongmin and Moon, Seungjae and Kim, Junsoo and Lee, Sungjae and Kim, Minsub and Lee, Dongsoo and Kim, Joo-Young},
  year = {2022},
  month = aug,
  pages = {1--17},
  issn = {2573-2048},
  doi = {10.1109/HCS55958.2022.9895626},
  abstract = {\textbullet DFX: a low-latency multi-FPGA appliance for accelerating transformer-based text generation\textendash DFX is a multi-FPGA appliance that accelerates transformer-based text generation\textendash DFX adopts model parallelism to efficiently process the large-scale language model\textendash Xilinx Alveo U280 data center accelerator card provides high performance with low-cost\textendash FPGA-to-FPGA communication is enabled by QSFP cable at 100 Gb/s},
  keywords = {Communication cables,Data centers,Data models,Low latency communication,Parallel processing,Transformers},
  file = {/home/xav/Zotero/storage/RNMBP8P5/Hong et al. - 2022 - DFX A Low-latency Multi-FPGA Appliance for Accele.pdf;/home/xav/Zotero/storage/SM6JFJ4W/9895626.html}
}

@misc{honovichInstructionInductionFew2022,
  title = {Instruction {{Induction}}: {{From Few Examples}} to {{Natural Language Task Descriptions}}},
  shorttitle = {Instruction {{Induction}}},
  author = {Honovich, Or and Shaham, Uri and Bowman, Samuel R. and Levy, Omer},
  year = {2022},
  month = may,
  number = {arXiv:2205.10782},
  eprint = {2205.10782},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-30},
  abstract = {Large language models are able to perform a task by conditioning on a few input-output demonstrations - a paradigm known as in-context learning. We show that language models can explicitly infer an underlying task from a few demonstrations by prompting them to generate a natural language instruction that fits the examples. To explore this ability, we introduce the instruction induction challenge, compile a dataset consisting of 24 tasks, and define a novel evaluation metric based on executing the generated instruction. We discover that, to a large extent, the ability to generate instructions does indeed emerge when using a model that is both large enough and aligned to follow instructions; InstructGPT achieves 65.7\% of human performance in our execution-based metric, while the original GPT-3 model reaches only 9.8\% of human performance. This surprising result suggests that instruction induction might be a viable learning paradigm in and of itself, where instead of fitting a set of latent continuous parameters to the data, one searches for the best description in the natural language hypothesis space.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/QYV633C9/Honovich et al. - 2022 - Instruction Induction From Few Examples to Natura.pdf}
}

@misc{honovichUnnaturalInstructionsTuning2022,
  title = {Unnatural {{Instructions}}: {{Tuning Language Models}} with ({{Almost}}) {{No Human Labor}}},
  shorttitle = {Unnatural {{Instructions}}},
  author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09689},
  eprint = {arXiv:2212.09689},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.09689},
  urldate = {2023-01-02},
  abstract = {Instruction tuning enables pretrained language models to perform new tasks from inference-time natural language descriptions. These approaches rely on vast amounts of human supervision in the form of crowdsourced datasets or user interactions. In this work, we introduce Unnatural Instructions: a large dataset of creative and diverse instructions, collected with virtually no human labor. We collect 64,000 examples by prompting a language model with three seed examples of instructions and eliciting a fourth. This set is then expanded by prompting the model to rephrase each instruction, creating a total of approximately 240,000 examples of instructions, inputs, and outputs. Experiments show that despite containing a fair amount of noise, training on Unnatural Instructions rivals the effectiveness of training on open-source manually-curated datasets, surpassing the performance of models such as T0++ and Tk-Instruct across various benchmarks. These results demonstrate the potential of model-generated data as a cost-effective alternative to crowdsourcing for dataset expansion and diversification.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/922VNREA/Honovich et al. - 2022 - Unnatural Instructions Tuning Language Models wit.pdf;/home/xav/Zotero/storage/KXGSET2X/2212.html}
}

@misc{HowUseGPT3,
  title = {How to Use {{GPT-3}}, {{GPT-J}} and {{GPT-Neo}}, with Few-Shot Learning},
  urldate = {2022-06-11},
  abstract = {GPT-3, GPT-J and GPT-Neo are very powerful AI models. We're showing you here how to effectively use these models thanks to few-shot learning.},
  howpublished = {https://nlpcloud.io/effectively-using-gpt-j-gpt-neo-gpt-3-alternatives-few-shot-learning.html},
  langid = {english},
  file = {/home/xav/Zotero/storage/9UVJ5BCA/effectively-using-gpt-j-gpt-neo-gpt-3-alternatives-few-shot-learning.html}
}

@inproceedings{huangBreadthFirstReasoning2021,
  title = {Breadth {{First Reasoning Graph}} for {{Multi-hop Question Answering}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Huang, Yongjie and Yang, Meng},
  year = {2021},
  month = jun,
  pages = {5810--5821},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.464},
  urldate = {2022-05-04},
  abstract = {Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the question node and pass to the next sentences node hop by hop until all the edges have been passed, which can effectively prevent each node from over-smoothing or being updated multiple times unnecessarily. To introduce more semantics, we also define the reasoning graph as a weighted graph with considering the number of co-occurrence entities and the distance between sentences. Then we present a more direct and interpretable way to aggregate scores from different levels of granularity based on the GNN. On HotpotQA leaderboard, the proposed BFR-Graph achieves state-of-the-art on answer span prediction.},
  file = {/home/xav/Zotero/storage/68DIE7SR/Huang et Yang - 2021 - Breadth First Reasoning Graph for Multi-hop Questi.pdf}
}

@article{huangLanguageModelsZeroShot2022,
  title = {Language {{Models}} as {{Zero-Shot Planners}}: {{Extracting Actionable Knowledge}} for {{Embodied Agents}}},
  shorttitle = {Language {{Models}} as {{Zero-Shot Planners}}},
  author = {Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  year = {2022},
  month = mar,
  journal = {arXiv:2201.07207 [cs]},
  eprint = {2201.07207},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. "make breakfast"), to a chosen set of actionable steps (e.g. "open fridge"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/xav/Zotero/storage/WVWM7GGZ/Huang et al. - 2022 - Language Models as Zero-Shot Planners Extracting .pdf;/home/xav/Zotero/storage/7Z3MUZ98/2201.html}
}

@misc{huangLanguageNotAll2023,
  title = {Language {{Is Not All You Need}}: {{Aligning Perception}} with {{Language Models}}},
  shorttitle = {Language {{Is Not All You Need}}},
  author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Patra, Barun and Liu, Qiang and Aggarwal, Kriti and Chi, Zewen and Bjorck, Johan and Chaudhary, Vishrav and Som, Subhojit and Song, Xia and Wei, Furu},
  year = {2023},
  month = mar,
  number = {arXiv:2302.14045},
  eprint = {arXiv:2302.14045},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.14045},
  urldate = {2023-03-03},
  abstract = {A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/96LG7XZT/Huang et al. - 2023 - Language Is Not All You Need Aligning Perception .pdf}
}

@misc{huangLargeLanguageModels2022,
  title = {Large {{Language Models Can Self-Improve}}},
  author = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11610},
  eprint = {arXiv:2210.11610},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11610},
  urldate = {2023-02-07},
  abstract = {Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate "high-confidence" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%-{$>$}82.1\% on GSM8K, 78.2\%-{$>$}83.0\% on DROP, 90.0\%-{$>$}94.4\% on OpenBookQA, and 63.4\%-{$>$}67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/R22MIMWB/Huang et al. - 2022 - Large Language Models Can Self-Improve.pdf;/home/xav/Zotero/storage/ZMVNGHCR/2210.html}
}

@article{huangMixedmodalityRepresentationLearning2022,
  title = {Mixed-Modality {{Representation Learning}} and {{Pre-training}} for {{Joint Table-and-Text Retrieval}} in {{OpenQA}}},
  author = {Huang, Junjie and Zhong, Wanjun and Liu, Qianchu and Gong, Ming and Jiang, Daxin and Duan, Nan},
  year = {2022},
  journal = {undefined},
  urldate = {2022-10-22},
  abstract = {This work introduces an optimized OpenQA Table-TExt Retriever (OTTER) to jointly retrieve tabular and textual evidences and proposes to enhance mixed-modality representation learning via two mechanisms: modality-enhanced representation and mixed- modality negative sampling strategy. Retrieving evidences from tabular and textual resources is essential for open-domain question answering (OpenQA), which provides more comprehensive information. However, training an effective dense table-text retriever is difficult due to the challenges of table-text discrepancy and data sparsity problem. To address the above challenges, we introduce an optimized OpenQA Table-TExt Retriever (OTTER) to jointly retrieve tabular and textual evidences. Firstly, we propose to enhance mixed-modality representation learning via two mechanisms: modality-enhanced representation and mixed-modality negative sampling strategy. Secondly, to alleviate data sparsity problem and enhance the general retrieval ability, we conduct retrieval-centric mixedmodality synthetic pre-training. Experimental results demonstrate that OTTER substantially improves the performance of table-and-text retrieval on the OTT-QA dataset. Comprehensive analyses examine the effectiveness of all the proposed mechanisms. Besides, equipped with OTTER, our OpenQA system achieves the state-of-the-art result on the downstream QA task, with 10.1\% absolute performance gain in terms of the exact match over the previous best system. 1},
  langid = {english},
  file = {/home/xav/Zotero/storage/XHY62HQK/Huang et al. - 2022 - Mixed-modality Representation Learning and Pre-tra.pdf}
}

@misc{huangReasoningLargeLanguage2022,
  title = {Towards {{Reasoning}} in {{Large Language Models}}: {{A Survey}}},
  shorttitle = {Towards {{Reasoning}} in {{Large Language Models}}},
  author = {Huang, Jie and Chang, Kevin Chen-Chuan},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10403},
  eprint = {arXiv:2212.10403},
  publisher = {{arXiv}},
  urldate = {2023-02-06},
  abstract = {Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/6GH2U9BU/Huang et Chang - 2022 - Towards Reasoning in Large Language Models A Surv.pdf}
}

@misc{huangTextHideTacklingData2020,
  title = {{{TextHide}}: {{Tackling Data Privacy}} in {{Language Understanding Tasks}}},
  shorttitle = {{{TextHide}}},
  author = {Huang, Yangsibo and Song, Zhao and Chen, Danqi and Li, Kai and Arora, Sanjeev},
  year = {2020},
  month = oct,
  number = {arXiv:2010.06053},
  eprint = {arXiv:2010.06053},
  publisher = {{arXiv}},
  urldate = {2023-02-08},
  abstract = {An unsolved challenge in distributed or federated learning is to effectively mitigate privacy risks without slowing down training or reducing accuracy. In this paper, we propose TextHide aiming at addressing this challenge for natural language understanding tasks. It requires all participants to add a simple encryption step to prevent an eavesdropping attacker from recovering private text data. Such an encryption step is efficient and only affects the task performance slightly. In addition, TextHide fits well with the popular framework of fine-tuning pre-trained language models (e.g., BERT) for any sentence or sentence-pair task. We evaluate TextHide on the GLUE benchmark, and our experiments show that TextHide can effectively defend attacks on shared gradients or representations and the averaged accuracy reduction is only \$1.9\textbackslash\%\$. We also present an analysis of the security of TextHide using a conjecture about the computational intractability of a mathematical problem. Our code is available at https://github.com/Hazelsuko07/TextHide},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/9HNJSADV/Huang et al. - 2020 - TextHide Tackling Data Privacy in Language Unders.pdf}
}

@article{huberCCQANewWebScale2022,
  title = {{{CCQA}}: {{A New Web-Scale Question Answering Dataset}} for {{Model Pre-Training}}},
  shorttitle = {{{CCQA}}},
  author = {Huber, Patrick and Aghajanyan, Armen and O{\u g}uz, Barlas and Okhonko, Dmytro and Yih, Wen-tau and Gupta, Sonal and Chen, Xilun},
  year = {2022},
  month = may,
  journal = {arXiv:2110.07731 [cs]},
  eprint = {2110.07731},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {With the rise of large-scale pre-trained language models, open-domain question-answering (ODQA) has become an important research topic in NLP. Based on the popular pre-training fine-tuning approach, we posit that an additional in-domain pre-training stage using a large-scale, natural, and diverse question-answering (QA) dataset can be beneficial for ODQA. Consequently, we propose a novel QA dataset based on the Common Crawl project in this paper. Using the readily available schema.org annotation, we extract around 130 million multilingual question-answer pairs, including about 60 million English data-points. With this previously unseen number of natural QA pairs, we pre-train popular language models to show the potential of large-scale in-domain pre-training for the task of question-answering. In our experiments, we find that pre-training question-answering models on our Common Crawl Question Answering dataset (CCQA) achieves promising results in zero-shot, low resource and fine-tuned settings across multiple tasks, models and benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/4MKH5QHS/Huber et al. - 2022 - CCQA A New Web-Scale Question Answering Dataset f.pdf;/home/xav/Zotero/storage/ADF57UTA/2110.html}
}

@misc{huInteractiveQuestionClarification2020,
  title = {Interactive {{Question Clarification}} in {{Dialogue}} via {{Reinforcement Learning}}},
  author = {Hu, Xiang and Wen, Zujie and Wang, Yafang and Li, Xiaolong and {de Melo}, Gerard},
  year = {2020},
  month = dec,
  number = {arXiv:2012.09411},
  eprint = {arXiv:2012.09411},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.09411},
  urldate = {2023-02-07},
  abstract = {Coping with ambiguous questions has been a perennial problem in real-world dialogue systems. Although clarification by asking questions is a common form of human interaction, it is hard to define appropriate questions to elicit more specific intents from a user. In this work, we propose a reinforcement model to clarify ambiguous questions by suggesting refinements of the original query. We first formulate a collection partitioning problem to select a set of labels enabling us to distinguish potential unambiguous intents. We list the chosen labels as intent phrases to the user for further confirmation. The selected label along with the original user query then serves as a refined query, for which a suitable response can more easily be identified. The model is trained using reinforcement learning with a deep policy network. We evaluate our model based on real-world user clicks and demonstrate significant improvements across several different experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/94LCQRE7/Hu et al. - 2020 - Interactive Question Clarification in Dialogue via.pdf;/home/xav/Zotero/storage/DCZJJ9AG/2012.html}
}

@misc{huLoRALowRankAdaptation2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {arXiv:2106.09685},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2023-02-02},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/79W3ECDN/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf;/home/xav/Zotero/storage/4P8A5DZL/2106.html}
}

@misc{HumanintheLoopDataAnalysis,
  title = {Human-in-the-{{Loop Data Analysis}}: {{A Personal Perspective}}},
  urldate = {2022-11-20},
  howpublished = {https://www.researchgate.net/publication/325566536\_Human-in-the-Loop\_Data\_Analysis\_A\_Personal\_Perspective},
  file = {/home/xav/Zotero/storage/FBY5VVD4/Human-in-the-Loop Data Analysis A Personal Perspe.pdf;/home/xav/Zotero/storage/WT2XTQNT/325566536_Human-in-the-Loop_Data_Analysis_A_Personal_Perspective.html}
}

@misc{hupkesStateoftheartGeneralisationResearch2023,
  title = {State-of-the-Art Generalisation Research in {{NLP}}: {{A}} Taxonomy and Review},
  shorttitle = {State-of-the-Art Generalisation Research in {{NLP}}},
  author = {Hupkes, Dieuwke and Giulianelli, Mario and Dankers, Verna and Artetxe, Mikel and Elazar, Yanai and Pimentel, Tiago and Christodoulopoulos, Christos and Lasri, Karim and Saphra, Naomi and Sinclair, Arabella and Ulmer, Dennis and Schottmann, Florian and Batsuren, Khuyagbaatar and Sun, Kaiser and Sinha, Koustuv and Khalatbari, Leila and Ryskina, Maria and Frieske, Rita and Cotterell, Ryan and Jin, Zhijing},
  year = {2023},
  month = jan,
  number = {arXiv:2210.03050},
  eprint = {arXiv:2210.03050},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what `good generalisation' entails and how it should be evaluated is not well understood, nor are there any evaluation standards for generalisation. In this paper, we lay the groundwork to address both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they investigate, the type of data shift they consider, the source of this data shift, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis that maps out the current state of generalisation research in NLP, and we make recommendations for which areas might deserve attention in the future. Along with this paper, we release a webpage where the results of our review can be dynamically explored, and which we intend to update as new NLP generalisation studies are published. With this work, we aim to take steps towards making state-of-the-art generalisation testing the new status quo in NLP.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/B5TTRIKC/Hupkes et al. - 2023 - State-of-the-art generalisation research in NLP A.pdf}
}

@misc{huSurveyKnowledgeEnhancedPretrained2022,
  title = {A {{Survey}} of {{Knowledge-Enhanced Pre-trained Language Models}}},
  author = {Hu, Linmei and Liu, Zeyi and Zhao, Ziwang and Hou, Lei and Nie, Liqiang and Li, Juanzi},
  year = {2022},
  month = nov,
  number = {arXiv:2211.05994},
  eprint = {arXiv:2211.05994},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.05994},
  urldate = {2022-12-12},
  abstract = {Pre-trained Language Models (PLMs) which are trained on large text corpus via self-supervised learning method, have yielded promising performance on various tasks in Natural Language Processing (NLP). However, though PLMs with huge parameters can effectively possess rich knowledge learned from massive training text and benefit downstream tasks at the fine-tuning stage, they still have some limitations such as poor reasoning ability due to the lack of external knowledge. Research has been dedicated to incorporating knowledge into PLMs to tackle these issues. In this paper, we present a comprehensive review of Knowledge-Enhanced Pre-trained Language Models (KE-PLMs) to provide a clear insight into this thriving field. We introduce appropriate taxonomies respectively for Natural Language Understanding (NLU) and Natural Language Generation (NLG) to highlight these two main tasks of NLP. For NLU, we divide the types of knowledge into four categories: linguistic knowledge, text knowledge, knowledge graph (KG), and rule knowledge. The KE-PLMs for NLG are categorized into KG-based and retrieval-based methods. Finally, we point out some promising future directions of KE-PLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/EKQ39KIT/Hu et al. - 2022 - A Survey of Knowledge-Enhanced Pre-trained Languag.pdf}
}

@misc{huTeacherStudentArchitectureKnowledge2022,
  title = {Teacher-{{Student Architecture}} for {{Knowledge Learning}}: {{A Survey}}},
  shorttitle = {Teacher-{{Student Architecture}} for {{Knowledge Learning}}},
  author = {Hu, Chengming and Li, Xuan and Liu, Dan and Chen, Xi and Wang, Ju and Liu, Xue},
  year = {2022},
  month = oct,
  number = {arXiv:2210.17332},
  eprint = {arXiv:2210.17332},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.17332},
  urldate = {2023-01-23},
  abstract = {Although Deep Neural Networks (DNNs) have shown a strong capacity to solve large-scale problems in many areas, such DNNs with voluminous parameters are hard to be deployed in a real-time system. To tackle this issue, Teacher-Student architectures were first utilized in knowledge distillation, where simple student networks can achieve comparable performance to deep teacher networks. Recently, Teacher-Student architectures have been effectively and widely embraced on various knowledge learning objectives, including knowledge distillation, knowledge expansion, knowledge adaption, and multi-task learning. With the help of Teacher-Student architectures, current studies are able to achieve multiple knowledge-learning objectives through lightweight and effective student networks. Different from the existing knowledge distillation surveys, this survey detailedly discusses Teacher-Student architectures with multiple knowledge learning objectives. In addition, we systematically introduce the knowledge construction and optimization process during the knowledge learning and then analyze various Teacher-Student architectures and effective learning schemes that have been leveraged to learn representative and robust knowledge. This paper also summarizes the latest applications of Teacher-Student architectures based on different purposes (i.e., classification, recognition, and generation). Finally, the potential research directions of knowledge learning are investigated on the Teacher-Student architecture design, the quality of knowledge, and the theoretical studies of regression-based learning, respectively. With this comprehensive survey, both industry practitioners and the academic community can learn insightful guidelines about Teacher-Student architectures on multiple knowledge learning objectives.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/QZI9QUHV/Hu et al. - 2022 - Teacher-Student Architecture for Knowledge Learnin.pdf;/home/xav/Zotero/storage/3EL8Q9NX/2210.html}
}

@misc{huyAutoencodingLanguageModel2022,
  title = {Autoencoding {{Language Model Based Ensemble Learning}} for {{Commonsense Validation}} and {{Explanation}}},
  author = {Huy, Ngo Quang and Phuong, Tu Minh and Bach, Ngo Xuan},
  year = {2022},
  month = apr,
  number = {arXiv:2204.03324},
  eprint = {arXiv:2204.03324},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.03324},
  urldate = {2023-01-05},
  abstract = {An ultimate goal of artificial intelligence is to build computer systems that can understand human languages. Understanding commonsense knowledge about the world expressed in text is one of the foundational and challenging problems to create such intelligent systems. As a step towards this goal, we present in this paper ALMEn, an Autoencoding Language Model based Ensemble learning method for commonsense validation and explanation. By ensembling several advanced pre-trained language models including RoBERTa, DeBERTa, and ELECTRA with Siamese neural networks, our method can distinguish natural language statements that are against commonsense (validation subtask) and correctly identify the reason for making against commonsense (explanation selection subtask). Experimental results on the benchmark dataset of SemEval-2020 Task 4 show that our method outperforms state-of-the-art models, reaching 97.9\% and 95.4\% accuracies on the validation and explanation selection subtasks, respectively.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/CFPJ4EAY/Huy et al. - 2022 - Autoencoding Language Model Based Ensemble Learnin.pdf;/home/xav/Zotero/storage/ZMH9YX7Z/2204.html}
}

@article{huynhSurveyNLPRelatedCrowdsourcing2021,
  title = {A {{Survey}} of {{NLP-Related Crowdsourcing HITs}}: What Works and What Does Not},
  shorttitle = {A {{Survey}} of {{NLP-Related Crowdsourcing HITs}}},
  author = {Huynh, Jessica and Bigham, Jeffrey and Eskenazi, Maxine},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.05241 [cs]},
  eprint = {2111.05241},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Crowdsourcing requesters on Amazon Mechanical Turk (AMT) have raised questions about the reliability of the workers. The AMT workforce is very diverse and it is not possible to make blanket assumptions about them as a group. Some requesters now reject work en mass when they do not get the results they expect. This has the effect of giving each worker (good or bad) a lower Human Intelligence Task (HIT) approval score, which is unfair to the good workers. It also has the effect of giving the requester a bad reputation on the workers' forums. Some of the issues causing the mass rejections stem from the requesters not taking the time to create a well-formed task with complete instructions and/or not paying a fair wage. To explore this assumption, this paper describes a study that looks at the crowdsourcing HITs on AMT that were available over a given span of time and records information about those HITs. This study also records information from a crowdsourcing forum on the worker perspective on both those HITs and on their corresponding requesters. Results reveal issues in worker payment and presentation issues such as missing instructions or HITs that are not doable.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/PF86C5FD/Huynh et al. - 2021 - A Survey of NLP-Related Crowdsourcing HITs what w.pdf}
}

@misc{huynhUnderstandingEffectivenessVery2023,
  title = {Understanding the {{Effectiveness}} of {{Very Large Language Models}} on {{Dialog Evaluation}}},
  author = {Huynh, Jessica and Jiao, Cathy and Gupta, Prakhar and Mehri, Shikib and Bajaj, Payal and Chaudhary, Vishrav and Eskenazi, Maxine},
  year = {2023},
  month = jan,
  number = {arXiv:2301.12004},
  eprint = {arXiv:2301.12004},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.12004},
  urldate = {2023-02-01},
  abstract = {Language models have steadily increased in size over the past few years. They achieve a high level of performance on various natural language processing (NLP) tasks such as question answering and summarization. Large language models (LLMs) have been used for generation and can now output human-like text. Due to this, there are other downstream tasks in the realm of dialog that can now harness the LLMs' language understanding capabilities. Dialog evaluation is one task that this paper will explore. It concentrates on prompting with LLMs: BLOOM, OPT, GPT-3, Flan-T5, InstructDial and TNLGv2. The paper shows that the choice of datasets used for training a model contributes to how well it performs on a task as well as on how the prompt should be structured. Specifically, the more diverse and relevant the group of datasets that a model is trained on, the better dialog evaluation performs. This paper also investigates how the number of examples in the prompt and the type of example selection used affect the model's performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/F7ZEGLJH/Huynh et al. - 2023 - Understanding the Effectiveness of Very Large Lang.pdf}
}

@misc{ICLRInternationalConference,
  title = {{{ICLR}} | {{International Conference}} on {{Learning Representations}}},
  urldate = {2023-02-15},
  howpublished = {https://iclr.cc/},
  file = {/home/xav/Zotero/storage/5R4YGKSR/iclr.cc.html}
}

@inproceedings{ignatiusConceptualReferenceModel2021,
  title = {A {{Conceptual Reference Model}} for {{Human}} as a {{Service Provider}} in {{Cyber Physical Systems}}},
  booktitle = {2021 {{International Symposium}} on {{Software Engineering}} for {{Adaptive}} and {{Self-Managing Systems}} ({{SEAMS}})},
  author = {Ignatius, Hargyo Tri Nugroho and Bahsoon, Rami},
  year = {2021},
  month = may,
  eprint = {2103.12563},
  primaryclass = {cs},
  pages = {1--10},
  doi = {10.1109/SEAMS51251.2021.00012},
  urldate = {2022-11-17},
  abstract = {In Cyber Physical Systems humans are often kept in the loop as operators and/or service users. Yet in many cases, humans and machines collaborate and provide services to each other. Research on service models and service composition for CPS exist; however, humans as service providers have not been adequately considered as part of the CPS service composition model. We provide a classification of human-as-a-service in CPS, and we propose a Service Oriented Architecture (SOA) ontology model for the CPS environment as part of the Everything-as-a-Service paradigm. The model considers human characteristics and their dynamics, as a service provider or collaborator with the machine. As the ontology model is an enabler for engineering a self-adaptive CPS with human-machine collaboration as service providers, we describe how a commonly used self-adaptive reference model can be refined to benefit from the vision. We evaluate the ontological contribution against criteria that relates to accuracy, completeness, adaptability, clarity, and consistency. We demonstrate the feasibility of our conceptual reference model using a use case from the medical domain and we show how human-machine service provision is possible.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Software Engineering},
  file = {/home/xav/Zotero/storage/HDSEEHB7/Ignatius et Bahsoon - 2021 - A Conceptual Reference Model for Human as a Servic.pdf}
}

@misc{IJCAIInternationalJoint,
  title = {{{IJCAI}} | {{International Joint Conferences}} on {{Artificial Intelligence Organization}}},
  urldate = {2023-02-15},
  howpublished = {https://www.ijcai.org/},
  file = {/home/xav/Zotero/storage/YGSV37U3/www.ijcai.org.html}
}

@misc{imaniMathPrompterMathematicalReasoning2023,
  title = {{{MathPrompter}}: {{Mathematical Reasoning}} Using {{Large Language Models}}},
  shorttitle = {{{MathPrompter}}},
  author = {Imani, Shima and Du, Liang and Shrivastava, Harsh},
  year = {2023},
  month = mar,
  number = {arXiv:2303.05398},
  eprint = {arXiv:2303.05398},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.05398},
  urldate = {2023-03-16},
  abstract = {Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose `MathPrompter', a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple Algebraic expressions or Python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the MultiArith dataset (\$78.7\textbackslash\%\textbackslash rightarrow92.5\textbackslash\%\$) evaluated using 175B parameter GPT-based LLM.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{IRISGithub2022,
  title = {{{IRIS Github}}},
  year = {2022},
  month = apr,
  urldate = {2022-05-14},
  abstract = {The Internet Reasoning Service},
  howpublished = {Knowledge Media Institute, The Open University}
}

@misc{islamDiscreteFactorialRepresentations2022,
  title = {Discrete {{Factorial Representations}} as an {{Abstraction}} for {{Goal Conditioned Reinforcement Learning}}},
  author = {Islam, Riashat and Zang, Hongyu and Goyal, Anirudh and Lamb, Alex and Kawaguchi, Kenji and Li, Xin and Laroche, Romain and Bengio, Yoshua and Combes, Remi Tachet Des},
  year = {2022},
  month = oct,
  number = {arXiv:2211.00247},
  eprint = {arXiv:2211.00247},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.00247},
  urldate = {2023-02-06},
  abstract = {Goal-conditioned reinforcement learning (RL) is a promising direction for training agents that are capable of solving multiple tasks and reach a diverse set of objectives. How to \textbackslash textit\{specify\} and \textbackslash textit\{ground\} these goals in such a way that we can both reliably reach goals during training as well as generalize to new goals during evaluation remains an open area of research. Defining goals in the space of noisy and high-dimensional sensory inputs poses a challenge for training goal-conditioned agents, or even for generalization to novel goals. We propose to address this by learning factorial representations of goals and processing the resulting representation via a discretization bottleneck, for coarser goal specification, through an approach we call DGRL. We show that applying a discretizing bottleneck can improve performance in goal-conditioned RL setups, by experimentally evaluating this method on tasks ranging from maze environments to complex robotic navigation and manipulation. Additionally, we prove a theorem lower-bounding the expected return on out-of-distribution goals, while still allowing for specifying goals with expressive combinatorial structure.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/NKCED54Y/Islam et al. - 2022 - Discrete Factorial Representations as an Abstracti.pdf;/home/xav/Zotero/storage/DS97BTIJ/2211.html}
}

@article{issaKnowledgeGraphCompleteness2021,
  title = {Knowledge {{Graph Completeness}}: {{A Systematic Literature Review}}},
  shorttitle = {Knowledge {{Graph Completeness}}},
  author = {Issa, Subhi and Adekunle, Onaopepo and Hamdi, Fay{\c c}al and Cherfi, Samira Si-Said and Dumontier, Michel and Zaveri, Amrapali},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {31322--31339},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3056622},
  abstract = {The quality of a Knowledge Graph (also known as Linked Data) is an important aspect to indicate its fitness for use in an application. Several quality dimensions are identified, such as accuracy, completeness, timeliness, provenance, and accessibility, which are used to assess the quality. While many prior studies offer a landscape view of data quality dimensions, here we focus on presenting a systematic literature review for assessing the completeness of Knowledge Graph. We gather existing approaches from the literature and analyze them qualitatively and quantitatively. In particular, we unify and formalize commonly used terminologies across 56 articles related to the completeness dimension of data quality and provide a comprehensive list of methodologies and metrics used to evaluate the different types of completeness. We identify seven types of completeness, including three types that were not previously identified in previous surveys. We also analyze nine different tools capable of assessing Knowledge Graph completeness. The aim of this Systematic Literature Review is to provide researchers and data curators a comprehensive and deeper understanding of existing works on completeness and its properties, thereby encouraging further experimentation and development of new approaches focused on completeness as a data quality dimension of Knowledge Graph.},
  keywords = {Assessment,Bibliographies,completeness,Data integrity,data quality,KG,knowledge graph,linked data,Linked data,LOD,Measurement,metrics,Search problems,survey,systematic literature review,Systematics,Tools},
  annotation = {QID: Q110802090},
  file = {/home/xav/Zotero/storage/TK5PFQ3X/Issa et al. - 2021 - Knowledge Graph Completeness A Systematic Literat.pdf;/home/xav/Zotero/storage/FSHM44ZG/9344615.html}
}

@misc{IssueTreesDefinitive,
  title = {Issue {{Trees}}: {{The Definitive Guide}} [+{{In-depth Examples}}]},
  shorttitle = {Issue {{Trees}}},
  journal = {Crafting Cases},
  urldate = {2022-05-30},
  abstract = {In this COMPREHENSIVE guide, you'll learn 3 powerful techniques to build Issue Trees for case interviews, see TONS of realistic examples and much more.},
  howpublished = {https://www.craftingcases.com/issue-tree-guide/},
  langid = {american},
  file = {/home/xav/Zotero/storage/B7D5IQF6/issue-tree-guide.html}
}

@misc{iyerOPTIMLScalingLanguage2022,
  title = {{{OPT-IML}}: {{Scaling Language Model Instruction Meta Learning}} through the {{Lens}} of {{Generalization}}},
  shorttitle = {{{OPT-IML}}},
  author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
  year = {2022},
  month = dec,
  number = {arXiv:2212.12017},
  eprint = {arXiv:2212.12017},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.12017},
  urldate = {2022-12-31},
  abstract = {Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/9KEIDSQG/Iyer et al. - 2022 - OPT-IML Scaling Language Model Instruction Meta L.pdf}
}

@misc{iyerOPTIMLScalingLanguage2023,
  title = {{{OPT-IML}}: {{Scaling Language Model Instruction Meta Learning}} through the {{Lens}} of {{Generalization}}},
  shorttitle = {{{OPT-IML}}},
  author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and Li, Xian and O'Horo, Brian and Pereyra, Gabriel and Wang, Jeff and Dewan, Christopher and Celikyilmaz, Asli and Zettlemoyer, Luke and Stoyanov, Ves},
  year = {2023},
  month = jan,
  number = {arXiv:2212.12017},
  eprint = {arXiv:2212.12017},
  publisher = {{arXiv}},
  urldate = {2023-02-01},
  abstract = {Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction MetaLearning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instructiontuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats \textendash{} PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/C5NZUZT8/Iyer et al. - 2023 - OPT-IML Scaling Language Model Instruction Meta L.pdf}
}

@misc{izacardAtlasFewshotLearning2022,
  title = {Atlas: {{Few-shot Learning}} with {{Retrieval Augmented Language Models}}},
  shorttitle = {Atlas},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and {Dwivedi-Yu}, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  year = {2022},
  month = nov,
  number = {arXiv:2208.03299},
  eprint = {arXiv:2208.03299},
  publisher = {{arXiv}},
  urldate = {2023-02-10},
  abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/KDDF8WG5/Izacard et al. - 2022 - Atlas Few-shot Learning with Retrieval Augmented .pdf}
}

@misc{izacardFewshotLearningRetrieval2022,
  title = {Few-Shot {{Learning}} with {{Retrieval Augmented Language Models}}},
  author = {Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and {Dwivedi-Yu}, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03299},
  eprint = {arXiv:2208.03299},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.03299},
  urldate = {2022-08-16},
  abstract = {Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42\% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3\% despite having 50x fewer parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/6QA7XMRB/Izacard et al. - 2022 - Few-shot Learning with Retrieval Augmented Languag.pdf}
}

@article{jacksonNaturalLanguageSimulations2022,
  title = {From {{Natural Language}} to {{Simulations}}: {{Applying GPT-3 Codex}} to {{Automate Simulation Modeling}} of {{Logistics Systems}}},
  shorttitle = {From {{Natural Language}} to {{Simulations}}},
  author = {Jackson, Ilya and Saenz, Maria Jesus},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.12107 [cs]},
  eprint = {2202.12107},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Our work is the first attempt to apply Natural Language Processing to automate the development of simulation models of logistics systems. We demonstrated that the framework built on top of the fine-tuned Transdormer-based language model could produce functionally valid simulations of queuing and inventory control systems given the verbal description. The proposed framework has the potential to remove the tedium of programming and allow experts to focus on the high-level consideration of the problem and holistic thinking.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/VQQAX6DX/Jackson et Saenz - 2022 - From Natural Language to Simulations Applying GPT.pdf;/home/xav/Zotero/storage/QQZ89TLE/2202.html}
}

@misc{jangTGIFQASpatioTemporalReasoning2017,
  title = {{{TGIF-QA}}: {{Toward Spatio-Temporal Reasoning}} in {{Visual Question Answering}}},
  shorttitle = {{{TGIF-QA}}},
  author = {Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  year = {2017},
  month = dec,
  number = {arXiv:1704.04497},
  eprint = {arXiv:1704.04497},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.04497},
  urldate = {2023-02-01},
  abstract = {Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention, and show its effectiveness over conventional VQA techniques through empirical evaluations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/P4JD38NX/Jang et al. - 2017 - TGIF-QA Toward Spatio-Temporal Reasoning in Visua.pdf;/home/xav/Zotero/storage/LH38HFD5/1704.html}
}

@article{jeonLastQueryTransformer2021,
  title = {Last {{Query Transformer RNN}} for Knowledge Tracing},
  author = {Jeon, SeungKee},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.05038 [cs]},
  eprint = {2102.05038},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {This paper presents an efficient model to predict a student's answer correctness given his past learning activities. Basically, I use both transformer encoder and RNN to deal with time series input. The novel point of the model is that it only uses the last input as query in transformer encoder, instead of all sequence, which makes QK matrix multiplication in transformer Encoder to have O(L) time complexity, instead of O(L\^2). It allows the model to input longer sequence. Using this model I achieved the 1st place in the 'Riiid! Answer Correctness Prediction' competition hosted on kaggle.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/M6AC54XY/Jeon - 2021 - Last Query Transformer RNN for knowledge tracing.pdf;/home/xav/Zotero/storage/ZFAQ4GAI/2102.html}
}

@inproceedings{jerniteDataGovernanceAge2022,
  title = {Data {{Governance}} in the {{Age}} of {{Large-Scale Data-Driven Language Technology}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Jernite, Yacine and Nguyen, Huu and Biderman, Stella and Rogers, Anna and Masoud, Maraim and Danchev, Valentin and Tan, Samson and Luccioni, Alexandra Sasha and Subramani, Nishant and Johnson, Isaac and Dupont, Gerard and Dodge, Jesse and Lo, Kyle and Talat, Zeerak and Radev, Dragomir and Gokaslan, Aaron and Nikpoor, Somaieh and Henderson, Peter and Bommasani, Rishi and Mitchell, Margaret},
  year = {2022},
  month = jun,
  pages = {2206--2222},
  publisher = {{ACM}},
  address = {{Seoul Republic of Korea}},
  doi = {10.1145/3531146.3534637},
  urldate = {2023-02-08},
  isbn = {978-1-4503-9352-2},
  langid = {english},
  file = {/home/xav/Zotero/storage/RKP4JB4V/Jernite et al. - 2022 - Data Governance in the Age of Large-Scale Data-Dri.pdf}
}

@misc{jeronymoInParsv2LargeLanguage2023,
  title = {{{InPars-v2}}: {{Large Language Models}} as {{Efficient Dataset Generators}} for {{Information Retrieval}}},
  shorttitle = {{{InPars-v2}}},
  author = {Jeronymo, Vitor and Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and Zavrel, Jakub and Nogueira, Rodrigo},
  year = {2023},
  month = jan,
  number = {arXiv:2301.01820},
  eprint = {arXiv:2301.01820},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.01820},
  urldate = {2023-01-23},
  abstract = {Recently, InPars introduced a method to efficiently use large language models (LLMs) in information retrieval tasks: via few-shot examples, an LLM is induced to generate relevant queries for documents. These synthetic query-document pairs can then be used to train a retriever. However, InPars and, more recently, Promptagator, rely on proprietary LLMs such as GPT-3 and FLAN to generate such datasets. In this work we introduce InPars-v2, a dataset generator that uses open-source LLMs and existing powerful rerankers to select synthetic query-document pairs for training. A simple BM25 retrieval pipeline followed by a monoT5 reranker finetuned on InPars-v2 data achieves new state-of-the-art results on the BEIR benchmark. To allow researchers to further improve our method, we open source the code, synthetic data, and finetuned models: https://github.com/zetaalphavector/inPars/tree/master/tpu},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/K57TISV7/Jeronymo et al. - 2023 - InPars-v2 Large Language Models as Efficient Data.pdf;/home/xav/Zotero/storage/QVR2MFTT/2301.html}
}

@article{jiangHowCanWe2020,
  title = {How {{Can We Know What Language Models Know}}?},
  author = {Jiang, Zhengbao and Xu, Frank F. and Araki, Jun and Neubig, Graham},
  year = {2020},
  month = may,
  journal = {arXiv:1911.12543 [cs]},
  eprint = {1911.12543},
  primaryclass = {cs},
  urldate = {2022-04-25},
  abstract = {Recent work has presented intriguing results examining the knowledge contained in language models (LM) by having the LM fill in the blanks of prompts such as "Obama is a \_ by profession". These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as "Obama worked as a \_" may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1\% to 39.6\%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/VRE862GP/Jiang et al. - 2020 - How Can We Know What Language Models Know.pdf;/home/xav/Zotero/storage/94ZHK4Z7/1911.html}
}

@misc{jiangInstancewisePromptTuning2022,
  title = {Instance-Wise {{Prompt Tuning}} for {{Pretrained Language Models}}},
  author = {Jiang, Yuezihan and Yang, Hao and Lin, Junyang and Zhao, Hanyu and Yang, An and Zhou, Chang and Yang, Hongxia and Yang, Zhi and Cui, Bin},
  year = {2022},
  month = jun,
  number = {arXiv:2206.01958},
  eprint = {arXiv:2206.01958},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.01958},
  urldate = {2023-02-07},
  abstract = {Prompt Learning has recently gained great popularity in bridging the gap between pretraining tasks and various downstream tasks. It freezes Pretrained Language Models (PLMs) and only tunes a few task-related parameters (prompts) for downstream tasks, greatly reducing the cost of tuning giant models. The key enabler of this is the idea of querying PLMs with task-specific knowledge implicated in prompts. This paper reveals a major limitation of existing methods that the indiscriminate prompts for all input data in a task ignore the intrinsic knowledge from input data, resulting in sub-optimal performance. We introduce Instance-wise Prompt Tuning (IPT), the first prompt learning paradigm that injects knowledge from the input data instances to the prompts, thereby providing PLMs with richer and more concrete context information. We devise a series of strategies to produce instance-wise prompts, addressing various concerns like model quality and cost-efficiency. Across multiple tasks and resource settings, IPT significantly outperforms task-based prompt learning methods, and achieves comparable performance to conventional finetuning with only 0.5\% - 1.5\% of tuned parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/XBAQKFW7/Jiang et al. - 2022 - Instance-wise Prompt Tuning for Pretrained Languag.pdf;/home/xav/Zotero/storage/7FKJHIUI/2206.html}
}

@inproceedings{jiaTempQuestionsBenchmarkTemporal2018,
  title = {{{TempQuestions}}: {{A Benchmark}} for {{Temporal Question Answering}}},
  shorttitle = {{{TempQuestions}}},
  booktitle = {Companion {{Proceedings}} of the {{The Web Conference}} 2018},
  author = {Jia, Zhen and Abujabal, Abdalghani and Saha Roy, Rishiraj and Str{\"o}tgen, Jannik and Weikum, Gerhard},
  year = {2018},
  month = apr,
  series = {{{WWW}} '18},
  pages = {1057--1062},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Republic and Canton of Geneva, CHE}},
  doi = {10.1145/3184558.3191536},
  urldate = {2023-02-01},
  abstract = {Answering complex questions is one of the challenges that question-answering (QA) systems face today. While complexity has several facets, question dimensions like temporal and spatial intents necessitate specialized treatment. Methods geared towards such questions need benchmarks that reflect the desired aspects and challenges. Here, we take a key step in this direction, and release a new benchmark, TempQuestions, containing 1,271 questions, that are all temporal in nature, paired with their answers. As a key contribution that enabled the creation of this resource, we provide a crisp definition for temporal questions. Most questions require decomposing them into sub-questions, and the questions are of a kind that they would be best evaluated on a combination of structured data and unstructured text sources. Experiments with two QA systems demonstrate the need for further research on complex questions.},
  isbn = {978-1-4503-5640-4},
  keywords = {benchmarks,question answering,temporal questions},
  file = {/home/xav/Zotero/storage/6G7UTZ5B/Jia et al. - 2018 - TempQuestions A Benchmark for Temporal Question A.pdf}
}

@inproceedings{jiaTEQUILATemporalQuestion2018,
  title = {{{TEQUILA}}: {{Temporal Question Answering}} over {{Knowledge Bases}}},
  shorttitle = {{{TEQUILA}}},
  booktitle = {Proceedings of the 27th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Jia, Zhen and Abujabal, Abdalghani and Roy, Rishiraj Saha and Stroetgen, Jannik and Weikum, Gerhard},
  year = {2018},
  month = oct,
  eprint = {1908.03650},
  primaryclass = {cs},
  pages = {1807--1810},
  doi = {10.1145/3269206.3269247},
  urldate = {2023-02-01},
  abstract = {Question answering over knowledge bases (KB-QA) poses challenges in handling complex questions that need to be decomposed into sub-questions. An important case, addressed here, is that of temporal questions, where cues for temporal relations need to be discovered and handled. We present TEQUILA, an enabler method for temporal QA that can run on top of any KB-QA engine. TEQUILA has four stages. It detects if a question has temporal intent. It decomposes and rewrites the question into non-temporal sub-questions and temporal constraints. Answers to sub-questions are then retrieved from the underlying KB-QA engine. Finally, TEQUILA uses constraint reasoning on temporal intervals to compute final answers to the full question. Comparisons against state-of-the-art baselines show the viability of our method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/IWZ7L3QX/Jia et al. - 2018 - TEQUILA Temporal Question Answering over Knowledg.pdf;/home/xav/Zotero/storage/KVBT2NYF/1908.html}
}

@misc{jiControllingFocusPretrained2022,
  title = {Controlling the {{Focus}} of {{Pretrained Language Generation Models}}},
  author = {Ji, Jiabao and Kim, Yoon and Glass, James and He, Tianxing},
  year = {2022},
  month = mar,
  number = {arXiv:2203.01146},
  eprint = {arXiv:2203.01146},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.01146},
  urldate = {2022-07-19},
  abstract = {The finetuning of pretrained transformer-based language generation models are typically conducted in an end-to-end manner, where the model learns to attend to relevant parts of the input by itself. However, there does not exist a mechanism to directly control the model's focus. This work aims to develop a control mechanism by which a user can select spans of context as "highlights" for the model to focus on, and generate relevant output. To achieve this goal, we augment a pretrained model with trainable "focus vectors" that are directly applied to the model's embeddings, while the model itself is kept fixed. These vectors, trained on automatic annotations derived from attribution methods, act as indicators for context importance. We test our approach on two core generation tasks: dialogue response generation and abstractive summarization. We also collect evaluation data where the highlight-generation pairs are annotated by humans. Our experiments show that the trained focus vectors are effective in steering the model to generate outputs that are relevant to user-selected highlights.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/xav/Zotero/storage/GKMQQI9M/Ji et al. - 2022 - Controlling the Focus of Pretrained Language Gener.pdf;/home/xav/Zotero/storage/2236GSRW/2203.html}
}

@misc{jinBiomedicalQuestionAnswering2021,
  title = {Biomedical {{Question Answering}}: {{A Survey}} of {{Approaches}} and {{Challenges}}},
  shorttitle = {Biomedical {{Question Answering}}},
  author = {Jin, Qiao and Yuan, Zheng and Xiong, Guangzhi and Yu, Qianlan and Ying, Huaiyuan and Tan, Chuanqi and Chen, Mosha and Huang, Songfang and Liu, Xiaozhong and Yu, Sheng},
  year = {2021},
  month = sep,
  number = {arXiv:2102.05281},
  eprint = {arXiv:2102.05281},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {Automatic Question Answering (QA) has been successfully applied in various domains such as search engines and chatbots. Biomedical QA (BQA), as an emerging QA task, enables innovative applications to effectively perceive, access and understand complex biomedical knowledge. There have been tremendous developments of BQA in the past two decades, which we classify into 5 distinctive approaches: classic, information retrieval, machine reading comprehension, knowledge base and question entailment approaches. In this survey, we introduce available datasets and representative methods of each BQA approach in detail. Despite the developments, BQA systems are still immature and rarely used in real-life settings. We identify and characterize several key challenges in BQA that might lead to this issue, and discuss some potential future directions to explore.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/LG6EBD3F/Jin et al. - 2021 - Biomedical Question Answering A Survey of Approac.pdf}
}

@article{jinEdgeformersGraphEmpoweredTransformers2023,
  title = {Edgeformers: {{Graph-Empowered Transformers}} for {{Representation Learning}} on {{Textual-Edge Networks}}},
  shorttitle = {Edgeformers},
  author = {Jin, Bowen and Zhang, Yu and Meng, Yu and Han, Jiawei},
  year = {2023},
  doi = {10.48550/ARXIV.2302.11050},
  urldate = {2023-03-11},
  abstract = {Edges in many real-world social/information networks are associated with rich text information (e.g., user-user communications or user-product reviews). However, mainstream network representation learning models focus on propagating and aggregating node attributes, lacking specific designs to utilize text semantics on edges. While there exist edge-aware graph neural networks, they directly initialize edge attributes as a feature vector, which cannot fully capture the contextualized text semantics of edges. In this paper, we propose Edgeformers, a framework built upon graph-enhanced Transformers, to perform edge and node representation learning by modeling texts on edges in a contextualized way. Specifically, in edge representation learning, we inject network information into each Transformer layer when encoding edge texts; in node representation learning, we aggregate edge representations through an attention mechanism within each node's ego-graph. On five public datasets from three different domains, Edgeformers consistently outperform state-of-the-art baselines in edge classification and link prediction, demonstrating the efficacy in learning edge and node representations, respectively.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG),Social and Information Networks (cs.SI)}
}

@misc{jinHeterformerTransformerArchitecture2022,
  title = {Heterformer: {{A Transformer Architecture}} for {{Node Representation Learning}} on {{Heterogeneous Text-Rich Networks}}},
  shorttitle = {Heterformer},
  author = {Jin, Bowen and Zhang, Yu and Zhu, Qi and Han, Jiawei},
  year = {2022},
  month = may,
  number = {arXiv:2205.10282},
  eprint = {arXiv:2205.10282},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10282},
  urldate = {2022-11-18},
  abstract = {We study node representation learning on heterogeneous text-rich networks, where nodes and edges are multi-typed and some types of nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have demonstrated their power in encoding network and text signals, respectively, less focus has been given to delicately coupling these two types of models on heterogeneous text-rich networks. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. In this paper, we propose Heterformer, a Heterogeneous GNN-nested transformer that blends GNNs and PLMs into a unified model. Different from previous "cascaded architectures" that directly add GNN layers upon a PLM, our Heterformer alternately stacks two modules - a graph-attention-based neighbor aggregation module and a transformer-based text and neighbor joint encoding module - to facilitate thorough mutual enhancement between network and text signals. Meanwhile, Heterformer is capable of characterizing network heterogeneity and nodes without text information. Comprehensive experiments on three large-scale datasets from different domains demonstrate the superiority of Heterformer over state-of-the-art baselines in link prediction, transductive/inductive node classification, node clustering, and semantics-based retrieval.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/L52LVXNQ/Jin et al. - 2022 - Heterformer A Transformer Architecture for Node R.pdf}
}

@inproceedings{jinHooksHeadlineLearning2020,
  title = {Hooks in the {{Headline}}: {{Learning}} to {{Generate Headlines}} with {{Controlled Styles}}},
  shorttitle = {Hooks in the {{Headline}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Orii, Lisa and Szolovits, Peter},
  year = {2020},
  month = jul,
  pages = {5082--5093},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.456},
  urldate = {2023-02-02},
  abstract = {Current summarization systems only produce plain, factual headlines, far from the practical needs for the exposure and memorableness of the articles. We propose a new task, Stylistic Headline Generation (SHG), to enrich the headlines with three style options (humor, romance and clickbait), thus attracting more readers. With no style-specific article-headline pair (only a standard headline summarization dataset and mono-style corpora), our method TitleStylist generates stylistic headlines by combining the summarization and reconstruction tasks into a multitasking framework. We also introduced a novel parameter sharing scheme to further disentangle the style from text. Through both automatic and human evaluation, we demonstrate that TitleStylist can generate relevant, fluent headlines with three target styles: humor, romance, and clickbait. The attraction score of our model generated headlines outperforms the state-of-the-art summarization model by 9.68\%, even outperforming human-written references.},
  file = {/home/xav/Zotero/storage/2G94TLXT/Jin et al. - 2020 - Hooks in the Headline Learning to Generate Headli.pdf}
}

@misc{jinWhatDiseaseDoes2020,
  title = {What {{Disease}} Does This {{Patient Have}}? {{A Large-scale Open Domain Question Answering Dataset}} from {{Medical Exams}}},
  shorttitle = {What {{Disease}} Does This {{Patient Have}}?},
  author = {Jin, Di and Pan, Eileen and Oufattole, Nassim and Weng, Wei-Hung and Fang, Hanyi and Szolovits, Peter},
  year = {2020},
  month = sep,
  number = {arXiv:2009.13081},
  eprint = {arXiv:2009.13081},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.13081},
  urldate = {2023-01-23},
  abstract = {Open domain question answering (OpenQA) tasks have been recently attracting more and more attention from the natural language processing (NLP) community. In this work, we present the first free-form multiple-choice OpenQA dataset for solving medical problems, MedQA, collected from the professional medical board exams. It covers three languages: English, simplified Chinese, and traditional Chinese, and contains 12,723, 34,251, and 14,123 questions for the three languages, respectively. We implement both rule-based and popular neural methods by sequentially combining a document retriever and a machine comprehension model. Through experiments, we find that even the current best method can only achieve 36.7\textbackslash\%, 42.0\textbackslash\%, and 70.1\textbackslash\% of test accuracy on the English, traditional Chinese, and simplified Chinese questions, respectively. We expect MedQA to present great challenges to existing OpenQA systems and hope that it can serve as a platform to promote much stronger OpenQA models from the NLP community in the future.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/RQKCDTXD/Jin et al. - 2020 - What Disease does this Patient Have A Large-scale.pdf;/home/xav/Zotero/storage/FK8FDD4S/2009.html}
}

@misc{jiPatientOutcomeZeroshot2023,
  title = {Patient {{Outcome}} and {{Zero-shot Diagnosis Prediction}} with {{Hypernetwork-guided Multitask Learning}}},
  author = {Ji, Shaoxiong and Marttinen, Pekka},
  year = {2023},
  month = jan,
  number = {arXiv:2109.03062},
  eprint = {arXiv:2109.03062},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Multitask deep learning has been applied to patient outcome prediction from text, taking clinical notes as input and training deep neural networks with a joint loss function of multiple tasks. However, the joint training scheme of multitask learning suffers from inter-task interference, and diagnosis prediction among the multiple tasks has the generalizability issue due to rare diseases or unseen diagnoses. To solve these challenges, we propose a hypernetwork-based approach that generates task-conditioned parameters and coefficients of multitask prediction heads to learn task-specific prediction and balance the multitask learning. We also incorporate semantic task information to improve the generalizability of our task-conditioned multitask model. Experiments on early and discharge notes extracted from the real-world MIMIC database show our method can achieve better performance on multitask patient outcome prediction than strong baselines in most cases. Besides, our method can effectively handle the scenario with limited information and improve zero-shot prediction on unseen diagnosis categories.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/QLWUTRM6/Ji et Marttinen - 2023 - Patient Outcome and Zero-shot Diagnosis Prediction.pdf}
}

@article{jiSurveyHallucinationNatural2022,
  title = {Survey of {{Hallucination}} in {{Natural Language Generation}}},
  author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Yejin and Dai, Wenliang and Madotto, Andrea and Fung, Pascale},
  year = {2022},
  month = nov,
  journal = {ACM Computing Surveys},
  eprint = {2202.03629},
  primaryclass = {cs},
  pages = {3571730},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3571730},
  urldate = {2023-01-24},
  abstract = {Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before. In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG. CCS Concepts: \textbullet{} Computing methodologies \textrightarrow{} Natural language generation; Neural networks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {A.1,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/UIMLT94F/Ji et al. - 2022 - Survey of Hallucination in Natural Language Genera.pdf}
}

@article{jiSurveyKnowledgeGraphs2022,
  title = {A {{Survey}} on {{Knowledge Graphs}}: {{Representation}}, {{Acquisition}} and {{Applications}}},
  shorttitle = {A {{Survey}} on {{Knowledge Graphs}}},
  author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
  year = {2022},
  month = feb,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {2},
  eprint = {2002.00388},
  pages = {494--514},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2021.3070843},
  urldate = {2022-05-04},
  abstract = {Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction towards cognition and human-level intelligence. In this survey, we provide a comprehensive review of knowledge graph covering overall research topics about 1) knowledge graph representation learning, 2) knowledge acquisition and completion, 3) temporal knowledge graph, and 4) knowledge-aware applications, and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning, are reviewed. We further explore several emerging topics, including meta relational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of datasets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {QID: Q113531802},
  file = {/home/xav/Zotero/storage/E6T4ZY4A/Ji et al. - 2022 - A Survey on Knowledge Graphs Representation, Acqu.pdf}
}

@article{joSelectiveTokenGeneration2021,
  title = {Selective {{Token Generation}} for {{Few-shot Language Modeling}}},
  author = {Jo, Daejin and Kwon, Taehwan and Kim, Sungwoong and Kim, Eun-Sol},
  year = {2021},
  month = nov,
  urldate = {2022-09-23},
  abstract = {Natural language modeling with limited training data is challenging problem, and many algorithms make use of large-scale pretrained language models (PLMs) for this due to its great generalization...},
  langid = {english},
  file = {/home/xav/Zotero/storage/SRSGX5YY/Jo et al. - 2021 - Selective Token Generation for Few-shot Language M.pdf}
}

@misc{joshiTriviaQALargeScale2017,
  title = {{{TriviaQA}}: {{A Large Scale Distantly Supervised Challenge Dataset}} for {{Reading Comprehension}}},
  shorttitle = {{{TriviaQA}}},
  author = {Joshi, Mandar and Choi, Eunsol and Weld, Daniel S. and Zettlemoyer, Luke},
  year = {2017},
  month = may,
  number = {arXiv:1705.03551},
  eprint = {arXiv:1705.03551},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.03551},
  urldate = {2023-01-23},
  abstract = {We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23\% and 40\% vs. 80\%), suggesting that TriviaQA is a challenging testbed that is worth significant future study. Data and code available at -- http://nlp.cs.washington.edu/triviaqa/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/IJU8X7LI/Joshi et al. - 2017 - TriviaQA A Large Scale Distantly Supervised Chall.pdf;/home/xav/Zotero/storage/6DZF4MI5/1705.html}
}

@misc{josifoskiExploitingAsymmetrySynthetic2023,
  title = {Exploiting {{Asymmetry}} for {{Synthetic Training Data Generation}}: {{SynthIE}} and the {{Case}} of {{Information Extraction}}},
  shorttitle = {Exploiting {{Asymmetry}} for {{Synthetic Training Data Generation}}},
  author = {Josifoski, Martin and Sakota, Marija and Peyrard, Maxime and West, Robert},
  year = {2023},
  month = mar,
  number = {arXiv:2303.04132},
  eprint = {arXiv:2303.04132},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.04132},
  urldate = {2023-03-20},
  abstract = {Large language models (LLMs) show great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by the LLM: we show that, for problems with structured outputs, it is possible to prompt an LLM to perform the task in the opposite direction, to generate plausible text for the target structure. Leveraging the asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, demonstrate its superior quality compared to existing datasets in a human evaluation and use it to finetune small models (220M and 770M parameters). The models we introduce, SynthIE, outperform existing baselines of comparable size with a substantial gap of 57 and 79 absolute points in micro and macro F1, respectively. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/MBIGV559/Josifoski et al. - 2023 - Exploiting Asymmetry for Synthetic Training Data G.pdf;/home/xav/Zotero/storage/QJVPIITB/2303.html}
}

@article{jozashooriMapSDIScaledupSemantic2019,
  title = {{{MapSDI}}: {{A Scaled-up Semantic Data Integration Framework}} for {{Knowledge Graph Creation}}},
  shorttitle = {{{MapSDI}}},
  author = {Jozashoori, Samaneh and Vidal, Maria-Esther},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.01032 [cs]},
  eprint = {1909.01032},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Semantic web technologies have significantly contributed with effective solutions for the problems of data integration and knowledge graph creation. However, with the rapid growth of big data in diverse domains, different interoperability issues still demand to be addressed, being scalability one of the main challenges. In this paper, we address the problem of knowledge graph creation at scale and provide MapSDI, a mapping rule-based framework for optimizing semantic data integration into knowledge graphs. MapSDI allows for the semantic enrichment of large-sized, heterogeneous, and potentially low-quality data efficiently. The input of MapSDI is a set of data sources and mapping rules being generated by a mapping language such as RML. First, MapSDI pre-processes the sources based on semantic information extracted from mapping rules, by performing basic database operators; it projects out required attributes, eliminates duplicates, and selects relevant entries. All these operators are defined based on the knowledge encoded by the mapping rules which will be then used by the semantification engine (or RDFizer) to produce a knowledge graph. We have empirically studied the impact of MapSDI on existing RDFizers, and observed that knowledge graph creation time can be reduced on average in one order of magnitude. It is also shown, theoretically, that the sources and rules transformations provided by MapSDI are data-lossless.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Databases},
  file = {/home/xav/Zotero/storage/AT8WTZG2/Jozashoori et Vidal - 2019 - MapSDI A Scaled-up Semantic Data Integration Fram.pdf;/home/xav/Zotero/storage/VGDI7JRK/1909.html}
}

@misc{jukicSmoothSailingImproving2022,
  title = {Smooth {{Sailing}}: {{Improving Active Learning}} for {{Pre-trained Language Models}} with {{Representation Smoothness Analysis}}},
  shorttitle = {Smooth {{Sailing}}},
  author = {Juki{\'c}, Josip and {\v S}najder, Jan},
  year = {2022},
  month = dec,
  number = {arXiv:2212.11680},
  eprint = {arXiv:2212.11680},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.11680},
  urldate = {2023-01-04},
  abstract = {Developed as a solution to a practical need, active learning (AL) methods aim to reduce label complexity and the annotations costs in supervised learning. While recent work has demonstrated the benefit of using AL in combination with large pre-trained language models (PLMs), it has often overlooked the practical challenges that hinder the feasibility of AL in realistic settings. We address these challenges by leveraging representation smoothness analysis to improve the effectiveness of AL. We develop an early stopping technique that does not require a validation set -- often unavailable in realistic AL settings -- and observe significant improvements across multiple datasets and AL methods. Additionally, we find that task adaptation improves AL, whereas standard short fine-tuning in AL does not provide improvements over random sampling. Our work establishes the usefulness of representation smoothness analysis in AL and presents an AL stopping criterion that reduces label complexity.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/XTZQQ66A/Jukiƒá et ≈†najder - 2022 - Smooth Sailing Improving Active Learning for Pre-.pdf}
}

@misc{jungMaieuticPromptingLogically2022,
  title = {Maieutic {{Prompting}}: {{Logically Consistent Reasoning}} with {{Recursive Explanations}}},
  shorttitle = {Maieutic {{Prompting}}},
  author = {Jung, Jaehun and Qin, Lianhui and Welleck, Sean and Brahman, Faeze and Bhagavatula, Chandra and Bras, Ronan Le and Choi, Yejin},
  year = {2022},
  month = oct,
  number = {arXiv:2205.11822},
  eprint = {arXiv:2205.11822},
  publisher = {{arXiv}},
  urldate = {2023-02-07},
  abstract = {Despite their impressive capabilities, large pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which infers a correct answer to a question even from the noisy and inconsistent generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20\% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/7YVNIMVX/Jung et al. - 2022 - Maieutic Prompting Logically Consistent Reasoning.pdf}
}

@article{kadriuExtractiveApproachText2021,
  title = {Extractive Approach for Text Summarisation Using Graphs},
  author = {Kadriu, Kastriot and Obradovic, Milenko},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.10955 [cs]},
  eprint = {2106.10955},
  primaryclass = {cs},
  urldate = {2022-05-13},
  abstract = {Natural language processing is an important discipline with the aim of understanding text by its digital representation, that due to the diverse way we write and speak, is often not accurate enough. Our paper explores different graph-related algorithms that can be used in solving the text summarization problem using an extractive approach. We consider two metrics: sentence overlap and edit distance for measuring sentence similarity.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/AWCHA79P/Kadriu et Obradovic - 2021 - Extractive approach for text summarisation using g.pdf}
}

@misc{kalyanAMMUSSurveyTransformerbased2021,
  title = {{{AMMUS}} : {{A Survey}} of {{Transformer-based Pretrained Models}} in {{Natural Language Processing}}},
  shorttitle = {{{AMMUS}}},
  author = {Kalyan, Katikapalli Subramanyam and Rajasekharan, Ajit and Sangeetha, Sivanesan},
  year = {2021},
  month = aug,
  number = {arXiv:2108.05542},
  eprint = {arXiv:2108.05542},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/G5MF4DHW/Kalyan et al. - 2021 - AMMUS  A Survey of Transformer-based Pretrained M.pdf}
}

@article{kalyanpurFactbasedQuestionDecomposition2012,
  title = {Fact-Based Question Decomposition in {{DeepQA}}},
  author = {Kalyanpur, A. and Patwardhan, S. and Boguraev, B. K. and Lally, A. and {Chu-Carroll}, J.},
  year = {2012},
  month = may,
  journal = {IBM Journal of Research and Development},
  volume = {56},
  number = {3.4},
  pages = {13:1-13:11},
  issn = {0018-8646},
  doi = {10.1147/JRD.2012.2188934},
  abstract = {Factoid questions often contain more than one fact or assertion about their answers. Question-answering (QA) systems, however, typically do not use such fine-grained distinctions because of the need for deep understanding of the question in order to identify and separate the facts. We argue that decomposing complex factoid questions is beneficial to QA systems, because the more facts that support an answer candidate, the more likely it is to be the correct answer. We broadly categorize decomposable questions into two types: parallel and nested. Parallel decomposable questions contain subquestions that can be evaluated independent of each other. Nested questions require decompositions to be processed in sequence, with the answer to an ``inner'' subquestion plugged into an ``outer'' subquestion. In this paper, we present a novel question decomposition framework capable of handling both decomposition types, built on top of the base IBM Watson\texttrademark{} QA system for Jeopardy!\texttrademark. The framework contains a suite of decomposition rules that use predominantly lexico-syntactic features to identify facts within complex questions. It also contains a question-rewriting component and a candidate re-ranker, which uses machine learning and heuristic selection strategies to generate a final ranked answer list, taking into account answer confidences from the base QA system. We apply our decomposition framework to the particularly challenging domain of Final Jeopardy! questions, which are found to be difficult even for qualified Jeopardy! players, and we show a statistically significant improvement in the performance of our baseline QA system.},
  keywords = {Companies,Complexity theory,Context awareness,History,Machine learning,Syntactics},
  file = {/home/xav/Zotero/storage/EFVQYT9M/Kalyanpur et al. - 2012 - Fact-based question decomposition in DeepQA.pdf;/home/xav/Zotero/storage/YNLXXI3H/6177726.html}
}

@article{kangKALAKnowledgeAugmentedLanguage2022,
  title = {{{KALA}}: {{Knowledge-Augmented Language Model Adaptation}}},
  shorttitle = {{{KALA}}},
  author = {Kang, Minki and Baek, Jinheon and Hwang, Sung Ju},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2204.10555},
  abstract = {Pre-trained language models (PLMs) have achieved remarkable success on various natural language understanding tasks. Simple fine-tuning of PLMs, on the other hand, might be suboptimal for domain-specific tasks because they cannot possibly cover knowledge from all domains. While adaptive pre-training of PLMs can help them obtain domain-specific knowledge, it requires a large training cost. Moreover, adaptive pre-training can harm the PLM's performance on the downstream task by causing catastrophic forgetting of its general knowledge. To overcome such limitations of adaptive pre-training for PLM adaption, we propose a novel domain adaption framework for PLMs coined as Knowledge-Augmented Language model Adaptation (KALA), which modulates the intermediate hidden representations of PLMs with domain knowledge, consisting of entities and their relational facts. We validate the performance of our KALA on question answering and named entity recognition tasks on multiple datasets across various domains. The results show that, despite being computationally efficient, our KALA largely outperforms adaptive pre-training. Code is available at: https://github.com/Nardien/KALA.},
  file = {/home/xav/Zotero/storage/PJJS7FX8/Kang et al. - 2022 - KALA Knowledge-Augmented Language Model Adaptatio.pdf}
}

@misc{karadzhovDeliDataDatasetDeliberation2022,
  title = {{{DeliData}}: {{A}} Dataset for Deliberation in Multi-Party Problem Solving},
  shorttitle = {{{DeliData}}},
  author = {Karadzhov, Georgi and Stafford, Tom and Vlachos, Andreas},
  year = {2022},
  month = may,
  number = {arXiv:2108.05271},
  eprint = {arXiv:2108.05271},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.05271},
  urldate = {2022-10-10},
  abstract = {Dialogue systems research is traditionally focused on dialogues between two interlocutors, largely ignoring group conversations. Moreover, most previous research is focused either on task-oriented dialogue (e.g.\textbackslash{} restaurant bookings) or user engagement (chatbots), while research on systems for collaborative dialogues is an under-explored area. To this end, we introduce the first publicly available dataset containing collaborative conversations on solving a cognitive task, consisting of 500 group dialogues and 14k utterances. Furthermore, we propose a novel annotation schema that captures deliberation cues and release 50 dialogues annotated with it. Finally, we demonstrate the usefulness of the annotated data in training classifiers to predict the constructiveness of a conversation. The data collection platform, dataset and annotated corpus are publicly available at https://delibot.xyz},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/home/xav/Zotero/storage/HD5Q3N2K/Karadzhov et al. - 2022 - DeliData A dataset for deliberation in multi-part.pdf}
}

@misc{kazemiLAMBADABackwardChaining2022,
  title = {{{LAMBADA}}: {{Backward Chaining}} for {{Automated Reasoning}} in {{Natural Language}}},
  shorttitle = {{{LAMBADA}}},
  author = {Kazemi, Seyed Mehran and Kim, Najoung and Bhatia, Deepti and Xu, Xin and Ramachandran, Deepak},
  year = {2022},
  month = dec,
  number = {arXiv:2212.13894},
  eprint = {arXiv:2212.13894},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.13894},
  urldate = {2023-02-07},
  abstract = {Remarkable progress has been made on automated reasoning with knowledge specified as unstructured, natural text, by using the power of large language models (LMs) coupled with methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to the set of axioms that support it) is significantly more efficient at proof-finding problems. We import this intuition into the LM setting and develop a Backward Chaining algorithm, which we call LAMBADA, that decomposes reasoning into four sub-modules, each of which can be simply implemented by few-shot prompted LM inference. We show that LAMBADA achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/4ZG9WUDU/Kazemi et al. - 2022 - LAMBADA Backward Chaining for Automated Reasoning.pdf;/home/xav/Zotero/storage/ZG2B8ETJ/2212.html}
}

@inproceedings{keaneHowCaseBasedReasoning2019,
  title = {How {{Case-Based Reasoning Explains Neural Networks}}: {{A Theoretical Analysis}} of {{XAI Using Post-Hoc Explanation-by-Example}} from a {{Survey}} of {{ANN-CBR Twin-Systems}}},
  shorttitle = {How {{Case-Based Reasoning Explains Neural Networks}}},
  booktitle = {{{ICCBR}}},
  author = {Keane, Mark T. and Kenny, Eoin M.},
  year = {2019},
  doi = {10.1007/978-3-030-29249-2_11},
  abstract = {The twin-systems approach is advanced as one possible coherent, generic solution to the XAI problem, and the paper concludes by road-mapping future directions for this XAI solution, considering further tests of feature-weighting techniques. This paper proposes a theoretical analysis of one approach to the eXplainable AI (XAI) problem, using post-hoc explanation-by-example, that relies on the twinning of artificial neural networks (ANNs) with case-based reasoning (CBR) systems; so-called ANN-CBR twins. It surveys these systems to advance a new theoretical interpretation of previous work and define a road map for CBR's further role in XAI. A systematic survey of 1,102 papers was conducted to identify a fragmented literature on this topic and trace its influence to more recent work involving deep neural networks (DNNs). The twin-systems approach is advanced as one possible coherent, generic solution to the XAI problem. The paper concludes by road-mapping future directions for this XAI solution, considering (i) further tests of feature-weighting techniques, (ii) how explanatory cases might be deployed (e.g., in counterfactuals, a fortori cases), and (iii) the unwelcome, much-ignored issue of user evaluation.},
  file = {/home/xav/Zotero/storage/FQBNJGDP/Keane et Kenny - 2019 - How Case-Based Reasoning Explains Neural Networks.pdf}
}

@inproceedings{keContinualLearningLanguage2023,
  title = {Continual {{Learning}} of {{Language Models}}},
  booktitle = {The {{Eleventh International Conference}} on {{Learning Representations}}},
  author = {Ke, Zixuan and Shao, Yijia and Lin, Haowei and Konishi, Tatsuya and Kim, Gyuhak and Liu, Bing},
  year = {2023},
  month = feb,
  urldate = {2023-03-24},
  abstract = {Language models (LMs) have been instrumental for the rapid advance of natural language processing. This paper studies continual learning of LMs, in particular, continual domain-adaptive pre-training (or continual DAP-training). Existing research has shown that further pre-training an LM using a domain corpus to adapt the LM to the domain can improve the end-task performance in the domain. This paper proposes a novel method to continually DAP-train an LM with a sequence of unlabeled domain corpora to adapt the LM to these domains to improve their endtask performances. The key novelty of our method is a soft-masking mechanism that directly controls the update to the LM. A novel proxy is also proposed to preserve the general knowledge in the original LM. Additionally, it contrasts the representations of the previously learned domain knowledge (including the general knowledge in the pre-trained LM) and the knowledge from the current full network to achieve knowledge integration. The method not only overcomes catastrophic forgetting, but also achieves knowledge transfer to improve end-task performances. Empirical evaluation demonstrates the effectiveness of the proposed method.},
  langid = {english},
  file = {/home/xav/Zotero/storage/Q83XKEKT/Ke et al. - 2023 - Continual Learning of Language Models.pdf}
}

@article{keCONTINUALLEARNINGLANGUAGE2023,
  title = {{{CONTINUAL LEARNING OF LANGUAGE MODELS}}},
  author = {Ke, Zixuan and Shao, Yijia and Lin, Haowei and Konishi, Tatsuya and Kim, Gyuhak and Liu, Bing},
  year = {2023},
  langid = {english},
  file = {/home/xav/Zotero/storage/2UUX38RD/Ke et al. - 2023 - CONTINUAL LEARNING OF LANGUAGE MODELS.pdf}
}

@inproceedings{keJointGTGraphTextJoint2021,
  title = {{{JointGT}}: {{Graph-Text Joint Representation Learning}} for {{Text Generation}} from {{Knowledge Graphs}}},
  shorttitle = {{{JointGT}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Ke, Pei and Ji, Haozhe and Ran, Yu and Cui, Xin and Wang, Liwei and Song, Linfeng and Zhu, Xiaoyan and Huang, Minlie},
  year = {2021},
  pages = {2526--2538},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.223},
  urldate = {2022-05-04},
  abstract = {Existing pre-trained models for knowledgegraph-to-text (KG-to-text) generation simply fine-tune text-to-text pre-trained models such as BART or T5 on KG-to-text datasets, which largely ignore the graph structure during encoding and lack elaborate pre-training tasks to explicitly model graph-text alignments. To tackle these problems, we propose a graph-text joint representation learning model called JointGT. During encoding, we devise a structure-aware semantic aggregation module which is plugged into each Transformer layer to preserve the graph structure. Furthermore, we propose three new pre-training tasks to explicitly enhance the graph-text alignment including respective text / graph reconstruction, and graph-text alignment in the embedding space via Optimal Transport. Experiments show that JointGT obtains new stateof-the-art performance on various KG-to-text datasets1.},
  langid = {english},
  file = {/home/xav/Zotero/storage/AGXN99HL/Ke et al. - 2021 - JointGT Graph-Text Joint Representation Learning .pdf}
}

@book{kejriwalDomainSpecificKnowledgeGraph2019,
  title = {Domain-{{Specific Knowledge Graph Construction}}},
  author = {Kejriwal, Mayank},
  year = {2019},
  series = {{{SpringerBriefs}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-12375-8},
  urldate = {2022-05-04},
  isbn = {978-3-030-12374-1 978-3-030-12375-8},
  langid = {english},
  keywords = {Data Mining,Domain Discovery,Entity Resolution,Entity-Centric Search,Information Extraction,Knowledge Discory,Knowledge Graph Completion,Knowledge Graph Construction,Knowledge Graph Embeddings,Knowledge Graphs,Machine Learning,Natural Language Processing,Probabilistic Soft Logic,Querying,Semantic Web,Web Corpora,Wrapper Induction},
  file = {/home/xav/Zotero/storage/JBPNXTTL/Kejriwal - 2019 - Domain-Specific Knowledge Graph Construction.pdf}
}

@misc{keskarCTRLConditionalTransformer2019a,
  title = {{{CTRL}}: {{A Conditional Transformer Language Model}} for {{Controllable Generation}}},
  shorttitle = {{{CTRL}}},
  author = {Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R. and Xiong, Caiming and Socher, Richard},
  year = {2019},
  month = sep,
  number = {arXiv:1909.05858},
  eprint = {arXiv:1909.05858},
  publisher = {{arXiv}},
  urldate = {2022-07-19},
  abstract = {Large-scale language models show promising text generation capabilities, but users cannot easily control particular aspects of the generated text. We release CTRL, a 1.63 billion-parameter conditional transformer language model, trained to condition on control codes that govern style, content, and task-specific behavior. Control codes were derived from structure that naturally co-occurs with raw text, preserving the advantages of unsupervised learning while providing more explicit control over text generation. These codes also allow CTRL to predict which parts of the training data are most likely given a sequence. This provides a potential method for analyzing large amounts of data via model-based source attribution. We have released multiple full-sized, pretrained versions of CTRL at https://github.com/salesforce/ctrl.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/97XPY6PU/Keskar et al. - 2019 - CTRL A Conditional Transformer Language Model for.pdf}
}

@misc{keySpeakYouVerify2022,
  title = {I {{Speak}}, {{You Verify}}: {{Toward Trustworthy Neural Program Synthesis}}},
  shorttitle = {I {{Speak}}, {{You Verify}}},
  author = {Key, Darren and Li, Wen-Ding and Ellis, Kevin},
  year = {2022},
  month = sep,
  number = {arXiv:2210.00848},
  eprint = {arXiv:2210.00848},
  publisher = {{arXiv}},
  urldate = {2023-01-05},
  abstract = {We develop an approach for improving the trustworthiness and overall accuracy of program synthesizers based on large language models for source code. Given a natural language description of a programming problem, our method samples both candidate programs as well as candidate predicates specifying how the program should behave. We learn to analyze the agreement between programs and predicates to judge both which program is most likely to be correct, and also judge whether the language model is able to solve the programming problem in the first place. This latter capacity allows favoring high precision over broad recall: fostering trust by only proposing a program when the system is certain that it is correct.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Computer Science - Software Engineering},
  file = {/home/xav/Zotero/storage/R2KUFBKF/Key et al. - 2022 - I Speak, You Verify Toward Trustworthy Neural Pro.pdf}
}

@article{khashabiUnifiedQACrossingFormat2020,
  title = {{{UnifiedQA}}: {{Crossing Format Boundaries With}} a {{Single QA System}}},
  shorttitle = {{{UnifiedQA}}},
  author = {Khashabi, Daniel and Min, Sewon and Khot, Tushar and Sabharwal, Ashish and Tafjord, Oyvind and Clark, Peter and Hajishirzi, Hannaneh},
  year = {2020},
  month = oct,
  journal = {arXiv:2005.00700 [cs]},
  eprint = {2005.00700},
  primaryclass = {cs},
  urldate = {2022-04-25},
  abstract = {Question answering (QA) tasks have been posed using a variety of formats, such as extractive span selection, multiple choice, etc. This has led to format-specialized models, and even to an implicit division in the QA community. We argue that such boundaries are artificial and perhaps unnecessary, given the reasoning abilities we seek to teach are not governed by the format. As evidence, we use the latest advances in language modeling to build a single pre-trained QA model, UnifiedQA, that performs surprisingly well across 17 QA datasets spanning 4 diverse formats. UnifiedQA performs on par with 9 different models that were trained on individual datasets themselves. Even when faced with 12 unseen datasets of observed formats, UnifiedQA performs surprisingly well, showing strong generalization from its out-of-format training data. Finally, simply fine-tuning this pre-trained QA model into specialized models results in a new state of the art on 6 datasets, establishing UnifiedQA as a strong starting point for building QA systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/55DAKR4Z/Khashabi et al. - 2020 - UnifiedQA Crossing Format Boundaries With a Singl.pdf;/home/xav/Zotero/storage/BJZLYBAD/2005.html}
}

@misc{khattabDemonstrateSearchPredictComposingRetrieval2023,
  title = {Demonstrate-{{Search-Predict}}: {{Composing}} Retrieval and Language Models for Knowledge-Intensive {{NLP}}},
  shorttitle = {Demonstrate-{{Search-Predict}}},
  author = {Khattab, Omar and Santhanam, Keshav and Li, Xiang Lisa and Hall, David and Liang, Percy and Potts, Christopher and Zaharia, Matei},
  year = {2023},
  month = jan,
  number = {arXiv:2212.14024},
  eprint = {arXiv:2212.14024},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.14024},
  urldate = {2023-02-14},
  abstract = {Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple "retrieve-then-read" pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120\%, 8-39\%, and 80-290\% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/885AG6F7/Khattab et al. - 2023 - Demonstrate-Search-Predict Composing retrieval an.pdf;/home/xav/Zotero/storage/NLIXIHSK/2212.html}
}

@misc{khotDecomposedPromptingModular2022,
  title = {Decomposed {{Prompting}}: {{A Modular Approach}} for {{Solving Complex Tasks}}},
  shorttitle = {Decomposed {{Prompting}}},
  author = {Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02406},
  eprint = {arXiv:2210.02406},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.02406},
  urldate = {2023-02-07},
  abstract = {Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/QKKYI6TJ/Khot et al. - 2022 - Decomposed Prompting A Modular Approach for Solvi.pdf;/home/xav/Zotero/storage/CCHPGDAA/2210.html}
}

@article{khotLearningSolveComplex2021,
  title = {Learning to {{Solve Complex Tasks}} by {{Talking}} to {{Agents}}},
  author = {Khot, Tushar and Richardson, Kyle and Khashabi, Daniel and Sabharwal, Ashish},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.08542 [cs]},
  eprint = {2110.08542},
  primaryclass = {cs},
  urldate = {2022-03-28},
  abstract = {Humans often solve complex problems by interacting (in natural language) with existing agents, such as AI assistants, that can solve simpler sub-tasks. These agents themselves can be powerful systems built using extensive resources and privately held data. In contrast, common NLP benchmarks aim for the development of self-sufficient models for every task. To address this gap and facilitate research towards ``green'' AI systems that build upon existing agents, we propose a new benchmark called CommaQA that contains three kinds of complex reasoning tasks that are designed to be solved by ``talking'' to four agents with different capabilities. We demonstrate that state-of-the-art black-box models, which are unable to leverage existing agents, struggle on CommaQA (exact match score only reaches 40pts) even when given access to the agents' internal knowledge and gold fact supervision. On the other hand, models using gold question decomposition supervision can indeed solve CommaQA to a high accuracy (over 96\textbackslash\% exact match) by learning to utilize the agents. Even these additional supervision models, however, do not solve our compositional generalization test set. Finally the end-goal of learning to solve complex tasks by communicating with existing agents \textbackslash emph\{without relying on any additional supervision\} remains unsolved and we hope CommaQA serves as a novel benchmark to enable the development of such systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/VI7PZZ6V/Khot et al. - 2021 - Learning to Solve Complex Tasks by Talking to Agen.pdf;/home/xav/Zotero/storage/4ST8F9TL/2110.html}
}

@article{khotTextModularNetworks2021,
  title = {Text {{Modular Networks}}: {{Learning}} to {{Decompose Tasks}} in the {{Language}} of {{Existing Models}}},
  shorttitle = {Text {{Modular Networks}}},
  author = {Khot, Tushar and Khashabi, Daniel and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
  year = {2021},
  month = apr,
  journal = {arXiv:2009.00751 [cs]},
  eprint = {2009.00751},
  primaryclass = {cs},
  urldate = {2022-03-28},
  abstract = {We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model's reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/K4QB5H2W/Khot et al. - 2021 - Text Modular Networks Learning to Decompose Tasks.pdf;/home/xav/Zotero/storage/R8IQWTY8/2009.html}
}

@misc{kimDeepStoryVideoStory2017,
  title = {{{DeepStory}}: {{Video Story QA}} by {{Deep Embedded Memory Networks}}},
  shorttitle = {{{DeepStory}}},
  author = {Kim, Kyung-Min and Heo, Min-Oh and Choi, Seong-Ho and Zhang, Byoung-Tak},
  year = {2017},
  month = jul,
  number = {arXiv:1707.00836},
  eprint = {arXiv:1707.00836},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1707.00836},
  urldate = {2023-01-24},
  abstract = {Question-answering (QA) on video contents is a significant challenge for achieving human-level intelligence as it involves both vision and language in real-world settings. Here we demonstrate the possibility of an AI agent performing video story QA by learning from a large amount of cartoon videos. We develop a video-story learning model, i.e. Deep Embedded Memory Networks (DEMN), to reconstruct stories from a joint scene-dialogue video stream using a latent embedding space of observed data. The video stories are stored in a long-term memory component. For a given question, an LSTM-based attention model uses the long-term memory to recall the best question-story-answer triplet by focusing on specific words containing key information. We trained the DEMN on a novel QA dataset of children's cartoon video series, Pororo. The dataset contains 16,066 scene-dialogue pairs of 20.5-hour videos, 27,328 fine-grained sentences for scene description, and 8,913 story-related QA pairs. Our experimental results show that the DEMN outperforms other QA models. This is mainly due to 1) the reconstruction of video stories in a scene-dialogue combined form that utilize the latent embedding and 2) attention. DEMN also achieved state-of-the-art results on the MovieQA benchmark.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/3Q3LNAXV/Kim et al. - 2017 - DeepStory Video Story QA by Deep Embedded Memory .pdf;/home/xav/Zotero/storage/QJ42YSH3/1707.html}
}

@article{kimDonutDocumentUnderstanding2021,
  title = {Donut: {{Document Understanding Transformer}} without {{OCR}}},
  shorttitle = {Donut},
  author = {Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.15664 [cs]},
  eprint = {2111.15664},
  primaryclass = {cs},
  urldate = {2022-05-19},
  abstract = {Understanding document images (e.g., invoices) has been an important research topic and has many applications in document processing automation. Through the latest advances in deep learning-based Optical Character Recognition (OCR), current Visual Document Understanding (VDU) systems have come to be designed based on OCR. Although such OCR-based approach promise reasonable performance, they suffer from critical problems induced by the OCR, e.g., (1) expensive computational costs and (2) performance degradation due to the OCR error propagation. In this paper, we propose a novel VDU model that is end-to-end trainable without underpinning OCR framework. To this end, we propose a new task and a synthetic document image generator to pre-train the model to mitigate the dependencies on large-scale real document images. Our approach achieves state-of-the-art performance on various document understanding tasks in public benchmark datasets and private industrial service datasets. Through extensive experiments and analysis, we demonstrate the effectiveness of the proposed model especially with consideration for a real-world application.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ZP75QZUE/Kim et al. - 2021 - Donut Document Understanding Transformer without .pdf}
}

@misc{kimImprovingNeuralQuestion2018,
  title = {Improving {{Neural Question Generation}} Using {{Answer Separation}}},
  author = {Kim, Yanghoon and Lee, Hwanhee and Shin, Joongbo and Jung, Kyomin},
  year = {2018},
  month = nov,
  number = {arXiv:1809.02393},
  eprint = {arXiv:1809.02393},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1809.02393},
  urldate = {2023-01-03},
  abstract = {Neural question generation (NQG) is the task of generating a question from a given passage with deep neural networks. Previous NQG models suffer from a problem that a significant proportion of the generated questions include words in the question target, resulting in the generation of unintended questions. In this paper, we propose answer-separated seq2seq, which better utilizes the information from both the passage and the target answer. By replacing the target answer in the original passage with a special token, our model learns to identify which interrogative word should be used. We also propose a new module termed keyword-net, which helps the model better capture the key information in the target answer and generate an appropriate question. Experimental results demonstrate that our answer separation method significantly reduces the number of improper questions which include answers. Consequently, our model significantly outperforms previous state-of-the-art NQG models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/home/xav/Zotero/storage/CMCBSMGJ/Kim et al. - 2018 - Improving Neural Question Generation using Answer .pdf;/home/xav/Zotero/storage/GUCN46L4/1809.html}
}

@inproceedings{kimPrivacypreservingTextEmbedding2022,
  title = {Toward {{Privacy-preserving Text Embedding Similarity}} with {{Homomorphic Encryption}}},
  booktitle = {{{FINNLP}}},
  author = {Kim, Donggyu and Lee, Garam and Oh, Sungwoo},
  year = {2022},
  urldate = {2023-02-08},
  abstract = {Text embedding is an essential component to build efficient natural language applications based on text similarities such as search engines and chatbots. Certain industries like finance and healthcare demand strict privacy-preserving conditions that user's data should not be exposed to any potential malicious users even including service providers. From a privacy standpoint, text embeddings seem impossible to be interpreted but there is still a privacy risk that they can be recovered to original texts through inversion attacks. To satisfy such privacy requirements, in this paper, we study a Homomorphic Encryption (HE) based text similarity inference. To validate our method, we perform extensive experiments on two vital text similarity tasks. Through text embedding inversion tests, we prove that the benchmark datasets are vulnerable to inversion attacks and another privacy preserving approach, d{$\chi$}-privacy, a relaxed version of Local Differential Privacy method fails to prevent them. We show that our approach preserves the performance of models compared to that the baseline has degradation up to 10\% of scores for the minimum security.},
  file = {/home/xav/Zotero/storage/5UERAJIZ/Kim et al. - 2022 - Toward Privacy-preserving Text Embedding Similarit.pdf}
}

@misc{kimRetrievalAugmentedGenerationRAG2021,
  title = {Retrieval-{{Augmented Generation}} ({{RAG}}): {{Control Your Model}}'s {{Knowledge}} And\ldots{} {{Hallucinations}}!},
  shorttitle = {Retrieval-{{Augmented Generation}} ({{RAG}})},
  author = {Kim, Chan Woo},
  year = {2021},
  month = dec,
  journal = {Explained Relentlessly: Deep Learning \& Natural Language Processing},
  urldate = {2022-05-04},
  abstract = {Large LMs are really good today. But they're known to hallucinate. Maybe we should let it cheat a little with Wikipedia.},
  langid = {english},
  file = {/home/xav/Zotero/storage/ELGUZZLB/retrieval-augmented-generation-rag-control-your-models-knowledge-and-hallucinations-ea3c6345a65.html}
}

@misc{kimSODAMillionscaleDialogue2022,
  title = {{{SODA}}: {{Million-scale Dialogue Distillation}} with {{Social Commonsense Contextualization}}},
  shorttitle = {{{SODA}}},
  author = {Kim, Hyunwoo and Hessel, Jack and Jiang, Liwei and Lu, Ximing and Yu, Youngjae and Zhou, Pei and Bras, Ronan Le and Alikhani, Malihe and Kim, Gunhee and Sap, Maarten and Choi, Yejin},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10465},
  eprint = {arXiv:2212.10465},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10465},
  urldate = {2022-12-21},
  abstract = {We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. Using SODA, we train COSMO: a generalizable conversation agent outperforming previous best-performing agents on both in- and out-of-domain datasets. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a pre-trained language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than prior human-authored datasets - e.g., DailyDialog (Li et al., 2017), BlendedSkillTalk (Smith et al., 2020). In addition, extensive evaluations show that COSMO is significantly more natural and consistent on unseen datasets than best-performing dialogue models - e.g., GODEL (Peng et al., 2022), BlenderBot (Roller et al., 2021), DialoGPT (Zhang et al., 2020). Furthermore, it is sometimes even preferred to the original human-written gold responses. We make our data, models, and code public.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/SPRQH2K6/Kim et al. - 2022 - SODA Million-scale Dialogue Distillation with Soc.pdf}
}

@article{kleinLearningAnswerLearning2019,
  title = {Learning to {{Answer}} by {{Learning}} to {{Ask}}: {{Getting}} the {{Best}} of {{GPT-2}} and {{BERT Worlds}}},
  shorttitle = {Learning to {{Answer}} by {{Learning}} to {{Ask}}},
  author = {Klein, Tassilo and Nabi, Moin},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.02365 [cs]},
  eprint = {1911.02365},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Automatic question generation aims at the generation of questions from a context, with the corresponding answers being sub-spans of the given passage. Whereas, most of the methods mostly rely on heuristic rules to generate questions, more recently also neural network approaches have been proposed. In this work, we propose a variant of the self-attention Transformer network architectures model to generate meaningful and diverse questions. To this end, we propose an easy to use model consisting of the conjunction of the Transformer decoder GPT-2 model with Transformer encoder BERT for the downstream task for question answering. The model is trained in an end-to-end fashion, where the language model is trained to produce a question-answer-aware input representation that facilitates to generate an answer focused question. Our result of neural question generation from text on the SQuAD 1.1 dataset suggests that our method can produce semantically correct and diverse questions. Additionally, we assessed the performance of our proposed method for the downstream task of question answering. The analysis shows that our proposed generation \& answering collaboration framework relatively improves both tasks and is particularly powerful in the semi-supervised setup. The results further suggest a robust and comparably lean pipeline facilitating question generation in the small-data regime.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/Q3885Z84/Klein et Nabi - 2019 - Learning to Answer by Learning to Ask Getting the.pdf;/home/xav/Zotero/storage/RGIA8X3R/1911.html}
}

@misc{kocielnikCanYouLabel2022,
  title = {Can {{You Label Less}} by {{Using Out-of-Domain Data}}? {{Active}} \& {{Transfer Learning}} with {{Few-shot Instructions}}},
  shorttitle = {Can {{You Label Less}} by {{Using Out-of-Domain Data}}?},
  author = {Kocielnik, Rafal and Kangaslahti, Sara and Prabhumoye, Shrimai and Hari, Meena and Alvarez, R. Michael and Anandkumar, Anima},
  year = {2022},
  month = nov,
  number = {arXiv:2211.11798},
  eprint = {arXiv:2211.11798},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.11798},
  urldate = {2023-01-04},
  abstract = {Labeling social-media data for custom dimensions of toxicity and social bias is challenging and labor-intensive. Existing transfer and active learning approaches meant to reduce annotation effort require fine-tuning, which suffers from over-fitting to noise and can cause domain shift with small sample sizes. In this work, we propose a novel Active Transfer Few-shot Instructions (ATF) approach which requires no fine-tuning. ATF leverages the internal linguistic knowledge of pre-trained language models (PLMs) to facilitate the transfer of information from existing pre-labeled datasets (source-domain task) with minimum labeling effort on unlabeled target data (target-domain task). Our strategy can yield positive transfer achieving a mean AUC gain of 10.5\% compared to no transfer with a large 22b parameter PLM. We further show that annotation of just a few target-domain samples via active learning can be beneficial for transfer, but the impact diminishes with more annotation effort (26\% drop in gain between 100 and 2000 annotated examples). Finally, we find that not all transfer scenarios yield a positive gain, which seems related to the PLMs initial performance on the target-domain task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/8MDB4KKT/Kocielnik et al. - 2022 - Can You Label Less by Using Out-of-Domain Data Ac.pdf}
}

@misc{kociskyNarrativeQAReadingComprehension2017,
  title = {The {{NarrativeQA Reading Comprehension Challenge}}},
  author = {Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  year = {2017},
  month = dec,
  number = {arXiv:1712.07040},
  eprint = {arXiv:1712.07040},
  publisher = {{arXiv}},
  urldate = {2023-01-23},
  abstract = {Reading comprehension (RC)---in contrast to information retrieval---requires integrating information and reasoning about events, entities, and their relations across a full document. Question answering is conventionally used to assess RC ability, in both artificial agents and children learning to read. However, existing RC datasets and tasks are dominated by questions that can be solved by selecting answers using superficial information (e.g., local context similarity or global term frequency); they thus fail to test for the essential integrative aspect of RC. To encourage progress on deeper comprehension of language, we present a new dataset and set of tasks in which the reader must answer questions about stories by reading entire books or movie scripts. These tasks are designed so that successfully answering their questions requires understanding the underlying narrative rather than relying on shallow pattern matching or salience. We show that although humans solve the tasks easily, standard RC models struggle on the tasks presented here. We provide an analysis of the dataset and the challenges it presents.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  file = {/home/xav/Zotero/storage/KTTFSXVC/Koƒçisk√Ω et al. - 2017 - The NarrativeQA Reading Comprehension Challenge.pdf}
}

@misc{kojimaLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Zero-Shot Reasoners}}},
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2023},
  month = jan,
  number = {arXiv:2205.11916},
  eprint = {arXiv:2205.11916},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11916},
  urldate = {2023-02-06},
  abstract = {Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding "Let's think step by step" before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7\% to 78.7\% and GSM8K from 10.4\% to 40.7\% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/X2TJA8TN/Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf;/home/xav/Zotero/storage/MRIBPAKM/2205.html}
}

@article{koloninCognitiveArchitectureDecisionMaking2022,
  title = {Cognitive {{Architecture}} for {{Decision-Making Based}} on {{Brain Principles Programming}}},
  author = {Kolonin, Anton and Kurpatov, Andrey and Molchanov, Artem},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.07919 [cs]},
  eprint = {2204.07919},
  primaryclass = {cs},
  urldate = {2022-05-05},
  abstract = {We describe a cognitive architecture intended to solve a wide range of problems based on the five identified principles of brain activity, with their implementation in three subsystems: logical-probabilistic inference, probabilistic formal concepts, and functional systems theory. Building an architecture involves the implementation of a task-driven approach that allows defining the target functions of applied applications as tasks formulated in terms of the operating environment corresponding to the task, expressed in the applied ontology. We provide a basic ontology for a number of practical applications as well as for the subject domain ontologies based upon it, describe the proposed architecture, and give possible examples of the execution of these applications in this architecture.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Engineering; Finance; and Science},
  file = {/home/xav/Zotero/storage/IMW6TZ4W/Kolonin et al. - 2022 - Cognitive Architecture for Decision-Making Based o.pdf;/home/xav/Zotero/storage/X3I37D8F/2204.html}
}

@misc{krishnaHurdlesProgressLongform2021,
  title = {Hurdles to {{Progress}} in {{Long-form Question Answering}}},
  author = {Krishna, Kalpesh and Roy, Aurko and Iyyer, Mohit},
  year = {2021},
  month = may,
  number = {arXiv:2103.06332},
  eprint = {arXiv:2103.06332},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.06332},
  urldate = {2023-01-16},
  abstract = {The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81\% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/CY49VD5X/Krishna et al. - 2021 - Hurdles to Progress in Long-form Question Answerin.pdf;/home/xav/Zotero/storage/X4XQ3VG3/2103.html}
}

@inproceedings{kubalUnifiedModelParaphrase2021,
  title = {Unified {{Model}} for {{Paraphrase Generation}} and {{Paraphrase Identification}}},
  author = {Kubal, Divesh R. and Palivela, Dr Hemant},
  year = {2021},
  doi = {10.20944/PREPRINTS202104.0630.V1},
  abstract = {A light-weight unified model is proposed which aims to solve the problems of Paraphrase Identification and generation by using carefully selected data-points and a fine-tuned T5 model. Paraphrase Generation is one of the most important and challenging tasks in the field of 1 Natural Language Generation. The paraphrasing techniques help to identify or to extract/generate 2 phrases/sentences conveying the similar meaning. The paraphrasing task can be bifurcated into 3 two sub-tasks namely, Paraphrase Identification (PI) and Paraphrase Generation (PG). Most of 4 the existing proposed state-of-the-art systems have the potential to solve only one problem at a 5 time. This paper proposes a light-weight unified model that can simultaneously classify whether 6 given pair of sentences are paraphrases of each other and the model can also generate multiple 7 paraphrases given an input sentence. Paraphrase Generation module aims to generate fluent and 8 semantically similar paraphrases and the Paraphrase Identification system aims to classify whether 9 sentences pair are paraphrases of each other or not. The proposed approach uses an amalgamation 10 of data sampling or data variety with a granular fine-tuned Text-To-Text Transfer Transformer (T5) 11 model. This paper proposes a unified approach which aims to solve the problems of Paraphrase 12 Identification and generation by using carefully selected data-points and a fine-tuned T5 model. 13 The highlight of this study is that the same light-weight model trained by keeping the objective of 14 Paraphrase Generation can also be used for solving the Paraphrase Identification task. Hence, the 15 proposed system is light-weight in terms of the model's size along with the data used to train the 16 model which facilitates the quick learning of the model without having to compromise with the 17 results. The proposed system is then evaluated against the popular evaluation metrics like BLEU 18 (BiLingual Evaluation Understudy):, ROUGE (Recall-Oriented Understudy for Gisting Evaluation), 19 METEOR, WER (Word Error Rate), and GLEU (Google-BLEU) for Paraphrase Generation and 20 classification metrics like accuracy, precision, recall and F1-score for Paraphrase Identification 21 system. The proposed model achieves state-of-the-art results on both the tasks of Paraphrase 22 Identification and paraphrase Generation. 23},
  file = {/home/xav/Zotero/storage/SRLPSEPT/Kubal et Palivela - 2021 - Unified Model for Paraphrase Generation and Paraph.pdf}
}

@misc{kumarUsingNaturalLanguage2022,
  title = {Using {{Natural Language}} and {{Program Abstractions}} to {{Instill Human Inductive Biases}} in {{Machines}}},
  author = {Kumar, Sreejan and Correa, Carlos G. and Dasgupta, Ishita and Marjieh, Raja and Hu, Michael Y. and Hawkins, Robert D. and Daw, Nathaniel D. and Cohen, Jonathan D. and Narasimhan, Karthik and Griffiths, Thomas L.},
  year = {2022},
  month = oct,
  number = {arXiv:2205.11558},
  eprint = {arXiv:2205.11558},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11558},
  urldate = {2022-11-27},
  abstract = {Strong inductive biases give humans the ability to quickly learn to perform a variety of tasks. Although meta-learning is a method to endow neural networks with useful inductive biases, agents trained by meta-learning may sometimes acquire very different strategies from humans. We show that co-training these agents on predicting representations from natural language task descriptions and programs induced to generate such tasks guides them toward more human-like inductive biases. Human-generated language descriptions and program induction models that add new learned primitives both contain abstract concepts that can compress description length. Co-training on these representations result in more human-like behavior in downstream meta-reinforcement learning agents than less abstract controls (synthetic language descriptions, program induction without learned primitives), suggesting that the abstraction supported by these representations is key.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/xav/Zotero/storage/YRJGLEV9/Kumar et al. - 2022 - Using Natural Language and Program Abstractions to.pdf}
}

@article{kwiatkowskiNaturalQuestionsBenchmark2019,
  title = {Natural {{Questions}}: {{A Benchmark}} for {{Question Answering Research}}},
  shorttitle = {Natural {{Questions}}},
  author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  year = {2019},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {452--466},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  doi = {10.1162/tacl_a_00276},
  urldate = {2023-01-23},
  abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
  annotation = {QID: Q86346554},
  file = {/home/xav/Zotero/storage/KN8VS9DS/Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answer.pdf}
}

@book{lafourcadeGamesPurposeGWAPS2015,
  title = {Games with a {{Purpose}} ({{GWAPS}})},
  author = {Lafourcade, Mathieu and Joubert, Alain and Brun, Nathalie Le},
  year = {2015},
  month = jun,
  publisher = {{John Wiley \& Sons}},
  abstract = {Human brains can be seen as knowledge processors in a distributed system. Each of them can achieve, conscious or not, a small part of a treatment too important to be done by one. These are also "hunter / gatherers" of knowledge. Provided that the number of contributors is large enough, the results are usually better quality than if they were the result of the activity of a single person, even if it is a domain expert. This type of activity is done via online games.},
  googlebooks = {R7kOCgAAQBAJ},
  isbn = {978-1-119-13632-3},
  langid = {english},
  keywords = {Computers / Computer Science,Computers / Networking / General,Psychology / Cognitive Psychology \& Cognition},
  file = {/home/xav/Zotero/storage/LSQMET6P/Lafourcade et al. - 2015 - Games with a Purpose (GWAPS).pdf}
}

@misc{lampleCrosslingualLanguageModel2019,
  title = {Cross-Lingual {{Language Model Pretraining}}},
  author = {Lample, Guillaume and Conneau, Alexis},
  year = {2019},
  month = jan,
  number = {arXiv:1901.07291},
  eprint = {arXiv:1901.07291},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.07291},
  urldate = {2023-02-02},
  abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/CUL4C4RX/Lample et Conneau - 2019 - Cross-lingual Language Model Pretraining.pdf;/home/xav/Zotero/storage/PKJSFLCF/1901.html}
}

@misc{lampleHyperTreeProofSearch2022,
  title = {{{HyperTree Proof Search}} for {{Neural Theorem Proving}}},
  author = {Lample, Guillaume and Lachaux, Marie-Anne and Lavril, Thibaut and Martinet, Xavier and Hayat, Amaury and Ebner, Gabriel and Rodriguez, Aur{\'e}lien and Lacroix, Timoth{\'e}e},
  year = {2022},
  month = may,
  number = {arXiv:2205.11491},
  eprint = {arXiv:2205.11491},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11491},
  urldate = {2022-08-23},
  abstract = {We propose an online training procedure for a transformer-based automated theorem prover. Our approach leverages a new search algorithm, HyperTree Proof Search (HTPS), inspired by the recent success of AlphaZero. Our model learns from previous proof searches through online training, allowing it to generalize to domains far from the training distribution. We report detailed ablations of our pipeline's main components by studying performance on three environments of increasing complexity. In particular, we show that with HTPS alone, a model trained on annotated proofs manages to prove 65.4\% of a held-out set of Metamath theorems, significantly outperforming the previous state of the art of 56.5\% by GPT-f. Online training on these unproved theorems increases accuracy to 82.6\%. With a similar computational budget, we improve the state of the art on the Lean-based miniF2F-curriculum dataset from 31\% to 42\% proving accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/EVSA8BIE/Lample et al. - 2022 - HyperTree Proof Search for Neural Theorem Proving.pdf}
}

@misc{lanALBERTLiteBERT2020,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self-supervised Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2020},
  month = feb,
  number = {arXiv:1909.11942},
  eprint = {arXiv:1909.11942},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/DLK575D3/Lan et al. - 2020 - ALBERT A Lite BERT for Self-supervised Learning o.pdf}
}

@misc{lanComplexKnowledgeBase2022,
  title = {Complex {{Knowledge Base Question Answering}}: {{A Survey}}},
  shorttitle = {Complex {{Knowledge Base Question Answering}}},
  author = {Lan, Yunshi and He, Gaole and Jiang, Jinhao and Jiang, Jing and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2022},
  month = nov,
  number = {arXiv:2108.06688},
  eprint = {arXiv:2108.06688},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Early studies mainly focused on answering simple questions over KBs and achieved great success. However, their performance on complex questions are still far from satisfaction. Therefore, in recent years, researchers propose a large number of novel methods, which looked into the challenges of answering complex questions. In this survey, we review recent advances on KBQA with the focus on solving complex questions, which usually contain multiple subjects, express compound relations, or involve numerical operations. In detail, we begin with introducing the complex KBQA task and relevant background. Then, we present two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. Specifically, we illustrate their procedures with flow designs and discuss their difference and similarity. Next, we summarize the challenges that these two categories of methods encounter when answering complex questions, and explicate advanced solutions as well as techniques used in existing work. After that, we discuss the potential impact of pre-trained language models (PLMs) on complex KBQA. To help readers catch up with SOTA methods, we also provide a comprehensive evaluation and resource about complex KBQA task. Finally, we conclude and discuss several promising directions related to complex KBQA for future research.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/H9GDD2QU/Lan et al. - 2022 - Complex Knowledge Base Question Answering A Surve.pdf}
}

@misc{LanguageModellingScale,
  title = {Language Modelling at Scale: {{Gopher}}, Ethical Considerations, and Retrieval},
  shorttitle = {Language Modelling at Scale},
  urldate = {2022-05-04},
  abstract = {Language, and its role in demonstrating and facilitating comprehension - or intelligence - is a fundamental part of being human. It gives people the ability to communicate thoughts and concepts, express ideas, create memories, and build mutual understanding. These are foundational parts of social intelligence. It's why our teams at DeepMind study aspects of language processing and communication, both in artificial agents and in humans.},
  howpublished = {https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval},
  langid = {english},
  file = {/home/xav/Zotero/storage/34WNYCKA/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval.html}
}

@article{lanSurveyComplexKnowledge2021,
  title = {A {{Survey}} on {{Complex Knowledge Base Question Answering}}: {{Methods}}, {{Challenges}} and {{Solutions}}},
  shorttitle = {A {{Survey}} on {{Complex Knowledge Base Question Answering}}},
  author = {Lan, Yunshi and He, Gaole and Jiang, Jinhao and Jiang, Jing and Zhao, Wayne Xin and Wen, Ji-Rong},
  year = {2021},
  month = may,
  journal = {arXiv:2105.11644 [cs]},
  eprint = {2105.11644},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Knowledge base question answering (KBQA) aims to answer a question over a knowledge base (KB). Recently, a large number of studies focus on semantically or syntactically complicated questions. In this paper, we elaborately summarize the typical challenges and solutions for complex KBQA. We begin with introducing the background about the KBQA task. Next, we present the two mainstream categories of methods for complex KBQA, namely semantic parsing-based (SP-based) methods and information retrieval-based (IR-based) methods. We then review the advanced methods comprehensively from the perspective of the two categories. Specifically, we explicate their solutions to the typical challenges. Finally, we conclude and discuss some promising directions for future research.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/JWPVZMFL/Lan et al. - 2021 - A Survey on Complex Knowledge Base Question Answer.pdf}
}

@article{laurentLearningFindProofs2022,
  title = {Learning to {{Find Proofs}} and {{Theorems}} by {{Learning}} to {{Refine Search Strategies}}},
  author = {Laurent, Jonathan and Platzer, A.},
  year = {2022},
  journal = {undefined},
  urldate = {2022-11-07},
  abstract = {This work proposes a new approach to automated theorem proving and deductive program synthesis where an AlphaZero-style agent is self-training to refine a high-level expert strategy expressed as a nondeterministic program. We propose a new approach to automated theorem proving and deductive program synthesis where an AlphaZero-style agent is self-training to refine a high-level expert strategy expressed as a nondeterministic program. An analogous teacher agent is self-training to generate tasks of suitable relevance and difficulty for the learner. This allows leveraging minimal amounts of domain knowledge to tackle problems for which training data is unavailable or hard to synthesize. We illustrate our approach on the problem of loop invariant synthesis for imperative programs and using neural networks to refine both the teacher and solver strategies.},
  langid = {english},
  file = {/home/xav/Zotero/storage/49WPBZBK/Laurent et Platzer - 2022 - Learning to Find Proofs and Theorems by Learning t.pdf;/home/xav/Zotero/storage/9ZHHU68I/50d24b9ae8e2ace2c7577b94f6ded83af7f2ec57.html}
}

@article{leeBPrefBenchmarkingPreferenceBased2021,
  title = {B-{{Pref}}: {{Benchmarking Preference-Based Reinforcement Learning}}},
  shorttitle = {B-{{Pref}}},
  author = {Lee, Kimin and Smith, Laura and Dragan, Anca and Abbeel, Pieter},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.03026 [cs]},
  eprint = {2111.03026},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/CCJ9UF78/Lee et al. - 2021 - B-Pref Benchmarking Preference-Based Reinforcemen.pdf;/home/xav/Zotero/storage/YCCVEDBI/2111.html}
}

@article{leeDesignProblemSolving2019,
  title = {Design of {{Problem Solving Primitives}} for {{Efficient Evidential Reasoning}}},
  author = {Lee, Gye Sung},
  year = {2019},
  month = aug,
  journal = {International Journal of Internet, Broadcasting and Communication},
  volume = {11},
  number = {3},
  pages = {49--58},
  doi = {10.7236/IJIBC.2019.11.3.49},
  urldate = {2022-11-21},
  abstract = {Efficient evidential reasoning is an important issue in the development of advanced knowledge based systems. Efficiency is closely related to the design of problems solving methods adopted in the system. The explicit modeling of problem-solving structures is suggested for efficient and effective reasoning. It is pointed out that the problem-solving method framework is often too coarse-grained and too abstract to specify the detailed design and implementation of a reasoning system. Therefore, as a key step in developing a new reasoning scheme based on properties of the problem, the problem-solving method framework is expanded by introducing finer grained problem-solving primitives and defining an overall control structure in terms of these primitives. Once the individual components of the control structure are defined in terms of problem solving primitives, the overall control algorithm for the reasoning system can be represented in terms of a finite state diagram.},
  langid = {english},
  file = {/home/xav/Zotero/storage/599GI5GC/Lee - 2019 - Design of Problem Solving Primitives for Efficient.pdf}
}

@misc{leeFactualityEnhancedLanguage2022,
  title = {Factuality {{Enhanced Language Models}} for {{Open-Ended Text Generation}}},
  author = {Lee, Nayeon and Ping, Wei and Xu, Peng and Patwary, Mostofa and Fung, Pascale and Shoeybi, Mohammad and Catanzaro, Bryan},
  year = {2022},
  month = oct,
  number = {arXiv:2206.04624},
  eprint = {arXiv:2206.04624},
  publisher = {{arXiv}},
  urldate = {2023-02-09},
  abstract = {Pretrained language models (LMs) are susceptible to generate text with nonfactual information. In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generation. We design the FACTUALITYPROMPTS test set and metrics to measure the factuality of LM generations. Based on that, we study the factual accuracy of LMs with parameter sizes ranging from 126M to 530B. Interestingly, we find that larger LMs are more factual than smaller ones, although a previous study suggests that larger LMs can be less truthful in terms of misconceptions. In addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ``uniform randomness'' introduced at every sampling step. We propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality. Furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., Wikipedia). We propose a factuality-enhanced training method that uses TOPICPREFIX for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/MR6MCXAZ/Lee et al. - 2022 - Factuality Enhanced Language Models for Open-Ended.pdf}
}

@article{leeFormNetStructuralEncoding2022,
  title = {{{FormNet}}: {{Structural Encoding}} beyond {{Sequential Modeling}} in {{Form Document Information Extraction}}},
  shorttitle = {{{FormNet}}},
  author = {Lee, Chen-Yu and Li, Chun-Liang and Dozat, Timothy and Perot, Vincent and Su, Guolong and Hua, Nan and Ainslie, Joshua and Wang, Renshen and Fujii, Yasuhisa and Pfister, Tomas},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.08411 [cs]},
  eprint = {2203.08411},
  primaryclass = {cs},
  urldate = {2022-05-17},
  abstract = {Sequence modeling has demonstrated state-ofthe-art performance on natural language and document understanding tasks. However, it is challenging to correctly serialize tokens in form-like documents in practice due to their variety of layout patterns. We propose FormNet, a structure-aware sequence model to mitigate the suboptimal serialization of forms. First, we design Rich Attention that leverages the spatial relationship between tokens in a form for more precise attention score calculation. Second, we construct Super-Tokens for each word by embedding representations from their neighboring tokens through graph convolutions. FormNet therefore explicitly recovers local syntactic information that may have been lost during serialization. In experiments, FormNet outperforms existing methods with a more compact model size and less pretraining data, establishing new state-of-the-art performance on CORD, FUNSD and Payment benchmarks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/5GBRQ27N/Lee et al. - 2022 - FormNet Structural Encoding beyond Sequential Mod.pdf}
}

@inproceedings{leeGenerativeRetrievalLong2022,
  title = {Generative {{Retrieval}} for {{Long Sequences}}},
  author = {Lee, Hyunji and Yang, Sohee and Oh, Hanseok and Seo, Minjoon},
  year = {2022},
  abstract = {Text retrieval is often formulated as mapping the query and the target items (e.g., passages) to the same vector space and finding the item whose embedding is closest to that of the query. In this paper, we explore a generative approach as an alternative, where we use an encoder-decoder model to memorize the target corpus in a generative manner and then finetune it on query-to-passage generation. As GENRE (Cao et al., 2021) has shown that entities can be retrieved in a generative way, our work can be considered as its generalization to longer text. We show that it consistently achieves comparable performance to traditional bi-encoder retrieval on diverse datasets and is especially strong at retrieving highly structured items, such as reasoning chains and graph relations, while demonstrat-ing superior GPU memory and time complexity. We also conjecture that generative retrieval is complementary to traditional retrieval, as we find that an ensemble of both outperforms homogeneous ensembles. time complexity from O ( h ) to O ( 1 ) (dictionary table of (b)). GRLS is also modified to be applied to various retrieval tasks, including the retrieval of lengthy sequences and multi-step retrieval. To let the model learn in advance what information would be at the end of the sequence to generate, GRLS uses retrieval corpus memorization before training on target retrieval tasks.}
}

@misc{leeKaggleDBQARealisticEvaluation2021,
  title = {{{KaggleDBQA}}: {{Realistic Evaluation}} of {{Text-to-SQL Parsers}}},
  shorttitle = {{{KaggleDBQA}}},
  author = {Lee, Chia-Hsuan and Polozov, Oleksandr and Richardson, Matthew},
  year = {2021},
  month = jun,
  number = {arXiv:2106.11455},
  eprint = {arXiv:2106.11455},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.11455},
  urldate = {2023-01-16},
  abstract = {The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains. Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases. In this work, we examine the challenges that still prevent these techniques from practical deployment. First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions. Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings. Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge. We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2\%, doubling their performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Programming Languages},
  file = {/home/xav/Zotero/storage/K8JFRAWD/Lee et al. - 2021 - KaggleDBQA Realistic Evaluation of Text-to-SQL Pa.pdf;/home/xav/Zotero/storage/VPYEL96G/2106.html}
}

@inproceedings{leeLatentRetrievalWeakly2019,
  title = {Latent {{Retrieval}} for {{Weakly Supervised Open Domain Question Answering}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
  year = {2019},
  month = jul,
  pages = {6086--6096},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1612},
  urldate = {2023-01-23},
  abstract = {Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates. We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR. We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system. In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable. Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task. We evaluate on open versions of five QA datasets. On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient. On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.},
  file = {/home/xav/Zotero/storage/UPWNCDDH/Lee et al. - 2019 - Latent Retrieval for Weakly Supervised Open Domain.pdf}
}

@inproceedings{leePromptiverseScalableGeneration2022,
  title = {Promptiverse: {{Scalable Generation}} of {{Scaffolding Prompts Through Human-AI Hybrid Knowledge Graph Annotation}}},
  shorttitle = {Promptiverse},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lee, Yoonjoo and Chung, John Joon Young and Kim, Tae Soo and Song, Jean Y and Kim, Juho},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--18},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3502087},
  urldate = {2023-02-07},
  abstract = {Online learners are hugely diverse with varying prior knowledge, but most instructional videos online are created to be one-size-fits-all. Thus, learners may struggle to understand the content by only watching the videos. Providing scaffolding prompts can help learners overcome these struggles through questions and hints that relate different concepts in the videos and elicit meaningful learning. However, serving diverse learners would require a spectrum of scaffolding prompts, which incurs high authoring effort. In this work, we introduce Promptiverse, an approach for generating diverse, multi-turn scaffolding prompts at scale, powered by numerous traversal paths over knowledge graphs. To facilitate the construction of the knowledge graphs, we propose a hybrid human-AI annotation tool, Grannotate. In our study (N=24), participants produced 40 times more on-par quality prompts with higher diversity, through Promptiverse and Grannotate, compared to hand-designed prompts. Promptiverse presents a model for creating diverse and adaptive learning experiences online.},
  isbn = {978-1-4503-9157-3},
  keywords = {human-AI hybrid annotation,knowledge graph,Scaffolding prompt},
  file = {/home/xav/Zotero/storage/65YBYIV6/Lee et al. - 2022 - Promptiverse Scalable Generation of Scaffolding P.pdf}
}

@misc{leeYouOnlyNeed2021,
  title = {You {{Only Need One Model}} for {{Open-domain Question Answering}}},
  author = {Lee, Haejun and Kedia, Akhil and Lee, Jongwon and Paranjape, Ashwin and Manning, Christopher D. and Woo, Kyoung-Gu},
  year = {2021},
  month = dec,
  number = {arXiv:2112.07381},
  eprint = {arXiv:2112.07381},
  publisher = {{arXiv}},
  urldate = {2022-06-25},
  abstract = {Recent works for Open-domain Question Answering refer to an external knowledge base using a retriever model, optionally rerank the passages with a separate reranker model and generate an answer using an another reader model. Despite performing related tasks, the models have separate parameters and are weakly-coupled during training. In this work, we propose casting the retriever and the reranker as hard-attention mechanisms applied sequentially within the transformer architecture and feeding the resulting computed representations to the reader. In this singular model architecture the hidden representations are progressively refined from the retriever to the reranker to the reader, which is more efficient use of model capacity and also leads to better gradient flow when we train it in an end-to-end manner. We also propose a pretraining methodology to effectively train this architecture. We evaluate our model on Natural Questions and TriviaQA open datasets and for a fixed parameter budget, our model outperforms the previous state-of-the-art model by 1.0 and 0.7 exact match scores.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/VQB6STLC/Lee et al. - 2021 - You Only Need One Model for Open-domain Question A.pdf}
}

@misc{lehmanDoesBERTPretrained2021,
  title = {Does {{BERT Pretrained}} on {{Clinical Notes Reveal Sensitive Data}}?},
  author = {Lehman, Eric and Jain, Sarthak and Pichotta, Karl and Goldberg, Yoav and Wallace, Byron C.},
  year = {2021},
  month = apr,
  number = {arXiv:2104.07762},
  eprint = {arXiv:2104.07762},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.07762},
  urldate = {2022-07-19},
  abstract = {Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated "attacks" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing\_patient\_data\_release},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/XWYLAWFD/Lehman et al. - 2021 - Does BERT Pretrained on Clinical Notes Reveal Sens.pdf;/home/xav/Zotero/storage/VWIZENJ9/2104.html}
}

@misc{leikeScalableAgentAlignment2018,
  title = {Scalable Agent Alignment via Reward Modeling: A Research Direction},
  shorttitle = {Scalable Agent Alignment via Reward Modeling},
  author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  year = {2018},
  month = nov,
  number = {arXiv:1811.07871},
  eprint = {arXiv:1811.07871},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.07871},
  urldate = {2023-02-08},
  abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/PZETY78C/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf;/home/xav/Zotero/storage/4MQBYLNR/1811.html}
}

@misc{leiterExplainableEvaluationMetrics2022,
  title = {Towards {{Explainable Evaluation Metrics}} for {{Natural Language Generation}}},
  author = {Leiter, Christoph and Lertvittayakumjorn, Piyawat and Fomicheva, Marina and Zhao, Wei and Gao, Yang and Eger, Steffen},
  year = {2022},
  month = mar,
  number = {arXiv:2203.11131},
  eprint = {arXiv:2203.11131},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.11131},
  urldate = {2022-12-16},
  abstract = {Unlike classical lexical overlap metrics such as BLEU, most current evaluation metrics (such as BERTScore or MoverScore) are based on black-box language models such as BERT or XLM-R. They often achieve strong correlations with human judgments, but recent research indicates that the lower-quality classical metrics remain dominant, one of the potential reasons being that their decision processes are transparent. To foster more widespread acceptance of the novel high-quality metrics, explainability thus becomes crucial. In this concept paper, we identify key properties and propose key goals of explainable machine translation evaluation metrics. We also provide a synthesizing overview over recent approaches for explainable machine translation metrics and discuss how they relate to those goals and properties. Further, we conduct own novel experiments, which (among others) find that current adversarial NLP techniques are unsuitable for automatically identifying limitations of high-quality black-box evaluation metrics, as they are not meaning-preserving. Finally, we provide a vision of future approaches to explainable evaluation metrics and their evaluation. We hope that our work can help catalyze and guide future research on explainable evaluation metrics and, mediately, also contribute to better and more transparent text generation systems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/CUBPYU8B/Leiter et al. - 2022 - Towards Explainable Evaluation Metrics for Natural.pdf;/home/xav/Zotero/storage/XKH78SW4/2203.html}
}

@misc{leiTVQALocalizedCompositional2019,
  title = {{{TVQA}}: {{Localized}}, {{Compositional Video Question Answering}}},
  shorttitle = {{{TVQA}}},
  author = {Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L.},
  year = {2019},
  month = may,
  number = {arXiv:1809.01696},
  eprint = {arXiv:1809.01696},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1809.01696},
  urldate = {2023-01-24},
  abstract = {Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at http://tvqa.cs.unc.edu.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/SH8RN68E/Lei et al. - 2019 - TVQA Localized, Compositional Video Question Answ.pdf;/home/xav/Zotero/storage/JMDBFGPC/1809.html}
}

@misc{lesterPowerScaleParameterEfficient2021,
  title = {The {{Power}} of {{Scale}} for {{Parameter-Efficient Prompt Tuning}}},
  author = {Lester, Brian and {Al-Rfou}, Rami and Constant, Noah},
  year = {2021},
  month = sep,
  number = {arXiv:2104.08691},
  eprint = {arXiv:2104.08691},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08691},
  urldate = {2023-02-08},
  abstract = {In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/QLHIJVXZ/Lester et al. - 2021 - The Power of Scale for Parameter-Efficient Prompt .pdf;/home/xav/Zotero/storage/HM3S6NEG/2104.html}
}

@misc{lewisBARTDenoisingSequencetoSequence2019,
  title = {{{BART}}: {{Denoising Sequence-to-Sequence Pre-training}} for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  shorttitle = {{{BART}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  year = {2019},
  month = oct,
  number = {arXiv:1910.13461},
  eprint = {arXiv:1910.13461},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.13461},
  urldate = {2023-02-17},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/XGV6388X/Lewis et al. - 2019 - BART Denoising Sequence-to-Sequence Pre-training .pdf;/home/xav/Zotero/storage/Z935W2CA/1910.html}
}

@misc{lewisMLQAEvaluatingCrosslingual2020,
  title = {{{MLQA}}: {{Evaluating Cross-lingual Extractive Question Answering}}},
  shorttitle = {{{MLQA}}},
  author = {Lewis, Patrick and O{\u g}uz, Barlas and Rinott, Ruty and Riedel, Sebastian and Schwenk, Holger},
  year = {2020},
  month = may,
  number = {arXiv:1910.07475},
  eprint = {arXiv:1910.07475},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.07475},
  urldate = {2023-03-07},
  abstract = {Question answering (QA) models have shown rapid progress enabled by the availability of large, high-quality benchmark datasets. Such annotated datasets are difficult and costly to collect, and rarely exist in languages other than English, making training QA systems in other languages challenging. An alternative to building large monolingual training datasets is to develop cross-lingual systems which can transfer to a target language without requiring training data in that language. In order to develop such systems, it is crucial to invest in high quality multilingual evaluation benchmarks to measure progress. We present MLQA, a multi-way aligned extractive QA evaluation benchmark intended to spur research in this area. MLQA contains QA instances in 7 languages, namely English, Arabic, German, Spanish, Hindi, Vietnamese and Simplified Chinese. It consists of over 12K QA instances in English and 5K in each other language, with each QA instance being parallel between 4 languages on average. MLQA is built using a novel alignment context strategy on Wikipedia articles, and serves as a cross-lingual extension to existing extractive QA datasets. We evaluate current state-of-the-art cross-lingual representations on MLQA, and also provide machine-translation-based baselines. In all cases, transfer results are shown to be significantly behind training-language performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/XZLIUAU7/Lewis et al. - 2020 - MLQA Evaluating Cross-lingual Extractive Question.pdf;/home/xav/Zotero/storage/4PU4GZUU/1910.html}
}

@book{lewisNaturalLanguageProcessing2022,
  title = {Natural {{Language Processing}} with {{Transformers}}\_ {{Building Language Applications}} with {{Hugging Face}}},
  author = {Lewis, Tunstall and Leandro, von Werra and Thomas, Wolf},
  year = {2022},
  file = {/home/xav/Zotero/storage/EZKSGXSK/Lewis et al. - Natural Language Processing with Transformers_ Bui.pdf}
}

@misc{lewkowyczSolvingQuantitativeReasoning2022,
  title = {Solving {{Quantitative Reasoning Problems}} with {{Language Models}}},
  author = {Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and {Gutman-Solo}, Theo and Wu, Yuhuai and Neyshabur, Behnam and {Gur-Ari}, Guy and Misra, Vedant},
  year = {2022},
  month = jun,
  number = {arXiv:2206.14858},
  eprint = {2206.14858},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-30},
  abstract = {Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva, a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/6QAL33Y9/Lewkowycz et al. - 2022 - Solving Quantitative Reasoning Problems with Langu.pdf}
}

@misc{liAdvanceMakingLanguage2022,
  title = {On the {{Advance}} of {{Making Language Models Better Reasoners}}},
  author = {Li, Yifei and Lin, Zeqi and Zhang, Shizhuo and Fu, Qiang and Chen, Bei and Lou, Jian-Guang and Chen, Weizhu},
  year = {2022},
  month = jun,
  number = {arXiv:2206.02336},
  eprint = {arXiv:2206.02336},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.02336},
  urldate = {2023-02-07},
  abstract = {Large language models such as GPT-3 and PaLM have shown remarkable performance in few-shot learning. However, they still struggle with reasoning tasks such as the arithmetic benchmark GSM8K. Recent advances deliberately guide the language model to generate a chain of reasoning steps before producing the final answer, successfully boosting the GSM8K benchmark from 17.9\% to 58.1\% in terms of problem solving rate. In this paper, we propose a new approach, DiVeRSe (Diverse Verifier on Reasoning Step), to further advance their reasoning capability. DiVeRSe first explores different prompts to enhance the diversity in reasoning paths. Second, DiVeRSe introduces a verifier to distinguish good answers from bad answers for a better weighted voting. Finally, DiVeRSe verifies the correctness of each single step rather than all the steps in a whole. We conduct extensive experiments using the latest language model code-davinci-002 and demonstrate that DiVeRSe can achieve new state-of-the-art performance on six out of eight reasoning benchmarks (e.g., GSM8K 74.4\% to 83.2\%), outperforming the PaLM model with 540B parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/XFYQIDMQ/Li et al. - 2022 - On the Advance of Making Language Models Better Re.pdf;/home/xav/Zotero/storage/V54IU42Z/2206.html}
}

@misc{liangCodePoliciesLanguage2022,
  title = {Code as {{Policies}}: {{Language Model Programs}} for {{Embodied Control}}},
  shorttitle = {Code as {{Policies}}},
  author = {Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  year = {2022},
  month = sep,
  number = {arXiv:2209.07753},
  eprint = {arXiv:2209.07753},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.07753},
  urldate = {2022-11-28},
  abstract = {Large language models (LLMs) trained on code completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g.,from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions ("faster") depending on context (i.e., behavioral commonsense). This paper presents code as policies: a robot-centric formalization of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8\% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Robotics},
  file = {/home/xav/Zotero/storage/ZZYDI8C3/Liang et al. - 2022 - Code as Policies Language Model Programs for Embo.pdf;/home/xav/Zotero/storage/TE6VLYWA/2209.html}
}

@misc{liangetal.HolisticEvaluationLanguage2022,
  title = {Holistic {{Evaluation}} of {{Language Models}}},
  author = {{Liang et al.}},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09110},
  eprint = {arXiv:2211.09110},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.09110},
  urldate = {2023-03-13},
  abstract = {Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5\% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9\% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0\%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/G3YIU5TB/Liang et al. - 2022 - Holistic Evaluation of Language Models.pdf;/home/xav/Zotero/storage/LJLPZB27/2211.html}
}

@misc{liangFoundationsRecentTrends2022,
  title = {Foundations and {{Recent Trends}} in {{Multimodal Machine Learning}}: {{Principles}}, {{Challenges}}, and {{Open Questions}}},
  shorttitle = {Foundations and {{Recent Trends}} in {{Multimodal Machine Learning}}},
  author = {Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe},
  year = {2022},
  month = sep,
  number = {arXiv:2209.03430},
  eprint = {arXiv:2209.03430},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.03430},
  urldate = {2023-01-29},
  abstract = {Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining two key principles of modality heterogeneity and interconnections that have driven subsequent innovations, and propose a taxonomy of 6 core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {/home/xav/Zotero/storage/GEK97NQG/Liang et al. - 2022 - Foundations and Recent Trends in Multimodal Machin.pdf;/home/xav/Zotero/storage/9RBVDA4R/2209.html}
}

@misc{liBranchTrainMergeEmbarrassinglyParallel2022,
  title = {Branch-{{Train-Merge}}: {{Embarrassingly Parallel Training}} of {{Expert Language Models}}},
  shorttitle = {Branch-{{Train-Merge}}},
  author = {Li, Margaret and Gururangan, Suchin and Dettmers, Tim and Lewis, Mike and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
  year = {2022},
  month = aug,
  number = {arXiv:2208.03306},
  eprint = {arXiv:2208.03306},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.03306},
  urldate = {2022-09-02},
  abstract = {We present Branch-Train-Merge (BTM), a communication-efficient algorithm for embarrassingly parallel training of large language models (LLMs). We show it is possible to independently train subparts of a new class of LLMs on different subsets of the data, eliminating the massive multi-node synchronization currently required to train LLMs. BTM learns a set of independent expert LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text. These ELMs can be added and removed to update data coverage, ensembled to generalize to new domains, or averaged to collapse back to a single LM for efficient inference. New ELMs are learned by branching from (mixtures of) ELMs in the current set, further training the parameters on data for the new domain, and then merging the resulting model back into the set for future use. Experiments show that BTM improves in- and out-of-domain perplexities as compared to GPT-style Transformer LMs, when controlling for training cost. Through extensive analysis, we show that these results are robust to different ELM initialization schemes, but require expert domain specialization; LM ensembles with random data splits do not perform well. We also present a study of scaling BTM into a new corpus of 64 domains (192B whitespace-separated tokens in total); the resulting LM (22.4B total parameters) performs as well as a Transformer LM trained with 2.5 times more compute. These gains grow with the number of domains, suggesting more aggressive parallelism could be used to efficiently train larger models in future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/Y6VFIL5A/Li et al. - 2022 - Branch-Train-Merge Embarrassingly Parallel Traini.pdf}
}

@misc{liCompetitionLevelCodeGeneration2022,
  title = {Competition-{{Level Code Generation}} with {{AlphaCode}}},
  author = {Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and Hubert, Thomas and Choy, Peter and {d'Autume}, Cyprien de Masson and Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and Gowal, Sven and Cherepanov, Alexey and Molloy, James and Mankowitz, Daniel J. and Robson, Esme Sutherland and Kohli, Pushmeet and {de Freitas}, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  year = {2022},
  month = feb,
  number = {arXiv:2203.07814},
  eprint = {arXiv:2203.07814},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.07814},
  urldate = {2022-12-08},
  abstract = {Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3\% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages},
  file = {/home/xav/Zotero/storage/QA4A34PD/Li et al. - 2022 - Competition-Level Code Generation with AlphaCode.pdf}
}

@misc{liComposingEnsemblesPretrained2022,
  title = {Composing {{Ensembles}} of {{Pre-trained Models}} via {{Iterative Consensus}}},
  author = {Li, Shuang and Du, Yilun and Tenenbaum, Joshua B. and Torralba, Antonio and Mordatch, Igor},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11522},
  eprint = {arXiv:2210.11522},
  publisher = {{arXiv}},
  urldate = {2022-11-28},
  abstract = {Large pre-trained models exhibit distinct and complementary capabilities dependent on the data they are trained on. Language models such as GPT-3 are capable of textual reasoning but cannot understand visual information, while vision models such as DALL-E can generate photorealistic photos but fail to understand complex language descriptions. In this work, we propose a unified framework for composing ensembles of different pre-trained models -- combining the strengths of each individual model to solve various multimodal problems in a zero-shot manner. We use pre-trained models as "generators" or "scorers" and compose them via closed-loop iterative consensus optimization. The generator constructs proposals and the scorers iteratively provide feedback to refine the generated result. Such closed-loop communication enables models to correct errors caused by other models, significantly boosting performance on downstream tasks, e.g. improving accuracy on grade school math problems by 7.5\%, without requiring any model finetuning. We demonstrate that consensus achieved by an ensemble of scorers outperforms the feedback of a single scorer, by leveraging the strengths of each expert model. Results show that the proposed method can be used as a general purpose framework for a wide range of zero-shot multimodal tasks, such as image generation, video question answering, mathematical reasoning, and robotic manipulation. Project page: https://energy-based-model.github.io/composing-pretrained-models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ZTPXWDY6/2210.11522.pdf}
}

@article{liDiTSelfsupervisedPretraining2022,
  title = {{{DiT}}: {{Self-supervised Pre-training}} for {{Document Image Transformer}}},
  shorttitle = {{{DiT}}},
  author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
  year = {2022},
  month = apr,
  journal = {arXiv:2203.02378 [cs]},
  eprint = {2203.02378},
  primaryclass = {cs},
  urldate = {2022-05-17},
  abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose DiT, a selfsupervised pre-trained Document Image Transformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \textrightarrow{} 92.69), document layout analysis (91.0 \textrightarrow{} 94.9), table detection (94.23 \textrightarrow{} 96.55) and text detection for OCR (93.07 \textrightarrow{} 94.29). The code and pre-trained models are publicly available at https://aka.ms/msdit.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/LBLTSKAY/Li et al. - 2022 - DiT Self-supervised Pre-training for Document Ima.pdf}
}

@misc{liExplanationRegenerationInformation2022,
  title = {Explanation {{Regeneration}} via {{Information Bottleneck}}},
  author = {Li, Qintong and Wu, Zhiyong and Kong, Lingpeng and Bi, Wei},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09603},
  eprint = {arXiv:2212.09603},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Due to the superior generative capacity of large pretrained language models, recent work built on prompt engineering enables explanation generation without specific training. However, explanation generated through single-pass prompting often lacks sufficiency and conciseness. To address this problem, we develop an information bottleneck method EIB to produce refined explanations that are sufficient and concise. Our approach regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained. Experiments on two out-of-domain tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/FAYTEB5K/Li et al. - 2022 - Explanation Regeneration via Information Bottlenec.pdf}
}

@misc{liExplanationsLargeLanguage2022,
  title = {Explanations from {{Large Language Models Make Small Reasoners Better}}},
  author = {Li, Shiyang and Chen, Jianshu and Shen, Yelong and Chen, Zhiyu and Zhang, Xinlu and Li, Zekun and Wang, Hong and Qian, Jing and Peng, Baolin and Mao, Yi and Chen, Wenhu and Yan, Xifeng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06726},
  eprint = {arXiv:2210.06726},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.06726},
  urldate = {2023-02-07},
  abstract = {Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5\% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/ZEMJ8CZM/Li et al. - 2022 - Explanations from Large Language Models Make Small.pdf;/home/xav/Zotero/storage/RI97IK3M/2210.html}
}

@book{liHumanintheLoopMethodDeveloping2020,
  title = {A {{Human-in-the-Loop Method}} for {{Developing Machine Learning Applications}}},
  author = {Li, Yang and Li, Menghan and Ren, Jianlong and Zuo, Chun and Ma, Jiajia and Kong, Weiyi},
  year = {2020},
  month = jan,
  doi = {10.1109/ICSAI48974.2019.9010163},
  abstract = {An intelligent system is usually built based on a machine-learning model trained over offline datasets. However, such a system is difficult to adapt to new patterns or new data in the online environment, i.e. offline model has relatively poor on-line generalization. Moreover, understanding and solving errors taking place in the online system is also hard because errors arise in offline training pipelines and could propagate. We propose a novel systematic method, including design, architecture, and implementation to mix people's experience and intelligence with a relatively low cost. The method includes three negative-feedback loops and one data loop, supporting iterative and incremental developing procedure. Based on the method, we implemented a general system architecture and conducted two case studies. The results illustrate the effectiveness of the method.},
  file = {/home/xav/Zotero/storage/TZYCBWPV/Li et al. - 2020 - A Human-in-the-Loop Method for Developing Machine .pdf}
}

@article{liLanguageModelingLatent2022,
  title = {Language {{Modeling}} with {{Latent Situations}}},
  author = {Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2212.10012},
  urldate = {2023-02-07},
  abstract = {Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in their inputs. We introduce SituationSupervision, a family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states. SituationSupervision has two components: an auxiliary situation modeling task that trains models to predict state representations in context, and a latent state inference procedure that imputes these states from partially annotated training data. SituationSupervision can be applied to both fine-tuning (by supervising LMs to encode state variables in their hidden representations) and prompting (by inducing LMs to interleave textual descriptions of entity states with output text). In both cases, SituationSupervision requires only a small number of state annotations to produce major coherence improvements (between 4-11\%), showing that standard LMs can be sample-efficiently trained to model not just language but the situations it describes.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/home/xav/Zotero/storage/URD2S45S/Li et al. - 2022 - Language Modeling with Latent Situations.pdf}
}

@misc{liLargeLanguageModels2022,
  title = {Large {{Language Models}} with {{Controllable Working Memory}}},
  author = {Li, Daliang and Rawat, Ankit Singh and Zaheer, Manzil and Wang, Xin and Lukasik, Michal and Veit, Andreas and Yu, Felix and Kumar, Sanjiv},
  year = {2022},
  month = nov,
  number = {arXiv:2211.05110},
  eprint = {arXiv:2211.05110},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.05110},
  urldate = {2023-02-09},
  abstract = {Large language models (LLMs) have led to a series of breakthroughs in natural language processing (NLP), owing to their excellent understanding and generation abilities. Remarkably, what further sets these models apart is the massive amounts of world knowledge they internalize during pretraining. While many downstream applications provide the model with an informational context to aid its performance on the underlying task, how the model's world knowledge interacts with the factual information presented in the context remains under explored. As a desirable behavior, an LLM should give precedence to the context whenever it contains task-relevant information that conflicts with the model's memorized knowledge. This enables model predictions to be grounded in the context, which can then be used to update or correct specific model predictions without frequent retraining. By contrast, when the context is irrelevant to the task, the model should ignore it and fall back on its internal knowledge. In this paper, we undertake a first joint study of the aforementioned two properties, namely controllability and robustness, in the context of LLMs. We demonstrate that state-of-the-art T5 and PaLM (both pretrained and finetuned) could exhibit poor controllability and robustness, which do not scale with increasing model size. As a solution, we propose a novel method - Knowledge Aware FineTuning (KAFT) - to strengthen both controllability and robustness by incorporating counterfactual and irrelevant contexts to standard supervised datasets. Our comprehensive evaluation showcases the utility of KAFT across model architectures and sizes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/QPCTTM83/Li et al. - 2022 - Large Language Models with Controllable Working Me.pdf;/home/xav/Zotero/storage/CRKQMXV4/2211.html}
}

@article{liLearningDiverseDocument2022,
  title = {Learning {{Diverse Document Representations}} with {{Deep Query Interactions}} for {{Dense Retrieval}}},
  author = {Li, Zehan and Yang, Nan and Wang, Liang and Wei, Furu},
  year = {2022},
  journal = {undefined},
  urldate = {2022-09-18},
  abstract = {A new dense retrieval model which learns diverse document representations with deep query interactions, which enables deep query-document interactions in document encoding and provides multi-faceted representations to better match different queries. In this paper, we propose a new dense retrieval model which learns diverse document representations with deep query interactions. Our model encodes each document with a set of generated pseudo-queries to get query-informed, multi-view document representations. It not only enjoys high inference efficiency like the vanilla dual-encoder models, but also enables deep query-document interactions in document encoding and provides multi-faceted representations to better match different queries. Experiments on several benchmarks demonstrate the effectiveness of the proposed method, out-performing strong dual encoder baselines. 1},
  langid = {english},
  file = {/home/xav/Zotero/storage/QKGPBMDG/Li et al. - 2022 - Learning Diverse Document Representations with Dee.pdf}
}

@misc{liMAQAMultimodalQA2023,
  title = {{{MAQA}}: {{A Multimodal QA Benchmark}} for {{Negation}}},
  shorttitle = {{{MAQA}}},
  author = {Li, Judith Yue and Jansen, Aren and Huang, Qingqing and Lee, Joonseok and Ganti, Ravi and Kuzmin, Dima},
  year = {2023},
  month = jan,
  number = {arXiv:2301.03238},
  eprint = {arXiv:2301.03238},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.03238},
  urldate = {2023-01-24},
  abstract = {Multimodal learning can benefit from the representation power of pretrained Large Language Models (LLMs). However, state-of-the-art transformer based LLMs often ignore negations in natural language and there is no existing benchmark to quantitatively evaluate whether multimodal transformers inherit this weakness. In this study, we present a new multimodal question answering (QA) benchmark adapted from labeled music videos in AudioSet (Gemmeke et al., 2017) with the goal of systematically evaluating if multimodal transformers can perform complex reasoning to recognize new concepts as negation of previously learned concepts. We show that with standard fine-tuning approach multimodal transformers are still incapable of correctly interpreting negation irrespective of model size. However, our experiments demonstrate that augmenting the original training task distributions with negated QA examples allow the model to reliably reason with negation. To do this, we describe a novel data generation procedure that prompts the 540B-parameter PaLM model to automatically generate negated QA examples as compositions of easily accessible video tags. The generated examples contain more natural linguistic patterns and the gains compared to template-based task augmentation approach are significant.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/xav/Zotero/storage/5XRNXLR9/Li et al. - 2023 - MAQA A Multimodal QA Benchmark for Negation.pdf;/home/xav/Zotero/storage/IZ36FWPE/2301.html}
}

@inproceedings{linBirdsHaveFour2020,
  title = {Birds Have Four Legs?! {{NumerSense}}: {{Probing Numerical Commonsense Knowledge}} of {{Pre-Trained Language Models}}},
  shorttitle = {Birds Have Four Legs?! {{NumerSense}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Lin, Bill Yuchen and Lee, Seyeon and Khanna, Rahul and Ren, Xiang},
  year = {2020},
  month = nov,
  pages = {6862--6868},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.557},
  urldate = {2023-02-01},
  abstract = {Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as ``neural knowledge bases'' via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06\% vs. 96.3\% in accuracy).},
  file = {/home/xav/Zotero/storage/N5CZQNZX/Lin et al. - 2020 - Birds have four legs‚Åà NumerSense Probing Numerica.pdf}
}

@misc{linPretrainedTransformersText2021,
  title = {Pretrained {{Transformers}} for {{Text Ranking}}: {{BERT}} and {{Beyond}}},
  shorttitle = {Pretrained {{Transformers}} for {{Text Ranking}}},
  author = {Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew},
  year = {2021},
  month = aug,
  number = {arXiv:2010.06467},
  eprint = {arXiv:2010.06467},
  publisher = {{arXiv}},
  urldate = {2023-01-23},
  abstract = {The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. For text ranking, transformer-based models produce high quality results across many domains, tasks, and settings.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/AEPIXBPA/Lin et al. - 2021 - Pretrained Transformers for Text Ranking BERT and.pdf}
}

@inproceedings{linPyseriniPythonToolkit2021,
  title = {Pyserini: {{A Python Toolkit}} for {{Reproducible Information Retrieval Research}} with {{Sparse}} and {{Dense Representations}}},
  shorttitle = {Pyserini},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Lin, Jimmy and Ma, Xueguang and Lin, Sheng-Chieh and Yang, Jheng-Hong and Pradeep, Ronak and Nogueira, Rodrigo},
  year = {2021},
  month = jul,
  series = {{{SIGIR}} '21},
  pages = {2356--2362},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3404835.3463238},
  urldate = {2022-08-07},
  abstract = {Pyserini is a Python toolkit for reproducible information retrieval research with sparse and dense representations. It aims to provide effective, reproducible, and easy-to-use first-stage retrieval in a multi-stage ranking architecture. Our toolkit is self-contained as a standard Python package and comes with queries, relevance judgments, pre-built indexes, and evaluation scripts for many commonly used IR test collections. We aim to support, out of the box, the entire research lifecycle of efforts aimed at improving ranking with modern neural approaches. In particular, Pyserini supports sparse retrieval (e.g., BM25 scoring using bag-of-words representations), dense retrieval (e.g., nearest-neighbor search on transformer-encoded representations), as well as hybrid retrieval that integrates both approaches. This paper provides an overview of toolkit features and presents empirical results that illustrate its effectiveness on two popular ranking tasks. Around this toolkit, our group has built a culture of reproducibility through shared norms and tools that enable rigorous automated testing.},
  isbn = {978-1-4503-8037-9},
  keywords = {first-stage retrieval,open-source search engine},
  file = {/home/xav/Zotero/storage/GN5M2ULH/Lin et al. - 2021 - Pyserini A Python Toolkit for Reproducible Inform.pdf}
}

@inproceedings{linReasoningParagraphEffects2019,
  title = {Reasoning {{Over Paragraph Effects}} in {{Situations}}},
  booktitle = {Proceedings of the 2nd {{Workshop}} on {{Machine Reading}} for {{Question Answering}}},
  author = {Lin, Kevin and Tafjord, Oyvind and Clark, Peter and Gardner, Matt},
  year = {2019},
  month = nov,
  pages = {58--62},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-5808},
  urldate = {2023-02-01},
  abstract = {A key component of successfully reading a passage of text is the ability to apply knowledge gained from the passage to a new situation. In order to facilitate progress on this kind of reading, we present ROPES, a challenging benchmark for reading comprehension targeting Reasoning Over Paragraph Effects in Situations. We target expository language describing causes and effects (e.g., ``animal pollinators increase efficiency of fertilization in flowers''), as they have clear implications for new situations. A system is presented a background passage containing at least one of these relations, a novel situation that uses this background, and questions that require reasoning about effects of the relationships in the background passage in the context of the situation. We collect background passages from science textbooks and Wikipedia that contain such phenomena, and ask crowd workers to author situations, questions, and answers, resulting in a 14,322 question dataset. We analyze the challenges of this task and evaluate the performance of state-of-the-art reading comprehension models. The best model performs only slightly better than randomly guessing an answer of the correct type, at 61.6\% F1, well below the human performance of 89.0\%.},
  file = {/home/xav/Zotero/storage/TE5FBTWM/Lin et al. - 2019 - Reasoning Over Paragraph Effects in Situations.pdf}
}

@misc{linTruthfulQAMeasuringHow2022,
  title = {{{TruthfulQA}}: {{Measuring How Models Mimic Human Falsehoods}}},
  shorttitle = {{{TruthfulQA}}},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  year = {2022},
  month = may,
  number = {arXiv:2109.07958},
  eprint = {arXiv:2109.07958},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.07958},
  urldate = {2023-01-08},
  abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/YRDD4HN6/Lin et al. - 2022 - TruthfulQA Measuring How Models Mimic Human False.pdf;/home/xav/Zotero/storage/7XD7SM65/2109.html}
}

@misc{lippingClothoAQACrowdsourcedDataset2022,
  title = {Clotho-{{AQA}}: {{A Crowdsourced Dataset}} for {{Audio Question Answering}}},
  shorttitle = {Clotho-{{AQA}}},
  author = {Lipping, Samuel and Sudarsanam, Parthasaarathy and Drossos, Konstantinos and Virtanen, Tuomas},
  year = {2022},
  month = jun,
  number = {arXiv:2204.09634},
  eprint = {arXiv:2204.09634},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.09634},
  urldate = {2023-01-24},
  abstract = {Audio question answering (AQA) is a multimodal translation task where a system analyzes an audio signal and a natural language question, to generate a desirable natural language answer. In this paper, we introduce Clotho-AQA, a dataset for Audio question answering consisting of 1991 audio files each between 15 to 30 seconds in duration selected from the Clotho dataset. For each audio file, we collect six different questions and corresponding answers by crowdsourcing using Amazon Mechanical Turk. The questions and answers are produced by different annotators. Out of the six questions for each audio, two questions each are designed to have 'yes' and 'no' as answers, while the remaining two questions have other single-word answers. For each question, we collect answers from three different annotators. We also present two baseline experiments to describe the usage of our dataset for the AQA task - an LSTM-based multimodal binary classifier for 'yes' or 'no' type answers and an LSTM-based multimodal multi-class classifier for 828 single-word answers. The binary classifier achieved an accuracy of 62.7\% and the multi-class classifier achieved a top-1 accuracy of 54.2\% and a top-5 accuracy of 93.7\%. Clotho-AQA dataset is freely available online at https://zenodo.org/record/6473207.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/xav/Zotero/storage/2KKTSH8J/Lipping et al. - 2022 - Clotho-AQA A Crowdsourced Dataset for Audio Quest.pdf;/home/xav/Zotero/storage/LBTCWBAP/2204.html}
}

@misc{liPretrainedLanguageModels2022,
  title = {Pretrained {{Language Models}} for {{Text Generation}}: {{A Survey}}},
  shorttitle = {Pretrained {{Language Models}} for {{Text Generation}}},
  author = {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2022},
  month = may,
  number = {arXiv:2201.05273},
  eprint = {arXiv:2201.05273},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Text Generation aims to produce plausible and readable text in a human language from input data. The resurgence of deep learning has greatly advanced this field, in particular, with the help of neural generation models based on pre-trained language models (PLMs). Text generation based on PLMs is viewed as a promising approach in both academia and industry. In this paper, we provide a survey on the utilization of PLMs in text generation. We begin with introducing three key aspects of applying PLMs to text generation: 1) how to encode the input into representations preserving input semantics which can be fused into PLMs; 2) how to design an effective PLM to serve as the generation model; and 3) how to effectively optimize PLMs given the reference text and to ensure that the generated texts satisfy special text properties. Then, we show the major challenges arisen in these aspects, as well as possible solutions for them. We also include a summary of various useful resources and typical text generation applications based on PLMs. Finally, we highlight the future research directions which will further improve these PLMs for text generation. This comprehensive survey is intended to help researchers interested in text generation problems to learn the core concepts, the main techniques and the latest developments in this area based on PLMs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/7KS7LGAW/Li et al. - 2022 - Pretrained Language Models for Text Generation A .pdf}
}

@article{liQuestionAwareMemoryNetwork2021,
  title = {Question-{{Aware Memory Network}} for {{Multi-hop Question Answering}} in {{Human-Robot Interaction}}},
  author = {Li, Xinmeng and Alazab, Mamoun and Li, Qian and Yu, Keping and Yin, Quanjun},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.13173 [cs]},
  eprint = {2104.13173},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Knowledge graph question answering is an important technology in intelligent human-robot interaction, which aims at automatically giving answer to human natural language question with the given knowledge graph. For the multi-relation question with higher variety and complexity, the tokens of the question have different priority for the triples selection in the reasoning steps. Most existing models take the question as a whole and ignore the priority information in it. To solve this problem, we propose question-aware memory network for multi-hop question answering, named QA2MN, to update the attention on question timely in the reasoning process. In addition, we incorporate graph context information into knowledge graph embedding model to increase the ability to represent entities and relations. We use it to initialize the QA2MN model and fine-tune it in the training process. We evaluate QA2MN on PathQuestion and WorldCup2014, two representative datasets for complex multi-hop question answering. The result demonstrates that QA2MN achieves state-of-the-art Hits@1 accuracy on the two datasets, which validates the effectiveness of our model.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/KJV6N3SJ/Li et al. - 2021 - Question-Aware Memory Network for Multi-hop Questi.pdf}
}

@misc{liSelfPromptingLargeLanguage2022,
  title = {Self-{{Prompting Large Language Models}} for {{Open-Domain QA}}},
  author = {Li, Junlong and Zhang, Zhuosheng and Zhao, Hai},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08635},
  eprint = {arXiv:2212.08635},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.08635},
  urldate = {2022-12-21},
  abstract = {Open-Domain Question Answering (ODQA) requires models to answer factoid questions with no context given. The common way for this task is to train models on a large-scale annotated dataset to retrieve related documents and generate answers based on these documents. In this paper, we show that the ODQA architecture can be dramatically simplified by treating Large Language Models (LLMs) as a knowledge corpus and propose a Self-Prompting framework for LLMs to perform ODQA so as to eliminate the need for training data and external knowledge corpus. Concretely, we firstly generate multiple pseudo QA pairs with background passages and one-sentence explanations for these QAs by prompting LLMs step by step and then leverage the generated QA pairs for in-context learning. Experimental results show our method surpasses previous state-of-the-art methods by +8.8 EM averagely on three widely-used ODQA datasets, and even achieves comparable performance with several retrieval-augmented fine-tuned models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/3RT8UDGH/Li et al. - 2022 - Self-Prompting Large Language Models for Open-Doma.pdf}
}

@misc{liskaStreamingQABenchmarkAdaptation2022,
  title = {{{StreamingQA}}: {{A Benchmark}} for {{Adaptation}} to {{New Knowledge}} over {{Time}} in {{Question Answering Models}}},
  shorttitle = {{{StreamingQA}}},
  author = {Li{\v s}ka, Adam and Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Gribovskaya, Elena and Terzi, Tayfun and Sezener, Eren and Agrawal, Devang and {d'Autume}, Cyprien de Masson and Scholtes, Tim and Zaheer, Manzil and Young, Susannah and {Gilsenan-McMahon}, Ellen and Austin, Sophia and Blunsom, Phil and Lazaridou, Angeliki},
  year = {2022},
  month = may,
  number = {arXiv:2205.11388},
  eprint = {arXiv:2205.11388},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11388},
  urldate = {2022-09-06},
  abstract = {Knowledge and language understanding of models evaluated through question answering (QA) has been usually studied on static snapshots of knowledge, like Wikipedia. However, our world is dynamic, evolves over time, and our models' knowledge becomes outdated. To study how semi-parametric QA models and their underlying parametric language models (LMs) adapt to evolving knowledge, we construct a new large-scale dataset, StreamingQA, with human written and generated questions asked on a given date, to be answered from 14 years of time-stamped news articles. We evaluate our models quarterly as they read new articles not seen in pre-training. We show that parametric models can be updated without full retraining, while avoiding catastrophic forgetting. For semi-parametric models, adding new articles into the search space allows for rapid adaptation, however, models with an outdated underlying LM under-perform those with a retrained LM. For questions about higher-frequency named entities, parametric updates are particularly beneficial. In our dynamic world, the StreamingQA dataset enables a more realistic evaluation of QA models, and our experiments highlight several promising directions for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/E7PDSY9V/Li≈°ka et al. - 2022 - StreamingQA A Benchmark for Adaptation to New Kno.pdf;/home/xav/Zotero/storage/4UEF2RP8/2205.html}
}

@article{liSurveyRetrievalAugmentedText2022,
  title = {A {{Survey}} on {{Retrieval-Augmented Text Generation}}},
  author = {Li, Huayang and Su, Yixuan and Cai, Deng and Wang, Yan and Liu, Lemao},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.01110 [cs]},
  eprint = {2202.01110},
  primaryclass = {cs},
  urldate = {2022-03-24},
  abstract = {Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/F3QUAYZQ/Li et al. - 2022 - A Survey on Retrieval-Augmented Text Generation.pdf}
}

@misc{liuAskingKnowledgeTraining2022,
  title = {Asking for {{Knowledge}}: {{Training RL Agents}} to {{Query External Knowledge Using Language}}},
  shorttitle = {Asking for {{Knowledge}}},
  author = {Liu, Iou-Jen and Yuan, Xingdi and C{\^o}t{\'e}, Marc-Alexandre and Oudeyer, Pierre-Yves and Schwing, Alexander G.},
  year = {2022},
  month = jul,
  number = {arXiv:2205.06111},
  eprint = {arXiv:2205.06111},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.06111},
  urldate = {2022-10-27},
  abstract = {To solve difficult tasks, humans ask questions to acquire knowledge from external sources. In contrast, classical reinforcement learning agents lack such an ability and often resort to exploratory behavior. This is exacerbated as few present-day environments support querying for knowledge. In order to study how agents can be taught to query external knowledge via language, we first introduce two new environments: the grid-world-based Q-BabyAI and the text-based Q-TextWorld. In addition to physical interactions, an agent can query an external knowledge source specialized for these environments to gather information. Second, we propose the "Asking for Knowledge" (AFK) agent, which learns to generate language commands to query for meaningful knowledge that helps solve the tasks. AFK leverages a non-parametric memory, a pointer mechanism and an episodic exploration bonus to tackle (1) irrelevant information, (2) a large query language space, (3) delayed reward for making meaningful queries. Extensive experiments demonstrate that the AFK agent outperforms recent baselines on the challenging Q-BabyAI and Q-TextWorld environments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/IYRUYKNW/Liu et al. - 2022 - Asking for Knowledge Training RL Agents to Query .pdf}
}

@misc{liuAutoregressiveStructuredPrediction2022,
  title = {Autoregressive {{Structured Prediction}} with {{Language Models}}},
  author = {Liu, Tianyu and Jiang, Yuchen and Monath, Nicholas and Cotterell, Ryan and Sachan, Mrinmaya},
  year = {2022},
  month = nov,
  number = {arXiv:2210.14698},
  eprint = {arXiv:2210.14698},
  publisher = {{arXiv}},
  urldate = {2023-02-14},
  abstract = {In recent years, NLP has moved towards the application of language models to a more diverse set of tasks. However, applying language models to structured prediction, e.g., predicting parse trees, taggings, and coreference chains, is not straightforward. Prior work on language model-based structured prediction typically flattens the target structure into a string to easily fit it into the language modeling framework. Such flattening limits the accessibility of structural information and can lead to inferior performance compared to approaches that overtly model the structure. In this work, we propose to construct a conditional language model over sequences of structure-building actions, rather than over strings in a way that makes it easier for the model to pick up on intra-structure dependencies. Our method sets the new state of the art on named entity recognition, end-to-end relation extraction, and coreference resolution.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/5HUYYUE7/Liu et al. - 2022 - Autoregressive Structured Prediction with Language.pdf}
}

@misc{liuAwesomeIncrementalLearning2023,
  title = {Awesome {{Incremental Learning}} / {{Lifelong}} Learning},
  author = {Liu, Xialei},
  year = {2023},
  month = mar,
  urldate = {2023-03-24},
  abstract = {Awesome Incremental Learning}
}

@misc{liuChainHindsightAligns2023,
  title = {Chain of {{Hindsight Aligns Language Models}} with {{Feedback}}},
  author = {Liu, Hao and Sferrazza, Carmelo and Abbeel, Pieter},
  year = {2023},
  month = mar,
  number = {arXiv:2302.02676},
  eprint = {arXiv:2302.02676},
  publisher = {{arXiv}},
  urldate = {2023-04-03},
  abstract = {Learning from human preferences is important for language models to match human needs and to align with human and social values. Prior works have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them inefficient in terms of data utilization and challenging to apply in general, or they depend on reinforcement learning, which often suffers of imperfect reward functions and relies on extremely challenging optimizations. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage of the language comprehension capabilities of language models. We condition the model on a sequence of model generations paired with feedback. By doing so, the model is trained to generate outputs based on feedback, while learning to identify and correct negative attributes or errors. Applying our method to large language models, we observed that Chain of Hindsight significantly surpasses previous methods in aligning language models with human preferences. We report significant improvements on summarization and dialogue tasks, with our approach markedly preferred in human evaluations.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/LGZWV5AD/Liu et al. - 2023 - Chain of Hindsight Aligns Language Models with Fee.pdf}
}

@misc{liuFewShotParameterEfficientFineTuning2022,
  title = {Few-{{Shot Parameter-Efficient Fine-Tuning}} Is {{Better}} and {{Cheaper}} than {{In-Context Learning}}},
  author = {Liu, Haokun and Tam, Derek and Muqeeth, Mohammed and Mohta, Jay and Huang, Tenghao and Bansal, Mohit and Raffel, Colin},
  year = {2022},
  month = may,
  number = {arXiv:2205.05638},
  eprint = {2205.05638},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-30},
  abstract = {Few-shot in-context learning (ICL) enables pre-trained language models to perform a previously-unseen task without any gradient-based training by feeding a small number of training examples as part of the input. ICL incurs substantial computational, memory, and storage costs because it involves processing all of the training examples every time a prediction is made. Parameter-efficient fine-tuning (e.g. adapter modules, prompt tuning, sparse update methods, etc.) offers an alternative paradigm where a small set of parameters are trained to enable a model to perform the new task. In this paper, we rigorously compare few-shot ICL and parameter-efficient fine-tuning and demonstrate that the latter offers better accuracy as well as dramatically lower computational costs. Along the way, we introduce a new parameter-efficient fine-tuning method called (IA)\$\^3\$ that scales activations by learned vectors, attaining stronger performance while only introducing a relatively tiny amount of new parameters. We also propose a simple recipe based on the T0 model called T-Few that can be applied to new tasks without task-specific tuning or modifications. We validate the effectiveness of T-Few on completely unseen tasks by applying it to the RAFT benchmark, attaining super-human performance for the first time and outperforming the state-of-the-art by 6\% absolute. All of the code used in our experiments is publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/A2FNEIXU/Liu et al. - 2022 - Few-Shot Parameter-Efficient Fine-Tuning is Better.pdf}
}

@misc{liuFourthParadigmModern,
  title = {The {{Fourth Paradigm}} of {{Modern Natural Language Processing Techniques}}},
  author = {Liu, Pengfei},
  urldate = {2022-12-21},
  abstract = {{$\blacksquare$} What is the ``Prompt''? {$\blacksquare$} What is the general workflow of prompt-based methods? {$\blacksquare$} What are the design considerations for prompt-based methods? {$\blacksquare$} What (unique) advantages could prompt learning bring to us? {$\blacksquare$} How does prompt-based research progress currently?},
  howpublished = {https://blender.cs.illinois.edu/course/fall22/lecture9.pdf},
  file = {/home/xav/Zotero/storage/XJYLK2R8/lecture9.pdf}
}

@misc{liuGeneratedKnowledgePrompting2022,
  title = {Generated {{Knowledge Prompting}} for {{Commonsense Reasoning}}},
  author = {Liu, Jiacheng and Liu, Alisa and Lu, Ximing and Welleck, Sean and West, Peter and Bras, Ronan Le and Choi, Yejin and Hajishirzi, Hannaneh},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08387},
  eprint = {arXiv:2110.08387},
  publisher = {{arXiv}},
  urldate = {2022-05-22},
  abstract = {It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning. Our code is available at \textbackslash url\{github.com/liujch1998/GKP\}},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/F2UGBRUW/Liu et al. - 2022 - Generated Knowledge Prompting for Commonsense Reas.pdf}
}

@misc{liuImprovingSummarizationFactual2022,
  title = {On {{Improving Summarization Factual Consistency}} from {{Natural Language Feedback}}},
  author = {Liu, Yixin and Deb, Budhaditya and Teruel, Milagro and Halfaker, Aaron and Radev, Dragomir and Awadallah, Ahmed H.},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09968},
  eprint = {arXiv:2212.09968},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.09968},
  urldate = {2022-12-28},
  abstract = {Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, for user preference alignment. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational feedback in natural language consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study two natural language generation tasks: 1) editing a summary using the human feedback, and 2) generating human feedback from the original summary. Using the two tasks, we further evaluate if models can automatically correct factual inconsistencies in generated summaries. We show that the human-edited summaries we collected are more factually consistent, and pre-trained language models can leverage our dataset to improve the factual consistency of original system-generated summaries in our proposed generation tasks. We make the DeFacto dataset publicly available at https://github.com/microsoft/DeFacto.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/SFGDZUFX/Liu et al. - 2022 - On Improving Summarization Factual Consistency fro.pdf}
}

@misc{liuLatePromptTuning2022,
  title = {Late {{Prompt Tuning}}: {{A Late Prompt Could Be Better Than Many Prompts}}},
  shorttitle = {Late {{Prompt Tuning}}},
  author = {Liu, Xiangyang and Sun, Tianxiang and Huang, Xuanjing and Qiu, Xipeng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.11292},
  eprint = {arXiv:2210.11292},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11292},
  urldate = {2023-02-02},
  abstract = {Prompt tuning is a parameter-efficient tuning (PETuning) method for utilizing pre-trained models (PTMs) that simply prepends a soft prompt to the input and only optimizes the prompt to adapt PTMs to downstream tasks. Although it is parameter- and deployment-efficient, its performance still lags behind other state-of-the-art PETuning methods. Besides, the training cost of prompt tuning is not significantly reduced due to the back-propagation through the entire model. Through empirical analyses, we shed some light on the lagging performance of prompt tuning and recognize a trade-off between the propagation distance from label signals to the inserted prompt and the influence of the prompt on model outputs. Further, we present Late Prompt Tuning (LPT) that inserts a late prompt into an intermediate layer of the PTM instead of the input layer or all layers. The late prompt is obtained by a neural prompt generator conditioned on the hidden states before the prompt insertion layer and therefore is instance-dependent. Through extensive experimental results across various tasks and PTMs, we show that LPT can achieve competitive performance to full model tuning and other PETuning methods under both full-data and few-shot scenarios while possessing faster training speed and lower memory cost.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/86JZVYRD/Liu et al. - 2022 - Late Prompt Tuning A Late Prompt Could Be Better .pdf;/home/xav/Zotero/storage/RZY5IKDV/2210.html}
}

@misc{liuMindEyeGrounded2022,
  title = {Mind's {{Eye}}: {{Grounded Language Model Reasoning}} through {{Simulation}}},
  shorttitle = {Mind's {{Eye}}},
  author = {Liu, Ruibo and Wei, Jason and Gu, Shixiang Shane and Wu, Te-Yen and Vosoughi, Soroush and Cui, Claire and Zhou, Denny and Dai, Andrew M.},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05359},
  eprint = {arXiv:2210.05359},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.05359},
  urldate = {2022-10-15},
  abstract = {Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world -- their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind's Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind's MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind's Eye can improve reasoning ability by a large margin (27.9\% zero-shot, and 46.0\% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind's Eye can obtain similar performance to models that are 100x larger. Finally, we confirm the robustness of Mind's Eye through ablation studies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/A3RZAL6Y/Liu et al. - 2022 - Mind's Eye Grounded Language Model Reasoning thro.pdf}
}

@inproceedings{liuMultiTaskDeepNeural2019,
  title = {Multi-{{Task Deep Neural Networks}} for {{Natural Language Understanding}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  year = {2019},
  month = jul,
  pages = {4487--4496},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1441},
  urldate = {2023-02-02},
  abstract = {In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7\% (2.2\% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.},
  file = {/home/xav/Zotero/storage/2IB3P8YI/Liu et al. - 2019 - Multi-Task Deep Neural Networks for Natural Langua.pdf}
}

@article{liuPretrainPromptPredict2021,
  title = {Pre-Train, {{Prompt}}, and {{Predict}}: {{A Systematic Survey}} of {{Prompting Methods}} in {{Natural Language Processing}}},
  shorttitle = {Pre-Train, {{Prompt}}, and {{Predict}}},
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.13586 [cs]},
  eprint = {2107.13586},
  primaryclass = {cs},
  urldate = {2022-04-23},
  abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {QID: Q109286554},
  file = {/home/xav/Zotero/storage/FB6KHDWE/Liu et al. - 2021 - Pre-train, Prompt, and Predict A Systematic Surve.pdf;/home/xav/Zotero/storage/NGTTAUKB/2107.html}
}

@misc{liuRainierReinforcedKnowledge2022,
  title = {Rainier: {{Reinforced Knowledge Introspector}} for {{Commonsense Question Answering}}},
  shorttitle = {Rainier},
  author = {Liu, Jiacheng and Hallinan, Skyler and Lu, Ximing and He, Pengfei and Welleck, Sean and Hajishirzi, Hannaneh and Choi, Yejin},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03078},
  eprint = {arXiv:2210.03078},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03078},
  urldate = {2023-02-07},
  abstract = {Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent. We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/9H4DHFGJ/Liu et al. - 2022 - Rainier Reinforced Knowledge Introspector for Com.pdf;/home/xav/Zotero/storage/BEDVF68J/2210.html}
}

@book{liuRepresentationLearningNatural2020,
  title = {Representation {{Learning}} for {{Natural Language Processing}}},
  author = {Liu, Zhiyuan and Lin, Yankai and Sun, Maosong},
  year = {2020},
  publisher = {{Springer Singapore}},
  address = {{Singapore}},
  doi = {10.1007/978-981-15-5573-2},
  urldate = {2022-05-09},
  isbn = {9789811555725 9789811555732},
  langid = {english},
  file = {/home/xav/Zotero/storage/UVNVQEH3/Liu et al. - 2020 - Representation Learning for Natural Language Proce.pdf}
}

@misc{liuRLETReinforcementLearning2022,
  title = {{{RLET}}: {{A Reinforcement Learning Based Approach}} for {{Explainable QA}} with {{Entailment Trees}}},
  shorttitle = {{{RLET}}},
  author = {Liu, Tengxiao and Guo, Qipeng and Hu, Xiangkun and Zhang, Yue and Qiu, Xipeng and Zhang, Zheng},
  year = {2022},
  month = oct,
  number = {arXiv:2210.17095},
  eprint = {arXiv:2210.17095},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.17095},
  urldate = {2023-02-08},
  abstract = {Interpreting the reasoning process from questions to answers poses a challenge in approaching explainable QA. A recently proposed structured reasoning format, entailment tree, manages to offer explicit logical deductions with entailment steps in a tree structure. To generate entailment trees, prior single pass sequence-to-sequence models lack visible internal decision probability, while stepwise approaches are supervised with extracted single step data and cannot model the tree as a whole. In this work, we propose RLET, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree. RLET iteratively performs single step reasoning with sentence selection and deduction generation modules, from which the training signal is accumulated across the tree with elaborately designed aligned reward function that is consistent with the evaluation. To the best of our knowledge, we are the first to introduce RL into the entailment tree generation task. Experiments on three settings of the EntailmentBank dataset demonstrate the strength of using RL framework.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/6RR8K6PC/Liu et al. - 2022 - RLET A Reinforcement Learning Based Approach for .pdf;/home/xav/Zotero/storage/ZFEQJIP9/2210.html}
}

@misc{liuRoBERTaRobustlyOptimized2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {arXiv:1907.11692},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.11692},
  urldate = {2023-02-10},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/CWY7Q6QG/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf;/home/xav/Zotero/storage/5VTXCE7M/1907.html}
}

@misc{liuSecondThoughtsAre2023,
  title = {Second {{Thoughts}} Are {{Best}}: {{Learning}} to {{Re-Align With Human Values}} from {{Text Edits}}},
  shorttitle = {Second {{Thoughts}} Are {{Best}}},
  author = {Liu, Ruibo and Jia, Chenyan and Zhang, Ge and Zhuang, Ziyu and Liu, Tony X. and Vosoughi, Soroush},
  year = {2023},
  month = jan,
  number = {arXiv:2301.00355},
  eprint = {arXiv:2301.00355},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.00355},
  urldate = {2023-01-08},
  abstract = {We present Second Thought, a new learning paradigm that enables language models (LMs) to re-align with human values. By modeling the chain-of-edits between value-unaligned and value-aligned text, with LM fine-tuning and additional refinement through reinforcement learning, Second Thought not only achieves superior performance in three value alignment benchmark datasets but also shows strong human-value transfer learning ability in few-shot scenarios. The generated editing steps also offer better interpretability and ease for interactive error correction. Extensive human evaluations further confirm its effectiveness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/home/xav/Zotero/storage/DK4746IS/Liu et al. - 2023 - Second Thoughts are Best Learning to Re-Align Wit.pdf}
}

@misc{liuSecondThoughtsAre2023a,
  title = {Second {{Thoughts}} Are {{Best}}: {{Learning}} to {{Re-Align With Human Values}} from {{Text Edits}}},
  shorttitle = {Second {{Thoughts}} Are {{Best}}},
  author = {Liu, Ruibo and Jia, Chenyan and Zhang, Ge and Zhuang, Ziyu and Liu, Tony X. and Vosoughi, Soroush},
  year = {2023},
  month = jan,
  number = {arXiv:2301.00355},
  eprint = {arXiv:2301.00355},
  publisher = {{arXiv}},
  urldate = {2023-04-03},
  abstract = {We present SECOND THOUGHTS, a new learning paradigm that enables language models (LMs) to re-align with human values. By modeling the chain-of-edits between value-unaligned and value-aligned text, with LM fine-tuning and additional refinement through reinforcement learning, SECOND THOUGHTS not only achieves superior performance in three value alignment benchmark datasets but also shows strong human-value transfer learning ability in few-shot scenarios. The generated editing steps also offer better interpretability and ease for interactive error correction. Extensive human evaluations further confirm its effectiveness.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/home/xav/Zotero/storage/XJTYEXGJ/Liu et al. - 2023 - Second Thoughts are Best Learning to Re-Align Wit.pdf}
}

@misc{liuTopictoEssayGenerationComprehensive2021,
  title = {Topic-to-{{Essay Generation}} with {{Comprehensive Knowledge Enhancement}}},
  author = {Liu, Zhiyue and Wang, Jiahai and Li, Zhenghong},
  year = {2021},
  month = jun,
  number = {arXiv:2106.15142},
  eprint = {arXiv:2106.15142},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.15142},
  urldate = {2022-12-31},
  abstract = {Generating high-quality and diverse essays with a set of topics is a challenging task in natural language generation. Since several given topics only provide limited source information, utilizing various topic-related knowledge is essential for improving essay generation performance. However, previous works cannot sufficiently use that knowledge to facilitate the generation procedure. This paper aims to improve essay generation by extracting information from both internal and external knowledge. Thus, a topic-to-essay generation model with comprehensive knowledge enhancement, named TEGKE, is proposed. For internal knowledge enhancement, both topics and related essays are fed to a teacher network as source information. Then, informative features would be obtained from the teacher network and transferred to a student network which only takes topics as input but provides comparable information compared with the teacher network. For external knowledge enhancement, a topic knowledge graph encoder is proposed. Unlike the previous works only using the nearest neighbors of topics in the commonsense base, our topic knowledge graph encoder could exploit more structural and semantic information of the commonsense knowledge graph to facilitate essay generation. Moreover, the adversarial training based on the Wasserstein distance is proposed to improve generation quality. Experimental results demonstrate that TEGKE could achieve state-of-the-art performance on both automatic and human evaluation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/84DFQ3ZA/Liu et al. - 2021 - Topic-to-Essay Generation with Comprehensive Knowl.pdf}
}

@inproceedings{liuWhatMakesGood2022,
  title = {What {{Makes Good In-Context Examples}} for {{GPT-3}}?},
  booktitle = {Proceedings of {{Deep Learning Inside Out}} ({{DeeLIO}} 2022): {{The}} 3rd {{Workshop}} on {{Knowledge Extraction}} and {{Integration}} for {{Deep Learning Architectures}}},
  author = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  year = {2022},
  month = may,
  pages = {100--114},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland and Online}},
  doi = {10.18653/v1/2022.deelio-1.10},
  urldate = {2023-02-07},
  abstract = {GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt. Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (44.3\% on the ToTTo dataset) and open-domain question answering (45.5\% on the NQ dataset).},
  file = {/home/xav/Zotero/storage/WP2L6MXN/Liu et al. - 2022 - What Makes Good In-Context Examples for GPT-3.pdf}
}

@misc{longpreFlanCollectionDesigning2023,
  title = {The {{Flan Collection}}: {{Designing Data}} and {{Methods}} for {{Effective Instruction Tuning}}},
  shorttitle = {The {{Flan Collection}}},
  author = {Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V. and Zoph, Barret and Wei, Jason and Roberts, Adam},
  year = {2023},
  month = feb,
  number = {arXiv:2301.13688},
  eprint = {arXiv:2301.13688},
  publisher = {{arXiv}},
  urldate = {2023-04-03},
  abstract = {We study the design decisions of publicly available instruction tuning methods, and break down the development of Flan 2022 (Chung et al., 2022). Through careful ablation studies on the Flan Collection of tasks and methods, we tease apart the effect of design decisions which enable Flan-T5 to outperform prior work by 3-17\%+ across evaluation settings. We find task balancing and enrichment techniques are overlooked but critical to effective instruction tuning, and in particular, training with mixed prompt settings (zero-shot, few-shot, and chain-of-thought) actually yields stronger (2\%+) performance in all settings. In further experiments, we show Flan-T5 requires less finetuning to converge higher and faster than T5 on single downstream tasks, motivating instruction-tuned models as more computationally-efficient starting checkpoints for new tasks. Finally, to accelerate research on instruction tuning, we make the Flan 2022 collection of datasets, templates, and methods publicly available at https://github.com/google-research/FLAN/tree/main/flan/v2.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/VQTX2S7G/Longpre et al. - 2023 - The Flan Collection Designing Data and Methods fo.pdf}
}

@misc{longpreMKQALinguisticallyDiverse2021,
  title = {{{MKQA}}: {{A Linguistically Diverse Benchmark}} for {{Multilingual Open Domain Question Answering}}},
  shorttitle = {{{MKQA}}},
  author = {Longpre, Shayne and Lu, Yi and Daiber, Joachim},
  year = {2021},
  month = aug,
  number = {arXiv:2007.15207},
  eprint = {arXiv:2007.15207},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.15207},
  urldate = {2023-03-07},
  abstract = {Progress in cross-lingual modeling depends on challenging, realistic, and diverse evaluation sets. We introduce Multilingual Knowledge Questions and Answers (MKQA), an open-domain question answering evaluation set comprising 10k question-answer pairs aligned across 26 typologically diverse languages (260k question-answer pairs in total). Answers are based on a heavily curated, language-independent data representation, making results comparable across languages and independent of language-specific passages. With 26 languages, this dataset supplies the widest range of languages to-date for evaluating question answering. We benchmark a variety of state-of-the-art methods and baselines for generative and extractive question answering, trained on Natural Questions, in zero shot and translation settings. Results indicate this dataset is challenging even in English, but especially in low-resource languages},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/HM23698T/Longpre et al. - 2021 - MKQA A Linguistically Diverse Benchmark for Multi.pdf;/home/xav/Zotero/storage/WCNKMJMY/2007.html}
}

@article{lopezfloresCollectiveIntelligenceSolve2015,
  title = {Collective {{Intelligence}} to Solve Creative Problems in Conceptual Design Phase},
  author = {Lopez Flores, Ren{\'e} and N{\'e}gny, St{\'e}phane and Belaud, Jean-Pierre and Le Lann, Jean-Marc},
  year = {2015},
  journal = {Procedia Engineering},
  volume = {131},
  pages = {850--860},
  publisher = {{Elsevier}},
  doi = {10.1016/j.proeng.2015.12.394},
  urldate = {2022-05-11},
  abstract = {In industry, there is an interest in the collective resolution of creative problems found on the phase of conceptual design. In this work we introduce an information-based software framework for collaboration in the problem resolution process. This framework proposes the implementation of techniques from the collective intelligence research field in combination with the systematic methods provided by TRIZ theory. Both approaches are centered in the human aspect of the innovation process, and are complementary. While collective intelligence focuses on the intelligence or behavior that emerges in collaborative work, the TRIZ theory is centered in the individual's capacity to solve problems. The framework's objective is to improve the individual creativity provided by TRIZ method and tools, with the value created by the collective contributions. This work aims to expand the horizon in the field of computer aided innovation (CAI), to the next evolutionary step called Open CAI 2.0.},
  keywords = {Collective intelligence,Computer Aided Innovation,Conceptual design,Innovation process,Problem resolution},
  file = {/home/xav/Zotero/storage/AJVMWJMW/Lopez Flores et al. - 2015 - Collective Intelligence to solve creative problems.pdf}
}

@misc{luDynamicPromptLearning2022,
  title = {Dynamic {{Prompt Learning}} via {{Policy Gradient}} for {{Semi-structured Mathematical Reasoning}}},
  author = {Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin},
  year = {2022},
  month = nov,
  number = {arXiv:2209.14610},
  eprint = {arXiv:2209.14610},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14610},
  urldate = {2023-02-07},
  abstract = {Mathematical reasoning, a core ability of human intelligence, presents unique challenges for machines in abstract thinking and logical reasoning. Recent large pre-trained language models such as GPT-3 have achieved remarkable progress on mathematical reasoning tasks written in text form, such as math word problems (MWP). However, it is unknown if the models can handle more complex problems that involve math reasoning over heterogeneous information, such as tabular data. To fill the gap, we present Tabular Math Word Problems (TabMWP), a new dataset containing 38,431 open-domain grade-level problems that require mathematical reasoning on both textual and tabular data. Each question in TabMWP is aligned with a tabular context, which is presented as an image, semi-structured text, and a structured table. There are two types of questions: free-text and multi-choice, and each problem is annotated with gold solutions to reveal the multi-step reasoning process. We evaluate different pre-trained models on TabMWP, including the GPT-3 model in a few-shot setting. As earlier studies suggest, since few-shot GPT-3 relies on the selection of in-context examples, its performance is unstable and can degrade to near chance. The unstable issue is more severe when handling complex problems like TabMWP. To mitigate this, we further propose a novel approach, PromptPG, which utilizes policy gradient to learn to select in-context examples from a small amount of training data and then constructs the corresponding prompt for the test example. Experimental results show that our method outperforms the best baseline by 5.31\% on the accuracy metric and reduces the prediction variance significantly compared to random selection, which verifies its effectiveness in the selection of in-context examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/J8DB82C4/Lu et al. - 2022 - Dynamic Prompt Learning via Policy Gradient for Se.pdf;/home/xav/Zotero/storage/TI8KUCYG/2209.html}
}

@misc{luLearnExplainMultimodal2022,
  title = {Learn to {{Explain}}: {{Multimodal Reasoning}} via {{Thought Chains}} for {{Science Question Answering}}},
  shorttitle = {Learn to {{Explain}}},
  author = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  year = {2022},
  month = oct,
  number = {arXiv:2209.09513},
  eprint = {arXiv:2209.09513},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.09513},
  urldate = {2023-02-07},
  abstract = {When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of \textasciitilde 21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20\% in few-shot GPT-3 and 3.99\% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96\%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40\% of the data. The data and code are available at https://scienceqa.github.io.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {/home/xav/Zotero/storage/6KMYWA95/Lu et al. - 2022 - Learn to Explain Multimodal Reasoning via Thought.pdf;/home/xav/Zotero/storage/PPIJP5DF/2209.html}
}

@article{luReACCRetrievalAugmentedCode2022,
  title = {{{ReACC}}: {{A Retrieval-Augmented Code Completion Framework}}},
  shorttitle = {{{ReACC}}},
  author = {Lu, Shuai and Duan, Nan and Han, Hojae and Guo, Daya and Hwang, Seung-won and Svyatkovskiy, Alexey},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2203.07722},
  abstract = {This work proposes a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval, and adopts a stagewise training approach that combines a source code retriever and an auto-regressive language model for programming language. Code completion, which aims to predict the following code token(s) according to the code context, can improve the productivity of software development. Recent work has proved that statistical language modeling with transformers can greatly improve the performance in the code completion task via learning from large-scale source code datasets. However, current approaches focus only on code context within the file or project, i.e. internal context. Our distinction is utilizing ``external'' context, inspired by human behaviors of copying from the related code snippets when writing code. Specifically, we propose a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval. We adopt a stagewise training approach that combines a source code retriever and an auto-regressive language model for programming language. We evaluate our approach in the code completion task in Python and Java programming languages, achieving a state-of-the-art performance on CodeXGLUE benchmark.},
  file = {/home/xav/Zotero/storage/DTH6Y958/Lu et al. - 2022 - ReACC A Retrieval-Augmented Code Completion Frame.pdf}
}

@misc{luSurveyDeepLearning2022,
  title = {A {{Survey}} of {{Deep Learning}} for {{Mathematical Reasoning}}},
  author = {Lu, Pan and Qiu, Liang and Yu, Wenhao and Welleck, Sean and Chang, Kai-Wei},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10535},
  eprint = {arXiv:2212.10535},
  publisher = {{arXiv}},
  urldate = {2023-01-05},
  abstract = {Mathematical reasoning is a fundamental aspect of human intelligence and is applicable in various fields, including science, engineering, finance, and everyday life. The development of artificial intelligence (AI) systems capable of solving math problems and proving theorems has garnered significant interest in the fields of machine learning and natural language processing. For example, mathematics serves as a testbed for aspects of reasoning that are challenging for powerful deep learning models, driving new algorithmic and modeling advances. On the other hand, recent advances in large-scale neural language models have opened up new benchmarks and opportunities to use deep learning for mathematical reasoning. In this survey paper, we review the key tasks, datasets, and methods at the intersection of mathematical reasoning and deep learning over the past decade. We also evaluate existing benchmarks and methods, and discuss future research directions in this domain.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/XN46N8ZZ/Lu et al. - 2022 - A Survey of Deep Learning for Mathematical Reasoni.pdf}
}

@misc{lyuFaithfulChainofThoughtReasoning2023,
  title = {Faithful {{Chain-of-Thought Reasoning}}},
  author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and {Callison-Burch}, Chris},
  year = {2023},
  month = feb,
  number = {arXiv:2301.13379},
  eprint = {arXiv:2301.13379},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.13379},
  urldate = {2023-02-07},
  abstract = {While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a faithful-by-construction framework that decomposes a reasoning task into two stages: Translation (Natural Language query \$\textbackslash rightarrow\$ symbolic reasoning chain) and Problem Solving (reasoning chain \$\textbackslash rightarrow\$ answer), using an LM and a deterministic solver respectively. We demonstrate the efficacy of our approach on 10 reasoning datasets from 4 diverse domains. It outperforms traditional CoT prompting on 9 out of the 10 datasets, with an average accuracy gain of 4.4 on Math Word Problems, 1.9 on Planning, 4.0 on Multi-hop Question Answering (QA), and 18.1 on Logical Inference, under greedy decoding. Together with self-consistency decoding, we achieve new state-of-the-art few-shot performance on 7 out of the 10 datasets, showing a strong synergy between faithfulness and accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/NHKCWCNH/Lyu et al. - 2023 - Faithful Chain-of-Thought Reasoning.pdf;/home/xav/Zotero/storage/GJQAZDXQ/2301.html}
}

@misc{maattaMathematicalReasoningAI2022,
  title = {Mathematical {{Reasoning With AI}}},
  author = {Maatta, Teemu},
  year = {2022},
  month = jun,
  journal = {Medium},
  urldate = {2022-08-23},
  abstract = {Mathematics is a challenge to AI research due to (a) infinite action space \& (b) lack of self-play. Yet, the AGI implies the ability for mathematical reasoning. Lean, Metamath, and Equations are environments used for proving formal mathematical theorems. Such tools offer high-level tactics to apply to the problem. The algorithm solves theorems by generating a set of tactics without human interaction. Let's review three techniques for automated neural theorem proving:     Expert iteration applied with GPT-f     HyperTree Proof Search with online training (Evariste)     Chain of thought-prompting applied with the PaLM model},
  howpublished = {https://pub.towardsai.net/mathematical-reasoning-with-ai-b6d7048b4617},
  langid = {english},
  file = {/home/xav/Zotero/storage/6MEPLIUC/Maatta - 2022 - Mathematical Reasoning With AI.pdf}
}

@misc{madaanLanguageModelsCode2022,
  title = {Language {{Models}} of {{Code}} Are {{Few-Shot Commonsense Learners}}},
  author = {Madaan, Aman and Zhou, Shuyan and Alon, Uri and Yang, Yiming and Neubig, Graham},
  year = {2022},
  month = dec,
  number = {arXiv:2210.07128},
  eprint = {arXiv:2210.07128},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.07128},
  urldate = {2023-02-07},
  abstract = {We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event -- or a reasoning-graph. To employ large language models (LMs) for this task, existing approaches ``serialize'' the output graph as a flat list of nodes and edges. Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all. We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (CODEX) outperforms natural-LMs that are fine-tuned on the target task (e.g., T5) and other strong LMs such as GPT-3 in the few-shot setting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/QGBJCDHS/Madaan et al. - 2022 - Language Models of Code are Few-Shot Commonsense L.pdf;/home/xav/Zotero/storage/2QE3JGGM/2210.html}
}

@article{madaanMemoryassistedPromptEditing2022,
  title = {Memory-Assisted Prompt Editing to Improve {{GPT-3}} after Deployment},
  author = {Madaan, Aman and Tandon, Niket and Clark, Peter and Yang, Yiming},
  year = {2022},
  month = mar,
  journal = {arXiv:2201.06009 [cs]},
  eprint = {2201.06009},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Large LMs such as GPT-3 are powerful, but can commit mistakes that are obvious to humans. For example, GPT-3 would mistakenly interpret "What word is similar to good?" to mean a homonym, while the user intended a synonym. Our goal is to effectively correct such errors via user interactions with the system but without retraining, which will be prohibitively costly. We pair GPT-3 with a growing memory of recorded cases where the model misunderstood the user's intents, along with user feedback for clarification. Such a memory allows our system to produce enhanced prompts for any new query based on the user feedback for error correction on similar cases in the past. On four tasks (two lexical tasks, two advanced ethical reasoning tasks), we show how a (simulated) user can interactively teach a deployed GPT-3, substantially increasing its accuracy over the queries with different kinds of misunderstandings by the GPT-3. Our approach is a step towards the low-cost utility enhancement for very large pre-trained LMs. All the code and data is available at https://github.com/madaan/memprompt.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/B4HUUXFF/Madaan et al. - 2022 - Memory-assisted prompt editing to improve GPT-3 af.pdf;/home/xav/Zotero/storage/8NIUSJQW/2201.html}
}

@misc{mahabadiPERFECTPromptfreeEfficient2022,
  title = {{{PERFECT}}: {{Prompt-free}} and {{Efficient Few-shot Learning}} with {{Language Models}}},
  shorttitle = {{{PERFECT}}},
  author = {Mahabadi, Rabeeh Karimi and Zettlemoyer, Luke and Henderson, James and Saeidi, Marzieh and Mathias, Lambert and Stoyanov, Veselin and Yazdani, Majid},
  year = {2022},
  month = apr,
  number = {arXiv:2204.01172},
  eprint = {arXiv:2204.01172},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.01172},
  urldate = {2022-12-07},
  abstract = {Current methods for few-shot fine-tuning of pretrained masked language models (PLMs) require carefully engineered prompts and verbalizers for each new task to convert examples into a cloze-format that the PLM can score. In this work, we propose PERFECT, a simple and efficient method for few-shot fine-tuning of PLMs without relying on any such handcrafting, which is highly effective given as few as 32 data points. PERFECT makes two key design choices: First, we show that manually engineered task prompts can be replaced with task-specific adapters that enable sample-efficient fine-tuning and reduce memory and storage costs by roughly factors of 5 and 100, respectively. Second, instead of using handcrafted verbalizers, we learn new multi-token label embeddings during fine-tuning, which are not tied to the model vocabulary and which allow us to avoid complex auto-regressive decoding. These embeddings are not only learnable from limited data but also enable nearly 100x faster training and inference. Experiments on a wide range of few-shot NLP tasks demonstrate that PERFECT, while being simple and efficient, also outperforms existing state-of-the-art few-shot learning methods. Our code is publicly available at https://github.com/facebookresearch/perfect.git.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/R58WEDXT/Mahabadi et al. - 2022 - PERFECT Prompt-free and Efficient Few-shot Learni.pdf;/home/xav/Zotero/storage/ZPLQPNVA/2204.html}
}

@article{mahajanIdentificationSemanticallySimilar2020,
  title = {Identification of {{Semantically Similar Sentences}} in {{Clinical Notes}}: {{Iterative Intermediate Training Using Multi-Task Learning}}},
  shorttitle = {Identification of {{Semantically Similar Sentences}} in {{Clinical Notes}}},
  author = {Mahajan, Diwakar and Poddar, Ananya and Liang, Jennifer J. and Lin, Yen-Ting and Prager, John M. and Suryanarayanan, Parthasarathy and Raghavan, Preethi and Tsou, Ching-Huei},
  year = {2020},
  month = nov,
  journal = {JMIR medical informatics},
  volume = {8},
  number = {11},
  pages = {e22508},
  issn = {2291-9694},
  doi = {10.2196/22508},
  abstract = {BACKGROUND: Although electronic health records (EHRs) have been widely adopted in health care, effective use of EHR data is often limited because of redundant information in clinical notes introduced by the use of templates and copy-paste during note generation. Thus, it is imperative to develop solutions that can condense information while retaining its value. A step in this direction is measuring the semantic similarity between clinical text snippets. To address this problem, we participated in the 2019 National NLP Clinical Challenges (n2c2)/Open Health Natural Language Processing Consortium (OHNLP) clinical semantic textual similarity (ClinicalSTS) shared task. OBJECTIVE: This study aims to improve the performance and robustness of semantic textual similarity in the clinical domain by leveraging manually labeled data from related tasks and contextualized embeddings from pretrained transformer-based language models. METHODS: The ClinicalSTS data set consists of 1642 pairs of deidentified clinical text snippets annotated in a continuous scale of 0-5, indicating degrees of semantic similarity. We developed an iterative intermediate training approach using multi-task learning (IIT-MTL), a multi-task training approach that employs iterative data set selection. We applied this process to bidirectional encoder representations from transformers on clinical text mining (ClinicalBERT), a pretrained domain-specific transformer-based language model, and fine-tuned the resulting model on the target ClinicalSTS task. We incrementally ensembled the output from applying IIT-MTL on ClinicalBERT with the output of other language models (bidirectional encoder representations from transformers for biomedical text mining [BioBERT], multi-task deep neural networks [MT-DNN], and robustly optimized BERT approach [RoBERTa]) and handcrafted features using regression-based learning algorithms. On the basis of these experiments, we adopted the top-performing configurations as our official submissions. RESULTS: Our system ranked first out of 87 submitted systems in the 2019 n2c2/OHNLP ClinicalSTS challenge, achieving state-of-the-art results with a Pearson correlation coefficient of 0.9010. This winning system was an ensembled model leveraging the output of IIT-MTL on ClinicalBERT with BioBERT, MT-DNN, and handcrafted medication features. CONCLUSIONS: This study demonstrates that IIT-MTL is an effective way to leverage annotated data from related tasks to improve performance on a target task with a limited data set. This contribution opens new avenues of exploration for optimized data set selection to generate more robust and universal contextual representations of text in the clinical domain.},
  langid = {english},
  pmcid = {PMC7732709},
  pmid = {33245284},
  keywords = {deep learning,electronic health records,multi-task learning,natural language processing,semantic textual similarity,transfer learning},
  annotation = {QID: Q103012103},
  file = {/home/xav/Zotero/storage/E8GNJS93/Mahajan et al. - 2020 - Identification of Semantically Similar Sentences i.pdf}
}

@misc{maharajDatasetExplorationModels2017,
  title = {A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question-Answering},
  author = {Maharaj, Tegan and Ballas, Nicolas and Rohrbach, Anna and Courville, Aaron and Pal, Christopher},
  year = {2017},
  month = feb,
  number = {arXiv:1611.07810},
  eprint = {arXiv:1611.07810},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.07810},
  urldate = {2023-01-24},
  abstract = {While deep convolutional neural networks frequently approach or exceed human-level performance at benchmark tasks involving static images, extending this success to moving images is not straightforward. Having models which can learn to understand video is of interest for many applications, including content recommendation, prediction, summarization, event/object detection and understanding human visual perception, but many domains lack sufficient data to explore and perfect video models. In order to address the need for a simple, quantitative benchmark for developing and understanding video, we present MovieFIB, a fill-in-the-blank question-answering dataset with over 300,000 examples, based on descriptive video annotations for the visually impaired. In addition to presenting statistics and a description of the dataset, we perform a detailed analysis of 5 different models' predictions, and compare these with human performance. We investigate the relative importance of language, static (2D) visual features, and moving (3D) visual features; the effects of increasing dataset size, the number of frames sampled; and of vocabulary size. We illustrate that: this task is not solvable by a language model alone; our model combining 2D and 3D visual information indeed provides the best result; all models perform significantly worse than human-level. We provide human evaluations for responses given by different models and find that accuracy on the MovieFIB evaluation corresponds well with human judgement. We suggest avenues for improving video models, and hope that the proposed dataset can be useful for measuring and encouraging progress in this very interesting field.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/7AWFGMA9/Maharaj et al. - 2017 - A dataset and exploration of models for understand.pdf;/home/xav/Zotero/storage/NGNV83D4/1611.html}
}

@misc{mahowaldDissociatingLanguageThought2023,
  title = {Dissociating Language and Thought in Large Language Models: A Cognitive Perspective},
  shorttitle = {Dissociating Language and Thought in Large Language Models},
  author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = {2023},
  month = jan,
  number = {arXiv:2301.06627},
  eprint = {arXiv:2301.06627},
  publisher = {{arXiv}},
  urldate = {2023-01-23},
  abstract = {Short abstract (100 words): Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their capabilities remain split. Here, we evaluate LLMs using a distinction between formal competence\textemdash knowledge of linguistic rules and patterns\textemdash and functional competence\textemdash understanding and using language in the world. We ground this distinction in human neuroscience, showing that these skills recruit different cognitive mechanisms. Although LLMs are close to mastering formal competence, they still fail at functional competence tasks, which often require drawing on non-linguistic capacities. In short, LLMs are good models of language but incomplete models of human thought.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/KUBEYNEY/Mahowald et al. - 2023 - Dissociating language and thought in large languag.pdf}
}

@inproceedings{malburgUsingSemanticWeb2020,
  title = {Using {{Semantic Web Services}} for {{AI-Based Research}} in {{Industry}} 4.0},
  booktitle = {Proceedings of the {{International Conference}} on {{Innovative Intelligent Industrial Production}} and {{Logistics}}},
  author = {Malburg, Lukas and Klein, Patrick and Bergmann, Ralph},
  year = {2020},
  eprint = {2007.03580},
  primaryclass = {cs},
  pages = {32--43},
  doi = {10.5220/0010135900320043},
  urldate = {2022-11-25},
  abstract = {The transition to Industry 4.0 requires smart manufacturing systems that are easily configurable and provide a high level of flexibility during manufacturing in order to achieve mass customization or to support cloud manufacturing. To realize this, Cyber-Physical Systems (CPSs) combined with Artificial Intelligence (AI) methods find their way into manufacturing shop floors. For using AI methods in the context of Industry 4.0, semantic web services are indispensable to provide a reasonable abstraction of the underlying manufacturing capabilities. In this paper, we present semantic web services for AI-based research in Industry 4.0. Therefore, we developed more than 300 semantic web services for a physical simulation factory based on Web Ontology Language for Web Services (OWL-S) and Web Service Modeling Ontology (WSMO) and linked them to an already existing domain ontology for intelligent manufacturing control. Suitable for the requirements of CPS environments, our pre- and postconditions are verified in near real-time by invoking other semantic web services in contrast to complex reasoning within the knowledge base. Finally, we evaluate our implementation by executing a cyber-physical workflow composed of semantic web services using a workflow management system.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/home/xav/Zotero/storage/P4PU25VK/Malburg et al. - 2020 - Using Semantic Web Services for AI-Based Research .pdf}
}

@inproceedings{maoCLEVRERHumansDescribingPhysical2022,
  title = {{{CLEVRER-Humans}}: {{Describing Physical}} and {{Causal Events}} the {{Human Way}}},
  shorttitle = {{{CLEVRER-Humans}}},
  booktitle = {Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems Datasets}} and {{Benchmarks Track}}},
  author = {Mao, Jiayuan and Yang, Xuelin and Zhang, Xikun and Goodman, Noah and Wu, Jiajun},
  year = {2022},
  month = oct,
  urldate = {2023-01-24},
  abstract = {Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of the causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting great challenges set forth by our benchmark.},
  langid = {english},
  file = {/home/xav/Zotero/storage/BG58CAWC/Mao et al. - 2022 - CLEVRER-Humans Describing Physical and Causal Eve.pdf}
}

@inproceedings{maoReaderGuidedPassageReranking2021,
  title = {Reader-{{Guided Passage Reranking}} for {{Open-Domain Question Answering}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Mao, Yuning and He, Pengcheng and Liu, Xiaodong and Shen, Yelong and Gao, Jianfeng and Han, Jiawei and Chen, Weizhu},
  year = {2021},
  month = aug,
  pages = {344--350},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.29},
  urldate = {2022-05-04},
  file = {/home/xav/Zotero/storage/NQFNCPAP/Mao et al. - 2021 - Reader-Guided Passage Reranking for Open-Domain Qu.pdf}
}

@inproceedings{martindaleIdentifyingFluentlyInadequate2019,
  title = {Identifying {{Fluently Inadequate Output}} in {{Neural}} and {{Statistical Machine Translation}}},
  booktitle = {Proceedings of {{Machine Translation Summit XVII}}: {{Research Track}}},
  author = {Martindale, Marianna and Carpuat, Marine and Duh, Kevin and McNamee, Paul},
  year = {2019},
  month = aug,
  pages = {233--243},
  publisher = {{European Association for Machine Translation}},
  address = {{Dublin, Ireland}},
  urldate = {2023-01-24},
  file = {/home/xav/Zotero/storage/L92XNGQ5/Martindale et al. - 2019 - Identifying Fluently Inadequate Output in Neural a.pdf}
}

@misc{martinsInftyFormerInfinite2022,
  title = {\$\textbackslash infty\$-Former: {{Infinite Memory Transformer}}},
  shorttitle = {\$\textbackslash infty\$-Former},
  author = {Martins, Pedro Henrique and Marinho, Zita and Martins, Andr{\'e} F. T.},
  year = {2022},
  month = mar,
  number = {arXiv:2109.00301},
  eprint = {arXiv:2109.00301},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.00301},
  urldate = {2023-02-14},
  abstract = {Transformers are unable to model long-term memories effectively, since the amount of computation they need to perform grows with the context length. While variations of efficient transformers have been proposed, they all have a finite memory capacity and are forced to drop old information. In this paper, we propose the \$\textbackslash infty\$-former, which extends the vanilla transformer with an unbounded long-term memory. By making use of a continuous-space attention mechanism to attend over the long-term memory, the \$\textbackslash infty\$-former's attention complexity becomes independent of the context length, trading off memory length with precision. In order to control where precision is more important, \$\textbackslash infty\$-former maintains "sticky memories" being able to model arbitrarily long contexts while keeping the computation budget fixed. Experiments on a synthetic sorting task, language modeling, and document grounded dialogue generation demonstrate the \$\textbackslash infty\$-former's ability to retain information from long sequences.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/6DJGRI3P/Martins et al. - 2022 - $infty$-former Infinite Memory Transformer.pdf;/home/xav/Zotero/storage/6ISNI76Z/2109.html}
}

@misc{maviSurveyMultihopQuestion2022,
  title = {A {{Survey}} on {{Multi-hop Question Answering}} and {{Generation}}},
  author = {Mavi, Vaibhav and Jangra, Anubhav and Jatowt, Adam},
  year = {2022},
  month = apr,
  number = {arXiv:2204.09140},
  eprint = {arXiv:2204.09140},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {The problem of Question Answering (QA) has attracted significant research interest for long. Its relevance to language understanding and knowledge retrieval tasks, along with the simple setting makes the task of QA crucial for strong AI systems. Recent success on simple QA tasks has shifted the focus to more complex settings. Among these, Multi-Hop QA (MHQA) is one of the most researched tasks over the recent years. The ability to answer multi-hop questions and perform multi step reasoning can significantly improve the utility of NLP systems. Consequently, the field has seen a sudden surge with high quality datasets, models and evaluation strategies. The notion of `multiple hops' is somewhat abstract which results in a large variety of tasks that require multi-hop reasoning. This implies that different datasets and models differ significantly which makes the field challenging to generalize and survey. This work aims to provide a general and formal definition of MHQA task, and organize and summarize existing MHQA frameworks. We also outline the best methods to create MHQA datasets. The paper provides a systematic and thorough introduction as well as the structuring of the existing attempts to this highly interesting, yet quite challenging task. CCS Concepts: \textbullet{} Information systems \textrightarrow{} Question answering; Content analysis and feature selection.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,I.2.7},
  file = {/home/xav/Zotero/storage/D5PI5WMF/Mavi et al. - 2022 - A Survey on Multi-hop Question Answering and Gener.pdf}
}

@article{mavromatisReaRevAdaptiveReasoning2022,
  title = {{{ReaRev}}: {{Adaptive Reasoning}} for {{Question Answering}} over {{Knowledge Graphs}}},
  shorttitle = {{{ReaRev}}},
  author = {Mavromatis, Costas and Karypis, G.},
  year = {2022},
  journal = {undefined},
  urldate = {2022-11-27},
  abstract = {This work introduces a new way to KGQA reasoning with respect to both instruction decoding and execution, and emulates breadth-first search (BFS) with graph neural networks (GNNs). Knowledge Graph Question Answering (KGQA) involves retrieving entities as answers from a Knowledge Graph (KG) using natural language queries. The challenge is to learn to reason over question-relevant KG facts that traverse KG entities and lead to the question answers. To facilitate reasoning, the question is decoded into instructions, which are dense question representations used to guide the KG traversals. However, if the derived instructions do not exactly match the underlying KG information, they may lead to reasoning under irrelevant context. Our method, termed R EA R EV , introduces a new way to KGQA reasoning with respect to both instruction decoding and execution. To improve instruction decoding, we perform reasoning in an adaptive manner, where KG-aware information is used to iteratively update the initial instructions. To improve instruction execution, we emulate breadth-first search (BFS) with graph neural networks (GNNs). The BFS strategy treats the instructions as a set and allows our method to decide on their execution order on the fly. Experimental results on three KGQA benchmarks demonstrate the R EA R EV 's effectiveness compared with previous state-of-the-art, especially when the KG is incomplete or when we tackle complex questions. Our code is publicly available at https: //github.com/cmavro/ReaRev\_KGQA},
  langid = {english},
  file = {/home/xav/Zotero/storage/VJSB2U4V/Mavromatis et Karypis - 2022 - ReaRev Adaptive Reasoning for Question Answering .pdf}
}

@misc{maXPromptExploringExtreme2022,
  title = {{{XPrompt}}: {{Exploring}} the {{Extreme}} of {{Prompt Tuning}}},
  shorttitle = {{{XPrompt}}},
  author = {Ma, Fang and Zhang, Chen and Ren, Lei and Wang, Jingang and Wang, Qifan and Wu, Wei and Quan, Xiaojun and Song, Dawei},
  year = {2022},
  month = oct,
  number = {arXiv:2210.04457},
  eprint = {arXiv:2210.04457},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.04457},
  urldate = {2022-12-21},
  abstract = {Prompt tuning learns soft prompts to condition frozen Pre-trained Language Models (PLMs) for performing downstream tasks in a parameter-efficient manner. While prompt tuning has gradually reached the performance level of fine-tuning as the model scale increases, there is still a large performance gap between prompt tuning and fine-tuning for models of moderate and small scales (typically less than 11B parameters). In this paper, we empirically show that the trained prompt tokens can have a negative impact on a downstream task and thus degrade its performance. To bridge the gap, we propose a novel Prompt tuning model with an eXtremely small scale (XPrompt) under the regime of lottery tickets hypothesis. Specifically, XPrompt eliminates the negative prompt tokens at different granularity levels through a hierarchical structured pruning, yielding a more parameter-efficient prompt yet with a competitive performance. Comprehensive experiments are carried out on SuperGLUE tasks, and the extensive results indicate that XPrompt is able to close the performance gap at smaller model scales.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/WIXDBYAY/Ma et al. - 2022 - XPrompt Exploring the Extreme of Prompt Tuning.pdf}
}

@misc{mccannNaturalLanguageDecathlon2018,
  title = {The {{Natural Language Decathlon}}: {{Multitask Learning}} as {{Question Answering}}},
  shorttitle = {The {{Natural Language Decathlon}}},
  author = {McCann, Bryan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  year = {2018},
  month = jun,
  number = {arXiv:1806.08730},
  eprint = {arXiv:1806.08730},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.08730},
  urldate = {2023-02-02},
  abstract = {Deep learning has improved performance on many natural language processing (NLP) tasks individually. However, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task. We introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks: question answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, zero-shot relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution. We cast all tasks as question answering over a context. Furthermore, we present a new Multitask Question Answering Network (MQAN) jointly learns all tasks in decaNLP without any task-specific modules or parameters in the multitask setting. MQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification. We demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and performance further improves with an anti-curriculum training strategy. Though designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. We also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/4LSR92AI/McCann et al. - 2018 - The Natural Language Decathlon Multitask Learning.pdf;/home/xav/Zotero/storage/GT3DBK6T/1806.html}
}

@phdthesis{mekroudIntegrationTechniquesDatamining2009,
  title = {Int\'egration Des Techniques Du {{Datamining}} et Des Bases de Donn\'ees Avanc\'ees Dans Le Processus de {{Gestion}} Des {{Connaissances}} : Proposition d'un Processus Hybride Bas\'e Sur Le Raisonnement \`a Partir de Cas},
  author = {MEKROUD, Nourredine},
  year = {2009},
  month = jul,
  file = {/home/xav/Zotero/storage/PGQZBJC5/5_MEKROUD_RIST.pdf;/home/xav/Zotero/storage/SU3YL3DJ/magisterMEKROUDNoureddineInf09.pdf}
}

@article{mengLocatingEditingFactual2022,
  title = {Locating and {{Editing Factual Knowledge}} in {{GPT}}},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2022},
  month = mar,
  journal = {arXiv:2202.05262 [cs]},
  eprint = {2202.05262},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {We investigate the mechanisms underlying factual knowledge recall in autoregressive transformer language models. First, we develop a causal intervention for identifying neuron activations capable of altering a model's factual predictions. Within large GPT-style models, this reveals two distinct sets of neurons that we hypothesize correspond to knowing an abstract fact and saying a concrete word, respectively. This insight inspires the development of ROME, a novel method for editing facts stored in model weights. For evaluation, we assemble CounterFact, a dataset of over twenty thousand counterfactuals and tools to facilitate sensitive measurements of knowledge editing. Using CounterFact, we confirm the distinction between saying and knowing neurons, and we find that ROME achieves state-of-the-art performance in knowledge editing compared to other methods. An interactive demo notebook, full code implementation, and the dataset are available at https://rome.baulab.info/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,I.2.7},
  file = {/home/xav/Zotero/storage/HWA6BYKJ/Meng et al. - 2022 - Locating and Editing Factual Knowledge in GPT.pdf;/home/xav/Zotero/storage/YDJ8ZME8/2202.html}
}

@misc{menickTeachingLanguageModels2022,
  title = {Teaching Language Models to Support Answers with Verified Quotes},
  author = {Menick, Jacob and Trebacz, Maja and Mikulik, Vladimir and Aslanides, John and Song, Francis and Chadwick, Martin and Glaese, Mia and Young, Susannah and {Campbell-Gillingham}, Lucy and Irving, Geoffrey and McAleese, Nat},
  year = {2022},
  month = mar,
  number = {arXiv:2203.11147},
  eprint = {arXiv:2203.11147},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.11147},
  urldate = {2022-10-29},
  abstract = {Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train "open-book" QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\textbackslash\% of the time on this Natural Questions subset, and 67\textbackslash\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\textbackslash\% and 80\textbackslash\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/HVARYL76/Menick et al. - 2022 - Teaching language models to support answers with v.pdf;/home/xav/Zotero/storage/3823I74B/2203.html}
}

@misc{MetaResearchPublications,
  title = {{Meta Research Publications}},
  urldate = {2023-02-15},
  abstract = {Chez Meta, la recherche impr\`egne tout ce que nous faisons. Nous estimons que les questions de recherche les plus int\'eressantes sont le fruit de probl\`emes du monde r\'eel.},
  howpublished = {https://research.facebook.com/publications/},
  langid = {french},
  file = {/home/xav/Zotero/storage/CF6RZ2EL/publications.html}
}

@article{meyerComplexProblemSolving2009,
  title = {Complex {{Problem Solving}} after {{Unstructured Discussion}}: {{Effects}} of {{Information Distribution}} and {{Experience}}},
  shorttitle = {Complex {{Problem Solving}} after {{Unstructured Discussion}}},
  author = {Meyer, Bertolt and Scholl, Wolfgang},
  year = {2009},
  month = jul,
  journal = {Group Processes \& Intergroup Relations},
  volume = {12},
  number = {4},
  pages = {495--515},
  publisher = {{SAGE Publications Ltd}},
  issn = {1368-4302},
  doi = {10.1177/1368430209105045},
  urldate = {2022-09-20},
  abstract = {This study analyzes the effect of information overlap in groups discussing a complex problem on individual post-discussion complex problem solving (CPS). We hypothesize that information distribution among group members has an inverse u-shaped effect on individual post-discussion performance, favoring groups with a medium informational heterogeneity. As CPS is presumably correlated with experience, we also assume that exposure to the problem before the actual task leads to higher performance than less or no exposure. Experimental results support the first hypothesis: A medium overlap of instructional text paragraphs in dyads led to higher performance in a computer-simulated complex problem than complete or no overlap. The second hypothesis is not supported. Limitations of the study and practical implications are discussed.},
  langid = {english},
  annotation = {QID: Q58163705},
  file = {/home/xav/Zotero/storage/N6EN799M/Meyer et Scholl - 2009 - Complex Problem Solving after Unstructured Discuss.pdf}
}

@misc{mialonAugmentedLanguageModels2023,
  title = {Augmented {{Language Models}}: A {{Survey}}},
  shorttitle = {Augmented {{Language Models}}},
  author = {Mialon, Gr{\'e}goire and Dess{\`i}, Roberto and Lomeli, Maria and Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu, Roberta and Rozi{\`e}re, Baptiste and Schick, Timo and {Dwivedi-Yu}, Jane and Celikyilmaz, Asli and Grave, Edouard and LeCun, Yann and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.07842},
  eprint = {arXiv:2302.07842},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.07842},
  urldate = {2023-02-17},
  abstract = {This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/WE5Z86TW/Mialon et al. - 2023 - Augmented Language Models a Survey.pdf;/home/xav/Zotero/storage/B65UPCJ3/2302.html}
}

@misc{MicrosoftResearchPublications,
  title = {Microsoft {{Research Publications}}},
  journal = {Microsoft Research},
  urldate = {2023-02-15},
  abstract = {Below is an index of publications written by Microsoft researchers, often in collaboration with the academic community.},
  howpublished = {https://www.microsoft.com/en-us/research/publications/},
  langid = {american},
  file = {/home/xav/Zotero/storage/5D7L7PYK/publications.html}
}

@misc{minRethinkingRoleDemonstrations2022,
  title = {Rethinking the {{Role}} of {{Demonstrations}}: {{What Makes In-Context Learning Work}}?},
  shorttitle = {Rethinking the {{Role}} of {{Demonstrations}}},
  author = {Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = feb,
  number = {arXiv:2202.12837},
  eprint = {arXiv:2202.12837},
  publisher = {{arXiv}},
  urldate = {2022-05-22},
  abstract = {Large language models (LMs) are able to incontext learn\textemdash perform a new task via inference alone by conditioning on a few inputlabel pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required\textemdash randomly replacing labels in the demonstrations barely hurts performance, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/35NMKVV8/Min et al. - 2022 - Rethinking the Role of Demonstrations What Makes .pdf}
}

@article{mireshghallahQuantifyingPrivacyRisks2022,
  title = {Quantifying {{Privacy Risks}} of {{Masked Language Models Using Membership Inference Attacks}}},
  author = {Mireshghallah, Fatemehsadat and Goyal, Kartik and Uniyal, Archit and {Berg-Kirkpatrick}, Taylor and Shokri, Reza},
  year = {2022},
  month = mar,
  doi = {10.48550/arXiv.2203.03929},
  urldate = {2022-07-19},
  abstract = {The wide adoption and application of Masked language models\textasciitilde (MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities -- to what extent do MLMs leak information about their training data? Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive, implying the potential robustness of MLMs to privacy attacks. In this work, we posit that prior attempts were inconclusive because they based their attack solely on the MLM's model score. We devise a stronger membership inference attack based on likelihood ratio hypothesis testing that involves an additional reference MLM to more accurately quantify the privacy risks of memorization in MLMs. We show that masked language models are extremely susceptible to likelihood ratio membership inference attacks: Our empirical results, on models trained on medical notes, show that our attack improves the AUC of prior membership inference attacks from 0.66 to an alarmingly high 0.90 level, with a significant improvement in the low-error region: at 1\% false positive rate, our attack is 51X more powerful than prior work.},
  langid = {english},
  file = {/home/xav/Zotero/storage/RZNXF828/Mireshghallah et al. - 2022 - Quantifying Privacy Risks of Masked Language Model.pdf;/home/xav/Zotero/storage/QT9GRXVX/2203.html}
}

@misc{mireshghallahQuantifyingPrivacyRisks2022a,
  title = {Quantifying {{Privacy Risks}} of {{Masked Language Models Using Membership Inference Attacks}}},
  author = {Mireshghallah, Fatemehsadat and Goyal, Kartik and Uniyal, Archit and {Berg-Kirkpatrick}, Taylor and Shokri, Reza},
  year = {2022},
  month = nov,
  number = {arXiv:2203.03929},
  eprint = {arXiv:2203.03929},
  publisher = {{arXiv}},
  urldate = {2023-02-08},
  abstract = {The wide adoption and application of Masked language models (MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities. Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive, implying potential robustness of MLMs to privacy attacks. In this work, we posit that prior attempts were inconclusive because they based their attack solely on the MLM's model score. We devise a stronger membership inference attack based on likelihood ratio hypothesis testing that involves an additional reference MLM to more accurately quantify the privacy risks of memorization in MLMs. We show that masked language models are indeed susceptible to likelihood ratio membership inference attacks: Our empirical results, on models trained on medical notes, show that our attack improves the AUC of prior membership inference attacks from 0.66 to an alarmingly high 0.90 level.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/7FY6PJHU/Mireshghallah et al. - 2022 - Quantifying Privacy Risks of Masked Language Model.pdf}
}

@misc{mirowskiCoWritingScreenplaysTheatre2022,
  title = {Co-{{Writing Screenplays}} and {{Theatre Scripts}} with {{Language Models}}: {{An Evaluation}} by {{Industry Professionals}}},
  shorttitle = {Co-{{Writing Screenplays}} and {{Theatre Scripts}} with {{Language Models}}},
  author = {Mirowski, Piotr and Mathewson, Kory W. and Pittman, Jaylen and Evans, Richard},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14958},
  eprint = {arXiv:2209.14958},
  publisher = {{arXiv}},
  urldate = {2023-01-16},
  abstract = {Language models are increasingly attracting interest from writers. However, such models lack long-range semantic coherence, limiting their usefulness for longform creative writing. We address this limitation by applying language models hierarchically, in a system we call Dramatron. By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue. We illustrate Dramatron's usefulness as an interactive co-creative system with a user study of 15 theatre and film industry professionals. Participants co-wrote theatre scripts and screenplays with Dramatron and engaged in open-ended interviews. We report critical reflections both from our interviewees and from independent reviewers who watched stagings of the works to illustrate how both Dramatron and hierarchical text generation could be useful for human-machine co-creativity. Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations -- including plagiarism and bias -- and participatory models for the design and deployment of such tools.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/home/xav/Zotero/storage/YQ4QPV3I/Mirowski et al. - 2022 - Co-Writing Screenplays and Theatre Scripts with La.pdf}
}

@inproceedings{mirzaeeSPARTQATextualQuestion2021,
  title = {{{SPARTQA}}: {{A Textual Question Answering Benchmark}} for {{Spatial Reasoning}}},
  shorttitle = {{{SPARTQA}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Mirzaee, Roshanak and Rajaby Faghihi, Hossein and Ning, Qiang and Kordjamshidi, Parisa},
  year = {2021},
  month = jun,
  pages = {4582--4598},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.naacl-main.364},
  urldate = {2023-02-01},
  abstract = {This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text.},
  file = {/home/xav/Zotero/storage/RPH7QKJT/Mirzaee et al. - 2021 - SPARTQA A Textual Question Answering Benchmark fo.pdf}
}

@misc{mishraCrossTaskGeneralizationNatural2022,
  title = {Cross-{{Task Generalization}} via {{Natural Language Crowdsourcing Instructions}}},
  author = {Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Hajishirzi, Hannaneh},
  year = {2022},
  month = mar,
  number = {arXiv:2104.08773},
  eprint = {arXiv:2104.08773},
  publisher = {{arXiv}},
  urldate = {2023-02-01},
  abstract = {Humans (e.g., crowdworkers) have a remarkable ability in solving different tasks, by simply reading textual instructions that define them and looking at a few examples. Despite the success of the conventional supervised learning on individual datasets, such models often struggle with generalization across tasks (e.g., a question-answering system cannot solve classification tasks). A long-standing challenge in AI is to build a model that learns a new task by understanding the human-readable instructions that define it. To study this, we introduce NATURAL INSTRUCTIONS, a dataset of 61 distinct tasks, their human-authored instructions, and 193k task instances (input-output pairs). The instructions are obtained from crowdsourcing instructions used to create existing NLP datasets and mapped to a unified schema. Using this meta-dataset, we measure cross-task generalization by training models on seen tasks and measuring generalization to the remaining unseen ones. We adopt generative pre-trained language models to encode task-specific instructions along with input and generate task output. Our results indicate that models benefit from instructions when evaluated in terms of generalization to unseen tasks (19\% better for models utilizing instructions). These models, however, are far behind an estimated performance upperbound indicating significant room for more progress in this direction.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/TMXH8SBV/Mishra et al. - 2022 - Cross-Task Generalization via Natural Language Cro.pdf}
}

@misc{mitchellFastModelEditing2022,
  title = {Fast {{Model Editing}} at {{Scale}}},
  author = {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D.},
  year = {2022},
  month = jun,
  number = {arXiv:2110.11309},
  eprint = {arXiv:2110.11309},
  publisher = {{arXiv}},
  urldate = {2023-03-24},
  abstract = {While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks with Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/9UKGTMHC/Mitchell et al. - 2022 - Fast Model Editing at Scale.pdf}
}

@inproceedings{moghaddamTokenbasedPolicyManagement2017,
  title = {Token-Based Policy Management ({{TBPM}}): {{A}} Reliable Data Classification and Access Management Schema in Clouds},
  shorttitle = {Token-Based Policy Management ({{TBPM}})},
  booktitle = {2017 {{International Carnahan Conference}} on {{Security Technology}} ({{ICCST}})},
  author = {Moghaddam, Faraz Fatemi and Wieder, Philipp and Yahyapour, Ramin},
  year = {2017},
  month = oct,
  pages = {1--6},
  issn = {2153-0742},
  doi = {10.1109/CCST.2017.8167836},
  abstract = {Despite the considerable benefits of cloud computing as an emerging technology, there are some reliability and privacy concerns such as generating and managing access policies according to sensitivity of stored data in cloud storages. The most challenging issue in current information policy models is managing security levels, mapping between access requests and defined policies and considering the flexibility and scalability of this management schema according to the characteristics of cloud computing models. Accordingly, an efficient token-based access model has been presented in this paper to provide a semantic mapping between access requests of cloud users and defined policies and sub-policies of cloud customers according to the authentication and access management protocols of protection ontology. Furthermore, a policy-based session token has been introduced to enhance the reliability of access, decrease the time of mapping by eliminating un-necessary mapping from checked policies and decrease data overhead of by classification of policies and sub-policies.},
  keywords = {Access Control,Authentication,Cloud computing,Cloud Computing,Computational modeling,Policy Management,Protocols,Reliability,Security,Semantics,Token Session}
}

@article{moghimifarDomainAdaptativeCausality2020,
  title = {Domain {{Adaptative Causality Encoder}}},
  author = {Moghimifar, Farhad and Haffari, Gholamreza and Baktashmotlagh, Mahsa},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.13549 [cs]},
  eprint = {2011.13549},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Automated discovery of causal relationships from text is a challenging task. Current approaches which are mainly based on the extraction of low-level relations among individual events are limited by the shortage of publicly available labelled data. Therefore, the resulting models perform poorly when applied to a distributionally different domain for which labelled data did not exist at the time of training. To overcome this limitation, in this paper, we leverage the characteristics of dependency trees and adversarial learning to address the tasks of adaptive causality identification and localisation. The term adaptive is used since the training and test data come from two distributionally different datasets, which to the best of our knowledge, this work is the first to address. Moreover, we present a new causality dataset, namely MEDCAUS1, which integrates all types of causality in the text. Our experiments on four different benchmark causality datasets demonstrate the superiority of our approach over the existing baselines, by up to 7\% improvement, on the tasks of identification and localisation of the causal relations from the text.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/Y8JLWZ5S/Moghimifar et al. - 2020 - Domain Adaptative Causality Encoder.pdf}
}

@article{mottaSpecificationsKnowledgeComponents,
  title = {Specifications of {{Knowledge Components}} for {{Reuse}}},
  author = {Motta, Enrico and Fensel, Dieter and Gaspari, Mauro and Benjamins, Richard},
  pages = {8},
  abstract = {The IBROW project aims to support semi-automatic configuration of intelligent problem solvers out of reusable components. The project is developing solutions for the various types of technologies required to make reuse both technically and economically feasible. These technologies include innovative software architectures, modelling languages, software libraries and brokering agents. In this paper we focus on one particular aspect of the IBROW project: the specification of reusable library components. In particular, we illustrate a test case in which a pre-existing library of reusable components for parametric design is reformulated in terms of the framework and constructs provided by the IBROW modelling language. The exercise shows the advantages in terms of reusability and usability afforded by the IBROW approach. The proposed framework and language provide an effective organization for constructing libraries with large horizontal cover, thus maximizing reusability and avoiding the brittleness of traditional, monolithic libraries.},
  langid = {english},
  file = {/home/xav/Zotero/storage/QRYTCMEZ/Motta et al. - Specifications of Knowledge Components for Reuse.pdf}
}

@article{muennighoffSGPTGPTSentence2022,
  title = {{{SGPT}}: {{GPT Sentence Embeddings}} for {{Semantic Search}}},
  shorttitle = {{{SGPT}}},
  author = {Muennighoff, Niklas},
  year = {2022},
  month = mar,
  journal = {arXiv:2202.08904 [cs]},
  eprint = {2202.08904},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {GPT transformers are the largest language models available, yet semantic search is dominated by BERT transformers. We present SGPT-BE and SGPT-CE for applying GPT models as Bi-Encoders or Cross-Encoders to symmetric or asymmetric search. SGPT-BE produces semantically meaningful sentence embeddings by contrastive fine-tuning of only bias tensors and a novel pooling method. A 5.8 billion parameter SGPT-BE outperforms the best available sentence embeddings by 7\% setting a new state-of-the-art on BEIR. It outperforms the concurrently proposed OpenAI Embeddings of the 175 billion parameter Davinci endpoint, which fine-tunes 250,000 times more parameters.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/YHEKMNE3/Muennighoff - 2022 - SGPT GPT Sentence Embeddings for Semantic Search.pdf}
}

@misc{munMarioQAAnsweringQuestions2017,
  title = {{{MarioQA}}: {{Answering Questions}} by {{Watching Gameplay Videos}}},
  shorttitle = {{{MarioQA}}},
  author = {Mun, Jonghwan and Seo, Paul Hongsuck and Jung, Ilchae and Han, Bohyung},
  year = {2017},
  month = aug,
  number = {arXiv:1612.01669},
  eprint = {arXiv:1612.01669},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1612.01669},
  urldate = {2023-01-24},
  abstract = {We present a framework to analyze various aspects of models for video question answering (VideoQA) using customizable synthetic datasets, which are constructed automatically from gameplay videos. Our work is motivated by the fact that existing models are often tested only on datasets that require excessively high-level reasoning or mostly contain instances accessible through single frame inferences. Hence, it is difficult to measure capacity and flexibility of trained models, and existing techniques often rely on ad-hoc implementations of deep neural networks without clear insight into datasets and models. We are particularly interested in understanding temporal relationships between video events to solve VideoQA problems; this is because reasoning temporal dependency is one of the most distinct components in videos from images. To address this objective, we automatically generate a customized synthetic VideoQA dataset using \{\textbackslash em Super Mario Bros.\} gameplay videos so that it contains events with different levels of reasoning complexity. Using the dataset, we show that properly constructed datasets with events in various complexity levels are critical to learn effective models and improve overall performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/KY9EGHG2/Mun et al. - 2017 - MarioQA Answering Questions by Watching Gameplay .pdf;/home/xav/Zotero/storage/W4WU9WSQ/1612.html}
}

@misc{NAACLNorthAmerican,
  title = {{{NAACL}} | {{North American Chapter}} of the {{ACL}} ({{Association}} for {{Computational Linguistics}})},
  urldate = {2023-02-15},
  howpublished = {https://naacl.org/},
  file = {/home/xav/Zotero/storage/88SE4MIZ/naacl.org.html}
}

@misc{nakanoWebGPTBrowserassistedQuestionanswering2022,
  title = {{{WebGPT}}: {{Browser-assisted}} Question-Answering with Human Feedback},
  shorttitle = {{{WebGPT}}},
  author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and Jiang, Xu and Cobbe, Karl and Eloundou, Tyna and Krueger, Gretchen and Button, Kevin and Knight, Matthew and Chess, Benjamin and Schulman, John},
  year = {2022},
  month = mar,
  number = {arXiv:2112.09332},
  eprint = {2112.09332},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2112.09332},
  urldate = {2022-05-25},
  abstract = {We fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment, which allows the model to search and navigate the web. By setting up the task so that it can be performed by humans, we are able to train models on the task using imitation learning, and then optimize answer quality with human feedback. To make human evaluation of factual accuracy easier, models must collect references while browsing in support of their answers. We train and evaluate our models on ELI5, a dataset of questions asked by Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior cloning, and then performing rejection sampling against a reward model trained to predict human preferences. This model's answers are preferred by humans 56\% of the time to those of our human demonstrators, and 69\% of the time to the highest-voted answer from Reddit.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/IC8MCPNN/Nakano et al. - 2022 - WebGPT Browser-assisted question-answering with h.pdf;/home/xav/Zotero/storage/VEWM9QUI/2112.html}
}

@article{narasimhanCGEMsMetricModel2021,
  title = {{{CGEMs}}: {{A Metric Model}} for {{Automatic Code Generation}} Using {{GPT-3}}},
  shorttitle = {{{CGEMs}}},
  author = {Narasimhan, Aishwarya and Rao, Krishna Prasad Agara Venkatesha and B, Veena M.},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.10168 [cs]},
  eprint = {2108.10168},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Today, AI technology is showing its strengths in almost every industry and walks of life. From text generation, text summarization, chatbots, NLP is being used widely. One such paradigm is automatic code generation. An AI could be generating anything; hence the output space is unconstrained. A self-driving car is driven for 100 million miles to validate its safety, but tests cannot be written to monitor and cover an unconstrained space. One of the solutions to validate AI-generated content is to constrain the problem and convert it from abstract to realistic, and this can be accomplished by either validating the unconstrained algorithm using theoretical proofs or by using Monte-Carlo simulation methods. In this case, we use the latter approach to test/validate a statistically significant number of samples. This hypothesis of validating the AI-generated code is the main motive of this work and to know if AI-generated code is reliable, a metric model CGEMs is proposed. This is an extremely challenging task as programs can have different logic with different naming conventions, but the metrics must capture the structure and logic of the program. This is similar to the importance grammar carries in AI-based text generation, Q\&A, translations, etc. The various metrics that are garnered in this work to support the evaluation of generated code are as follows: Compilation, NL description to logic conversion, number of edits needed, some of the commonly used static-code metrics and NLP metrics. These metrics are applied to 80 codes generated using OpenAI's GPT-3. Post which a Neural network is designed for binary classification (acceptable/not acceptable quality of the generated code). The inputs to this network are the values of the features obtained from the metrics. The model achieves a classification accuracy of 76.92\% and an F1 score of 55.56\%. XAI is augmented for model interpretability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/xav/Zotero/storage/52JBP6NF/Narasimhan et al. - 2021 - CGEMs A Metric Model for Automatic Code Generatio.pdf;/home/xav/Zotero/storage/EXFAF8Z5/2108.html}
}

@inproceedings{naroueiIdentificationAccessControl2017,
  title = {Identification of {{Access Control Policy Sentences}} from {{Natural Language Policy Documents}}},
  booktitle = {{{DBSec}}},
  author = {Narouei, M. and Khanpour, Hamed and Takabi, H.},
  year = {2017},
  doi = {10.1007/978-3-319-61176-1_5},
  abstract = {This paper takes advantage of multiple natural language processing techniques including pointwise mutual information to identify access control policy sentences within natural language documents and effectively identifies policy sentences with an average recall and precision of 90\% on all datasets. Access control mechanisms are a necessary and crucial design element to any application's security. There are a plethora of accepted access control models in the information security realm. However, attribute-based access control (ABAC) has been proposed as a general model that could overcome the limitations of the dominant access control models (i.e., role-based access control) while unifying their advantages. One issue with migrating to an ABAC model is the information that needs to be encoded in the model is typically buried within existing natural language artifacts, hence difficult to interpret. This requires processing natural language documents and extracting policies from those documents. Software requirements and policy documents are the main sources of declaring organizational policies, but they are often huge and consist of a lot of general descriptive sentences that lack any access control content. Manually processing these documents to extract policies and then using them to build a model is a laborious and expensive process. This paper is the first step towards a new policy engineering approach for ABAC by processing policy documents and identifying access control contents. We take advantage of multiple natural language processing techniques including pointwise mutual information to identify access control policy sentences within natural language documents. We evaluate our approach on documents from different domains including conference management, education, and healthcare. Our methodology effectively identifies policy sentences with an average recall and precision of 90\% on all datasets, which bested the state-of-the-art by 5\%.},
  file = {/home/xav/Zotero/storage/DZLJ23QF/Narouei et al. - 2017 - Identification of Access Control Policy Sentences .pdf}
}

@misc{NBMEFinetuningDeBERTa,
  title = {{{NBME}} / {{Fine-tuning DeBERTa}} | {{TensorFlow}}},
  urldate = {2022-06-11},
  abstract = {Explore and run machine learning code with Kaggle Notebooks | Using data from multiple data sources},
  howpublished = {https://kaggle.com/ammarnassanalhajali/nbme-fine-tuning-deberta-tensorflow},
  langid = {english}
}

@misc{NeurIPSConferenceNeural,
  title = {{{NeurIPS}} | {{Conference}} on {{Neural Information Processing Systems}}},
  urldate = {2023-02-15},
  howpublished = {https://nips.cc/},
  file = {/home/xav/Zotero/storage/8DV8UCDD/nips.cc.html}
}

@article{ngSSMBASelfSupervisedManifold2020,
  title = {{{SSMBA}}: {{Self-Supervised Manifold Based Data Augmentation}} for {{Improving Out-of-Domain Robustness}}},
  shorttitle = {{{SSMBA}}},
  author = {Ng, Nathan and Cho, Kyunghyun and Ghassemi, Marzyeh},
  year = {2020},
  month = oct,
  journal = {arXiv:2009.10195 [cs, stat]},
  eprint = {2009.10195},
  primaryclass = {cs, stat},
  urldate = {2022-05-04},
  abstract = {Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8\% accuracy on OOD Amazon reviews, 1.8\% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/J6D5XYB8/Ng et al. - 2020 - SSMBA Self-Supervised Manifold Based Data Augment.pdf}
}

@article{nguyenCriteriaKnowledgeRepresentation2020,
  title = {Some {{Criteria}} of the {{Knowledge Representation Method}} for an {{Intelligent Problem Solver}} in {{STEM Education}}},
  author = {Nguyen, Hien D. and Do, Nhon V. and Tran, Nha P. and Pham, Xuan Hau and Pham, Vuong T.},
  year = {2020},
  month = may,
  journal = {Applied Computational Intelligence and Soft Computing},
  volume = {2020},
  pages = {e9834218},
  publisher = {{Hindawi}},
  issn = {1687-9724},
  doi = {10.1155/2020/9834218},
  urldate = {2022-05-04},
  abstract = {Nowadays, building intelligent systems for science, technology, engineering, and math (STEM) education is necessary to support the studying of learners. Intelligent problem solver (IPS) is a system that can be able to solve or tutor how to solve the problems automatically. Learners only declare hypothesis and goal of problems based on a sufficient specification language. They can request the program to solve it automatically or to give instructions that help them to solve it themselves. Knowledge representation plays a vital role in these kinds of intelligent systems. There are various methods for knowledge representation; however, they do not meet the requirements of an IPS in STEM education. In this paper, we propose the criteria of a knowledge model for an IPS in education. These criteria orient to develop a method for knowledge representation to meet actual requirements in practice, especially pedagogical requirements. For proving the effectiveness of these criteria, a knowledge model is also constructed. This model can satisfy these criteria and be applied to build IPS for courses, such as mathematics and physics.},
  langid = {english},
  file = {/home/xav/Zotero/storage/3RH9BNN6/Nguyen et al. - 2020 - Some Criteria of the Knowledge Representation Meth.pdf;/home/xav/Zotero/storage/9R5GQT9Q/9834218.html}
}

@incollection{niApproachMergingIDMRelated2019,
  title = {An {{Approach Merging}} the {{IDM-Related Knowledge}}},
  author = {Ni, Xin and Samet, Ahmed and Cavallucci, Denis},
  year = {2019},
  month = oct,
  pages = {147--158},
  doi = {10.1007/978-3-030-32497-1_13},
  abstract = {Patents are one of the main innovation knowledge sources for engineers and companies. Inventive Design Method (IDM) \textendash{} results from a research that extends from TRIZ and contains formal knowledge description components using ontologies, such as problems, partial solutions, and parameters. In this paper, we introduce IDM-Similar model that extends existing research work in IDM-related knowledge. A neural network named Word2vec and cosine similarity approach are used to build this model to compute the similarity among problems in wide range domains' patents covering from the chemistry to mechanics and the computer to physics. Our model assumes that a partial solution of a patent could be used to solve the problem of another patent from a different domain if these two problems are similar enough. Experiments show that our model is a promising alternative to classical TRIZ for engineers to associate their problems in a field to solutions from patents of another field. Consequently, the step dedicated to solution concepts ideation is improved using our work.},
  isbn = {978-3-030-32496-4},
  file = {/home/xav/Zotero/storage/BXHJ93VH/Ni et al. - 2019 - An Approach Merging the IDM-Related Knowledge.pdf}
}

@incollection{niBuildLinksProblems2020,
  title = {Build {{Links Between Problems}} and {{Solutions}} in the {{Patent}}},
  author = {Ni, Xin and Samet, Ahmed and Cavallucci, Denis},
  year = {2020},
  month = oct,
  pages = {64--76},
  doi = {10.1007/978-3-030-61295-5_6},
  abstract = {Inventive Design Method mostly relies on the presence of exploitable knowledge. It has been elaborated to formalize some aspects of TRIZ being expert-dependent. Patents are appropriate candidates since they contain problems and their corresponding partial solutions. When associated with patents of different fields, problems and partial solutions constitute a potential inventive solution scheme for a target problem. Nevertheless, our study found that links between these two major components are worth studying further. We postulate that problem-solution effectively matching contains a hidden value to automate the solution retrieval and uncover inventive details in patents in order to facilitate R\&D activities. In this paper, we assimilate this challenge to the field of the Question Answering system instead of the traditional syntactic analysis approaches and proposed a model called IDM-Matching. Technically, a state-of-the-art neural network model named XLNet in the Natural Language Processing field is combined into our IDM-Matching to capture the corresponding partial solution for the given query that we masked using the related problem. Then we construct links between these problems and solutions. The final experimental results on the real-world U.S. patent dataset illustrates our model's ability to effectively match IDM-related knowledge with each other. A detailed case study is demonstrated to prove the usage and latent perspective of our proposal in the TRIZ field.},
  isbn = {978-3-030-61294-8},
  file = {/home/xav/Zotero/storage/BSFH753N/Ni et al. - 2020 - Build Links Between Problems and Solutions in the .pdf}
}

@misc{nieEvoMoEEvolutionalMixtureofExperts2022,
  title = {{{EvoMoE}}: {{An Evolutional Mixture-of-Experts Training Framework}} via {{Dense-To-Sparse Gate}}},
  shorttitle = {{{EvoMoE}}},
  author = {Nie, Xiaonan and Miao, Xupeng and Cao, Shijie and Ma, Lingxiao and Liu, Qibin and Xue, Jilong and Miao, Youshan and Liu, Yi and Yang, Zhi and Cui, Bin},
  year = {2022},
  month = oct,
  number = {arXiv:2112.14397},
  eprint = {arXiv:2112.14397},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.14397},
  urldate = {2023-02-09},
  abstract = {Mixture-of-experts (MoE) is becoming popular due to its success in improving the model quality, especially in Transformers. By routing tokens with a sparse gate to a few experts (i.e., a small pieces of the full model), MoE can easily increase the model parameters to a very large scale while keeping the computation cost in a constant level. Most existing works just initialize some random experts, set a fixed gating strategy (e.g., Top-k), and train the model from scratch in an ad-hoc way. We identify that these MoE models are suffering from the immature experts and unstable sparse gate, which are harmful to the convergence performance. In this paper, we propose an efficient end-to-end MoE training framework called EvoMoE. EvoMoE starts from training one single expert and gradually evolves into a large and sparse MoE structure. EvoMoE mainly contains two phases: the expert-diversify phase to train the base expert for a while and spawn multiple diverse experts from it, and the gate-sparsify phase to learn an adaptive sparse gate and activate a dynamic number of experts. EvoMoE naturally decouples the joint learning of both the experts and the sparse gate and focuses on learning the basic knowledge with a single expert at the early training stage. Then it diversifies the experts and continues to train the MoE with a novel Dense-to-Sparse gate (DTS-Gate). Specifically, instead of using a permanent sparse gate, DTS-Gate begins as a dense gate that routes tokens to all experts, then gradually and adaptively becomes sparser while routes to fewer experts. Evaluations are conducted on three popular models and tasks, including RoBERTa for masked language modeling task, GPT for language modeling task and Transformer for machine translation task. The results show that EvoMoE outperforms existing baselines, including Switch, BASE Layer, Hash Layer and StableMoE.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/TTZ29JM8/Nie et al. - 2022 - EvoMoE An Evolutional Mixture-of-Experts Training.pdf;/home/xav/Zotero/storage/T5NMTZUM/2112.html}
}

@misc{nieFlexMoEScalingLargescale2023,
  title = {{{FlexMoE}}: {{Scaling Large-scale Sparse Pre-trained Model Training}} via {{Dynamic Device Placement}}},
  author = {Nie, Xiaonan and Miao, Xupeng},
  year = {2023}
}

@misc{nieHetuMoEEfficientTrillionscale2022,
  title = {{{HetuMoE}}: {{An Efficient Trillion-scale Mixture-of-Expert Distributed Training System}}},
  shorttitle = {{{HetuMoE}}},
  author = {Nie, Xiaonan and Zhao, Pinxue and Miao, Xupeng and Zhao, Tong and Cui, Bin},
  year = {2022},
  month = nov,
  number = {arXiv:2203.14685},
  eprint = {arXiv:2203.14685},
  publisher = {{arXiv}},
  urldate = {2023-02-09},
  abstract = {As giant dense models advance quality but require large amounts of GPU budgets for training, the sparsely gated Mixture-of-Experts (MoE), a kind of conditional computation architecture, is proposed to scale models while keeping their computation constant. Specifically, the input tokens are routed by the gate network and only activates part of the expert network. Existing MoE training systems only support part of mainstream MoE models (e.g. Top k) training under expensive highbandwidth GPU clusters. In this paper, we present HetuMoE, a high-performance large-scale sparse MoE training system built on Hetu. HetuMoE provides multiple gating strategies and efficient GPU kernel implementations. To further improve the training efficiency on commodity GPU clusters (e.g, with only 1 NiC), we introduce the hierarchical AllToAll communication that combines hierarchical networks and aggregating messages. Compared with existing state-of-the-art MoE systems, HetuMoE obtains at least 15\% speedup. Specifically, HetuMoE outperforms DeepSpeed-MoE up to 8.1\texttimes{} under the switch gate with a batch size of 32. Our code is available at: https://github.com/PKU-DAIR/Hetu.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing},
  file = {/home/xav/Zotero/storage/PZ4HTJZI/Nie et al. - 2022 - HetuMoE An Efficient Trillion-scale Mixture-of-Ex.pdf}
}

@misc{ningTORQUEReadingComprehension2020,
  title = {{{TORQUE}}: {{A Reading Comprehension Dataset}} of {{Temporal Ordering Questions}}},
  shorttitle = {{{TORQUE}}},
  author = {Ning, Qiang and Wu, Hao and Han, Rujun and Peng, Nanyun and Gardner, Matt and Roth, Dan},
  year = {2020},
  month = oct,
  number = {arXiv:2005.00242},
  eprint = {arXiv:2005.00242},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.00242},
  urldate = {2023-02-01},
  abstract = {A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as "what happened before/after [some event]?" We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51\% on the test set of TORQUE, about 30\% behind human performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/TN5Z3VCR/Ning et al. - 2020 - TORQUE A Reading Comprehension Dataset of Tempora.pdf;/home/xav/Zotero/storage/5GEDUJ68/2005.html}
}

@article{nkisi-orjiOntologyDrivenInformation2019,
  title = {Ontology Driven Information Retrieval.},
  author = {{Nkisi-Orji}, Ikechukwu},
  year = {2019},
  month = may,
  urldate = {2022-05-04},
  abstract = {Ontology-driven information retrieval deals with the use of entities specified in domain ontologies to enhance search and browse. The entities or concepts of lightweight ontological resources are traditionally used to index resources in specialised domains. Indexing with concepts is often achieved manually and reusing them to enhance search remains a challenge. Other challenges range from the difficulty in merging multiple ontologies for use in retrieval to the problem of integrating concept-based search into existing search systems. We mainly encounter these challenges in enterprise search environments, which have not kept pace with Web search engines and mostly rely on full-text search systems. Full-text search systems are keyword-based and suffer from well-known vocabulary mismatch problems. Ontologies model domain knowledge and have the potential for use in understanding the unstructured content of documents. In this thesis, we investigate the challenges of using domain ontologies for enhancing search in enterprise systems. Firstly, we investigate methods for annotating documents by identifying the best concepts that represent their contents. We explore ways to overcome the challenges of insufficient textual features in lightweight ontologies and introduce an unsupervised method for annotating documents based on generating concept descriptors from external resources. Specifically, we augment concepts with descriptive textual content by exploiting the taxonomic structure of an ontology to ensure that we generate useful descriptors. Secondly, the need often arises for cross-ontology reasoning when using multiple ontologies in ontology-driven search. Once again, we attempt to overcome the absence of rich features in lightweight ontologies by exploring the use of background knowledge for the alignment process. We propose novel ontology alignment techniques which integrate string metrics, semantic features, and term weights for discovering diverse correspondence types in supervised and unsupervised ontology alignment. Thirdly, we investigate different representational schemes for queries and documents and explore semantic ranking models using conceptual representations. Accordingly, we propose a semantic ranking model that incorporates the knowledge of concept relatedness and a predictive model to apply semantic ranking only when it is deemed beneficial for retrieval. Finally, we conduct comprehensive evaluations of the proposed methods and discuss our findings.},
  langid = {english},
  file = {/home/xav/Zotero/storage/5YL45YRL/Nkisi-Orji - 2019 - Ontology driven information retrieval..pdf;/home/xav/Zotero/storage/CJD27P5Q/638312.html}
}

@misc{normanMemoryKnowledgeAnswering1972,
  title = {Memory, {{Knowledge}}, and the {{Answering}} of {{Questions}}.},
  author = {Norman, Donald},
  year = {1972},
  month = may,
  abstract = {An examination of the nature of memory reveals that the representation of knowledge cannot be separated from the uses of  knowledge. The answering of questions is not a simple retrieval and response of stored information; rather the process is embedded in a general structural framework containing knowledge of the questioner, the question, and the world around. The teaching of knowledge requires an interactive process based on the knowledge the other person holds or lacks. A general formal structure for representing semantic information is proposed with examples of network structures for encoding general and specific knowledge. The structure is being tested by simulation on a digital computer. The result of these investigations is the realization that there is much more to the memory process than heretofore been described in our thoeries.},
  file = {/home/xav/Zotero/storage/SX6J3QWP/ED097003.pdf}
}

@article{nottinghamEmbodiedAgentsDream2023,
  title = {Do {{Embodied Agents Dream}} of {{Pixelated Sheep}}?: {{Embodied Decision Making}} Using {{Language Guided World Modelling}}},
  shorttitle = {Do {{Embodied Agents Dream}} of {{Pixelated Sheep}}?},
  author = {Nottingham, Kolby and Ammanabrolu, Prithviraj and Suhr, Alane and Choi, Yejin and Hajishirzi, Hannaneh and Singh, Sameer and Fox, Roy},
  year = {2023},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2301.12050},
  urldate = {2023-02-07},
  abstract = {Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world, which makes learning complex tasks with sparse rewards difficult. If initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that is tested and verified during exploration, to improve sample efficiency in embodied RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM on the basis of its experiences. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences,Machine Learning (cs.LG)},
  file = {/home/xav/Zotero/storage/L9TRBB3Z/Nottingham et al. - 2023 - Do Embodied Agents Dream of Pixelated Sheep Embo.pdf}
}

@misc{NTCIRNIITestbeds,
  title = {{{NTCIR}} | {{NII Testbeds}} and {{Community}} for {{Information}} Access {{Research}}},
  urldate = {2023-02-15},
  howpublished = {https://research.nii.ac.jp/ntcir/},
  file = {/home/xav/Zotero/storage/Q882C2H6/index-en.html}
}

@misc{nyeShowYourWork2021,
  title = {Show {{Your Work}}: {{Scratchpads}} for {{Intermediate Computation}} with {{Language Models}}},
  shorttitle = {Show {{Your Work}}},
  author = {Nye, Maxwell and Andreassen, Anders Johan and {Gur-Ari}, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and Sutton, Charles and Odena, Augustus},
  year = {2021},
  month = nov,
  number = {arXiv:2112.00114},
  eprint = {arXiv:2112.00114},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.00114},
  urldate = {2023-03-13},
  abstract = {Large pre-trained language models perform remarkably well on tasks that can be done "in one pass", such as generating realistic text or synthesizing computer programs. However, they struggle with tasks that require unbounded multi-step computation, such as adding integers or executing programs. Surprisingly, we find that these same models are able to perform complex multi-step computations -- even in the few-shot regime -- when asked to perform the operation "step by step", showing the results of intermediate computations. In particular, we train transformers to perform multi-step computations by asking them to emit intermediate computation steps into a "scratchpad". On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/xav/Zotero/storage/HPKXU4NX/Nye et al. - 2021 - Show Your Work Scratchpads for Intermediate Compu.pdf;/home/xav/Zotero/storage/MJ2ME5BT/2112.html}
}

@inproceedings{ochanomizuuniversityjapanLanguageModelTemporal2021,
  title = {Towards a {{Language Model}} for {{Temporal Commonsense Reasoning}}},
  booktitle = {Student {{Research Workshop}}},
  author = {{Ochanomizu University, Japan} and Kimura, Mayuko and Pereira, Lis Kanashiro and {Ochanomizu University, Japan} and Kobayashi, Ichiro and {Ochanomizu University, Japan}},
  year = {2021},
  pages = {78--84},
  doi = {10.26615/issn.2603-2821.2021_012},
  urldate = {2023-01-30},
  abstract = {Temporal commonsense reasoning is a challenging task as it requires temporal knowledge usually not explicitly stated in text. In this work, we propose an ensemble model for temporal commonsense reasoning. Our model relies on pre-trained contextual representations from transformer-based language models (i.e., BERT), and on a variety of training methods for enhancing model generalization: 1) multistep fine-tuning using carefully selected auxiliary tasks and datasets, and 2) a specifically designed temporal task-adaptive pre-trainig task aimed to capture temporal commonsense knowledge. Our model greatly outperforms the standard fine-tuning approach and strong baselines on the MC-TACO dataset.},
  langid = {english},
  file = {/home/xav/Zotero/storage/LX84BRJW/Ochanomizu University, Japan et al. - 2021 - Towards a Language Model for Temporal Commonsense .pdf}
}

@article{olmoGPT3toplanExtractingPlans2021,
  title = {{{GPT3-to-plan}}: {{Extracting}} Plans from Text Using {{GPT-3}}},
  shorttitle = {{{GPT3-to-plan}}},
  author = {Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.07131 [cs]},
  eprint = {2106.07131},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Operations in many essential industries including finance and banking are often characterized by the need to perform repetitive sequential tasks. Despite their criticality to the business, workflows are rarely fully automated or even formally specified, though there may exist a number of natural language documents describing these procedures for the employees of the company. Plan extraction methods provide us with the possibility of extracting structure plans from such natural language descriptions of the plans/workflows, which could then be leveraged by an automated system. In this paper, we investigate the utility of generalized language models in performing such extractions directly from such texts. Such models have already been shown to be quite effective in multiple translation tasks, and our initial results seem to point to their effectiveness also in the context of plan extractions. Particularly, we show that GPT-3 is able to generate plan extraction results that are comparable to many of the current state of the art plan extraction methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/S7E9TU9W/Olmo et al. - 2021 - GPT3-to-plan Extracting plans from text using GPT.pdf;/home/xav/Zotero/storage/EGYDCNPG/2106.html}
}

@inproceedings{onishiWhoDidWhat2016,
  title = {Who Did {{What}}: {{A Large-Scale Person-Centered Cloze Dataset}}},
  shorttitle = {Who Did {{What}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Onishi, Takeshi and Wang, Hai and Bansal, Mohit and Gimpel, Kevin and McAllester, David},
  year = {2016},
  month = nov,
  pages = {2230--2235},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1241},
  urldate = {2023-02-01},
  file = {/home/xav/Zotero/storage/VX8YYTUZ/Onishi et al. - 2016 - Who did What A Large-Scale Person-Centered Cloze .pdf}
}

@misc{openaiGPT4TechnicalReport2023,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI},
  year = {2023},
  urldate = {2023-03-17},
  file = {/home/xav/Zotero/storage/F28GI4KA/gpt-4.pdf}
}

@misc{OpenAIPublications2020,
  title = {{{OpenAI Publications}}},
  year = {2020},
  month = sep,
  journal = {OpenAI},
  urldate = {2023-02-15},
  abstract = {OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity.},
  howpublished = {https://openai.com/publications/},
  langid = {english},
  file = {/home/xav/Zotero/storage/6YVFX94T/publications.html}
}

@misc{osbandFineTuningLanguageModels2022,
  title = {Fine-{{Tuning Language Models}} via {{Epistemic Neural Networks}}},
  author = {Osband, Ian and Asghari, Seyed Mohammad and Van Roy, Benjamin and McAleese, Nat and Aslanides, John and Irving, Geoffrey},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01568},
  eprint = {arXiv:2211.01568},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01568},
  urldate = {2023-01-04},
  abstract = {Large language models are now part of a powerful new paradigm in machine learning. These models learn a wide range of capabilities from training on large unsupervised text corpora. In many applications, these capabilities are then fine-tuned through additional training on specialized data to improve performance in that setting. In this paper, we augment these models with an epinet: a small additional network architecture that helps to estimate model uncertainty and form an epistemic neural network (ENN). ENNs are neural networks that can know what they don't know. We show that, using an epinet to prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same performance while using 2x less data. We also investigate performance in synthetic neural network generative models designed to build understanding. In each setting, using an epinet outperforms heuristic active learning schemes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/3C4TGZQQ/Osband et al. - 2022 - Fine-Tuning Language Models via Epistemic Neural N.pdf}
}

@misc{ostermannMCScriptNovelDataset2018,
  title = {{{MCScript}}: {{A Novel Dataset}} for {{Assessing Machine Comprehension Using Script Knowledge}}},
  shorttitle = {{{MCScript}}},
  author = {Ostermann, Simon and Modi, Ashutosh and Roth, Michael and Thater, Stefan and Pinkal, Manfred},
  year = {2018},
  month = mar,
  number = {arXiv:1803.05223},
  eprint = {arXiv:1803.05223},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.05223},
  urldate = {2023-02-01},
  abstract = {We introduce a large dataset of narrative texts and questions about these texts, intended to be used in a machine comprehension task that requires reasoning using commonsense knowledge. Our dataset complements similar datasets in that we focus on stories about everyday activities, such as going to the movies or working in the garden, and that the questions require commonsense knowledge, or more specifically, script knowledge, to be answered. We show that our mode of data collection via crowdsourcing results in a substantial amount of such inference questions. The dataset forms the basis of a shared task on commonsense and script knowledge organized at SemEval 2018 and provides challenging test cases for the broader natural language understanding community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BBA387VK/Ostermann et al. - 2018 - MCScript A Novel Dataset for Assessing Machine Co.pdf;/home/xav/Zotero/storage/PV4VDRCD/1803.html}
}

@misc{ouyangTrainingLanguageModels2022,
  type = {{{RLHF}}},
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  month = mar,
  number = {arXiv:2203.02155},
  eprint = {arXiv:2203.02155},
  publisher = {{arXiv}},
  urldate = {2022-05-22},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/CUIYNDIU/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf}
}

@misc{pagnoniSocraticPretrainingQuestionDriven2022,
  title = {Socratic {{Pretraining}}: {{Question-Driven Pretraining}} for {{Controllable Summarization}}},
  shorttitle = {Socratic {{Pretraining}}},
  author = {Pagnoni, Artidoro and Fabbri, Alexander R. and Kry{\'s}ci{\'n}ski, Wojciech and Wu, Chien-Sheng},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10449},
  eprint = {arXiv:2212.10449},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10449},
  urldate = {2023-02-07},
  abstract = {In long document controllable summarization, where labeled data is scarce, pretrained models struggle to adapt to the task and effectively respond to user queries. In this paper, we introduce Socratic pretraining, a question-driven, unsupervised pretraining objective specifically designed to improve controllability in summarization tasks. By training a model to generate and answer relevant questions in a given context, Socratic pretraining enables the model to more effectively adhere to user-provided queries and identify relevant content to be summarized. We demonstrate the effectiveness of this approach through extensive experimentation on two summarization domains, short stories and dialogue, and multiple control strategies: keywords, questions, and factoid QA pairs. Our pretraining method relies only on unlabeled documents and a question generation system and outperforms pre-finetuning approaches that use additional supervised data. Furthermore, our results show that Socratic pretraining cuts task-specific labeled data requirements in half, is more faithful to user-provided queries, and achieves state-of-the-art performance on QMSum and SQuALITY.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/7ZGUN9LI/Pagnoni et al. - 2022 - Socratic Pretraining Question-Driven Pretraining .pdf;/home/xav/Zotero/storage/25XKPEAW/2212.html}
}

@misc{palaniyappanStudyingPsychosisUsing2023,
  title = {Studying Psychosis Using {{Natural Language Generation}}: {{A}} Review of Emerging Opportunities},
  shorttitle = {Studying Psychosis Using {{Natural Language Generation}}},
  author = {Palaniyappan, Lena and Benrimoh, David and Voppel, Alban and Rocca, Roberta},
  year = {2023},
  month = jan,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/rdn3k},
  urldate = {2023-02-09},
  abstract = {Disrupted language in psychotic disorders, such as schizophrenia, can manifest as false contents and formal deviations, often described as thought disorder. These features play a critical role in the social dysfunction associated with psychosis, but we continue to lack insights regarding how these symptoms develop. Natural language Generation (NLG) is a field of computer science that focuses on generating human-like language for various applications. The theory that psychosis is related to the evolution of language in humans suggests that NLG systems that are sufficiently evolved to generate human-like language may also exhibit psychosis-like features.  In this conceptual review, we propose using NLG systems that are at various stages of development as in-silico tools to study linguistic features of psychosis. This will allow us to gain a better understanding of the relationship between language and psychosis and potentially pave the way for new therapeutic approaches to address this vexing challenge.},
  archiveprefix = {arxiv},
  langid = {american},
  keywords = {Computational Neuroscience,computational psychiatry,deep learning,explainable models,formal thought disorders,Neuroscience,Psychiatry,schizophrenia,toy models},
  file = {/home/xav/Zotero/storage/MKTBMGAK/Palaniyappan et al. - 2023 - Studying psychosis using Natural Language Generati.pdf}
}

@misc{palLargescaleModelPersonalization2023,
  title = {Large-Scale {{Model Personalization}} via {{Low Rank}} and {{Sparse}} Decomposition},
  author = {Pal, Soumyabrata and Varshney, Prateek and Jain, Prateek and Thakurta, Abhradeep Guha and Madan, Gagan and Aggarwal, Gaurav and Shenoy, Pradeep and Srivastava, Gaurav},
  year = {2023},
  month = jan,
  number = {arXiv:2210.03505},
  eprint = {arXiv:2210.03505},
  publisher = {{arXiv}},
  urldate = {2023-03-24},
  abstract = {Personalization of machine learning (ML) predictions for individual users/domains/enterprises is critical for practical recommendation style systems. Standard personalization approaches involve learning a user/domain specific embedding that is fed into a fixed global model which can be limiting. On the other hand, personalizing/fine-tuning model itself for each user/domain \textendash{} a.k.a meta-learning \textendash{} has high storage/infrastructure cost. We propose a novel meta-learning style approach that models network weights as a sum of low-rank and sparse matrices. This captures common information from multiple individuals/users together in the low-rank part while sparse part captures user-specific idiosyncrasies. Furthermore, the framework is up to two orders of magnitude more scalable (in terms of storage/infrastructure cost) than user-specific finetuning of model. We then study the framework in the linear setting, where the problem reduces to that of estimating the sum of a rank-r and a k-column sparse matrix using a small number of linear measurements. We propose an alternating minimization method with iterative hard thresholding \textendash{} AMHT-LRS\textendash{} to learn the low-rank and sparse part. For the realizable, Gaussian data setting, we show that AMHT-LRS solves the problem efficiently with nearly optimal samples. A significant challenge in personalization is ensuring privacy of each user's sensitive data. We alleviate this problem by proposing a differentially private variant of our method that also is equipped with strong generalization guarantees. Finally, on multiple standard recommendation datasets, we demonstrate that our approach allows personalized models to obtain superior performance in sparse data regime.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/H6A6Z7XK/Pal et al. - 2023 - Large-scale Model Personalization via Low Rank and.pdf}
}

@misc{pandyaQuestionAnsweringSurvey2021,
  title = {Question {{Answering Survey}}: {{Directions}}, {{Challenges}}, {{Datasets}}, {{Evaluation Matrices}}},
  shorttitle = {Question {{Answering Survey}}},
  author = {Pandya, Hariom A. and Bhatt, Brijesh S.},
  year = {2021},
  month = dec,
  number = {arXiv:2112.03572},
  eprint = {arXiv:2112.03572},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {The usage and amount of information available on the internet increase over the past decade. This digitization leads to the need for automated answering system to extract fruitful information from redundant and transitional knowledge sources. Such systems are designed to cater the most prominent answer from this giant knowledge source to the user's query using natural language understanding (NLU) and thus eminently depends on the Question-answering(QA) field. Question answering involves but not limited to the steps like mapping of user's question to pertinent query, retrieval of relevant information, finding the best suitable answer from the retrieved information etc. The current improvement of deep learning models evince compelling performance improvement in all these tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/NZ6WW4GY/Pandya et Bhatt - 2021 - Question Answering Survey Directions, Challenges,.pdf}
}

@article{panEndtoEndTableQuestion2022,
  title = {End-to-{{End Table Question Answering}} via {{Retrieval-Augmented Generation}}},
  author = {Pan, Feifei and Canim, Mustafa and Glass, Michael R. and Gliozzo, A. and Hendler, J.},
  year = {2022},
  journal = {ArXiv},
  doi = {10.48550/arXiv.2203.16714},
  abstract = {T-RAG, an end-to-end Table QA model, where a non-parametric dense vector index is fine-tuned jointly with BART, a parametric sequence- to-sequence model to generate answer tokens, is intro-duce. Most existing end-to-end Table Question Answering (Table QA) models consist of a two-stage framework with a retriever to select relevant table candidates from a corpus and a reader to locate the correct answers from table candidates. Even though the accuracy of the reader models is significantly improved with the recent transformer-based approaches, the overall performance of such frameworks still suffers from the poor accuracy of using traditional information retrieval techniques as re-trievers. To alleviate this problem, we intro-duce T-RAG, an end-to-end Table QA model, where a non-parametric dense vector index is fine-tuned jointly with BART, a parametric sequence-to-sequence model to generate answer tokens. Given any natural language question, T-RAG utilizes a unified pipeline to auto-matically search through a table corpus to directly locate the correct answer from table cell. We apply T-RAG on recent open-domain Table QA benchmarks and demonstrate that the fine-tuned T-RAG model is able to achieve state-of-the-art performance in both the end-to-end Table QA and the table retrieval tasks.},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/2YIKJ88S/Pan et al. - 2022 - End-to-End Table Question Answering via Retrieval-.pdf}
}

@article{papanikolaouDAREDataAugmented2020,
  title = {{{DARE}}: {{Data Augmented Relation Extraction}} with {{GPT-2}}},
  shorttitle = {{{DARE}}},
  author = {Papanikolaou, Yannis and Pierleoni, Andrea},
  year = {2020},
  month = apr,
  journal = {arXiv:2004.13845 [cs, stat]},
  eprint = {2004.13845},
  primaryclass = {cs, stat},
  urldate = {2022-05-04},
  abstract = {Real-world Relation Extraction (RE) tasks are challenging to deal with, either due to limited training data or class imbalance issues. In this work, we present Data Augmented Relation Extraction(DARE), a simple method to augment training data by properly fine-tuning GPT-2 to generate examples for specific relation types. The generated training data is then used in combination with the gold dataset to train a BERT-based RE classifier. In a series of experiments we show the advantages of our method, which leads in improvements of up to 11 F1 score points against a strong base-line. Also, DARE achieves new state of the art in three widely used biomedical RE datasets surpassing the previous best results by 4.7 F1 points on average.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/GXWAN4Q2/Papanikolaou et Pierleoni - 2020 - DARE Data Augmented Relation Extraction with GPT-.pdf;/home/xav/Zotero/storage/R9WW6TCF/2004.html}
}

@misc{papanikolaouSlotFillingBiomedical2022,
  title = {Slot {{Filling}} for {{Biomedical Information Extraction}}},
  author = {Papanikolaou, Yannis and Staib, Marlene and Grace, Justin and Bennett, Francine},
  year = {2022},
  month = apr,
  number = {arXiv:2109.08564},
  eprint = {arXiv:2109.08564},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.08564},
  urldate = {2022-08-25},
  abstract = {Information Extraction (IE) from text refers to the task of extracting structured knowledge from unstructured text. The task typically consists of a series of sub-tasks such as Named Entity Recognition and Relation Extraction. Sourcing entity and relation type specific training data is a major bottleneck in domains with limited resources such as biomedicine. In this work we present a slot filling approach to the task of biomedical IE, effectively replacing the need for entity and relation-specific training data, allowing us to deal with zero-shot settings. We follow the recently proposed paradigm of coupling a Tranformer-based bi-encoder, Dense Passage Retrieval, with a Transformer-based reading comprehension model to extract relations from biomedical text. We assemble a biomedical slot filling dataset for both retrieval and reading comprehension and conduct a series of experiments demonstrating that our approach outperforms a number of simpler baselines. We also evaluate our approach end-to-end for standard as well as zero-shot settings. Our work provides a fresh perspective on how to solve biomedical IE tasks, in the absence of relevant training data. Our code, models and datasets are available at https://github.com/ypapanik/biomedical-slot-filling.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/SH3CRNJJ/Papanikolaou et al. - 2022 - Slot Filling for Biomedical Information Extraction.pdf}
}

@misc{PapersCodeMCScript,
  title = {Papers with {{Code}} - {{MCScript}}: {{A Novel Dataset}} for {{Assessing Machine Comprehension Using Script Knowledge}}},
  shorttitle = {Papers with {{Code}} - {{MCScript}}},
  urldate = {2023-02-01},
  abstract = {No code available yet.},
  howpublished = {https://paperswithcode.com/paper/mcscript-a-novel-dataset-for-assessing},
  langid = {english},
  file = {/home/xav/Zotero/storage/PNVB3LE8/mcscript-a-novel-dataset-for-assessing.html}
}

@inproceedings{paranjapePromptingContrastiveExplanations2021,
  title = {Prompting {{Contrastive Explanations}} for {{Commonsense Reasoning Tasks}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL-IJCNLP}} 2021},
  author = {Paranjape, Bhargavi and Michael, Julian and Ghazvininejad, Marjan and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2021},
  month = aug,
  pages = {4179--4192},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.findings-acl.366},
  urldate = {2023-02-07},
  file = {/home/xav/Zotero/storage/EH73B4P7/Paranjape et al. - 2021 - Prompting Contrastive Explanations for Commonsense.pdf}
}

@misc{parmarInBoXBARTGetInstructions2022,
  title = {In-{{BoXBART}}: {{Get Instructions}} into {{Biomedical Multi-Task Learning}}},
  shorttitle = {In-{{BoXBART}}},
  author = {Parmar, Mihir and Mishra, Swaroop and Purohit, Mirali and Luo, Man and Murad, M. Hassan and Baral, Chitta},
  year = {2022},
  month = apr,
  number = {arXiv:2204.07600},
  eprint = {arXiv:2204.07600},
  publisher = {{arXiv}},
  urldate = {2022-05-22},
  abstract = {Single-task models have proven pivotal in solving specific tasks; however, they have limitations in real-world applications where multi-tasking is necessary and domain shifts are exhibited. Recently, instructional prompts have shown significant improvement towards multi-task generalization; however, the effect of instructional prompts and Multi-Task Learning (MTL) has not been systematically studied in the biomedical domain. Motivated by this, this paper explores the impact of instructional prompts for biomedical MTL. We introduce the BoX, a collection of 32 instruction tasks for Biomedical NLP across (X) various categories. Using this meta-dataset, we propose a unified model termed In-BoXBART, that can jointly learn all tasks of the BoX without any task-specific modules. To the best of our knowledge, this is the first attempt to propose a unified model in the biomedical domain and use instructions to achieve generalization across several biomedical tasks. Experimental results indicate that the proposed model: 1) outperforms the single-task baseline by \textasciitilde 3\% and multi-task (without instruction) baseline by \textasciitilde 18\% on an average, and 2) shows \textasciitilde 23\% improvement compared to the single-task baseline in few-shot learning (i.e., 32 instances per task) on an average. Our analysis indicates that there is significant room for improvement across tasks in the BoX, implying the scope for future research direction.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/QWLZYZQM/Parmar et al. - 2022 - In-BoXBART Get Instructions into Biomedical Multi.pdf}
}

@misc{pengGODELLargeScalePreTraining2022,
  title = {{{GODEL}}: {{Large-Scale Pre-Training}} for {{Goal-Directed Dialog}}},
  shorttitle = {{{GODEL}}},
  author = {Peng, Baolin and Galley, Michel and He, Pengcheng and Brockett, Chris and Liden, Lars and Nouri, Elnaz and Yu, Zhou and Dolan, Bill and Gao, Jianfeng},
  year = {2022},
  month = jun,
  number = {arXiv:2206.11309},
  eprint = {arXiv:2206.11309},
  publisher = {{arXiv}},
  urldate = {2022-08-07},
  abstract = {We introduce GODEL (Grounded Open Dialogue Language Model), a large pre-trained language model for dialog. In contrast with earlier models such as DialoGPT, GODEL leverages a new phase of grounded pre-training designed to better support adapting GODEL to a wide range of downstream dialog tasks that require information external to the current conversation (e.g., a database or document) to produce good responses. Experiments against an array of benchmarks that encompass task-oriented dialog, conversational QA, and grounded open-domain dialog show that GODEL outperforms state-of-the-art pre-trained dialog models in few-shot fine-tuning setups, in terms of both human and automatic evaluation. A novel feature of our evaluation methodology is the introduction of a notion of utility that assesses the usefulness of responses (extrinsic evaluation) in addition to their communicative features (intrinsic evaluation). We show that extrinsic evaluation offers improved inter-annotator agreement and correlation with automated metrics. Code and data processing scripts are publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/PAUAICMI/Peng et al. - 2022 - GODEL Large-Scale Pre-Training for Goal-Directed .pdf}
}

@misc{perezDiscoveringLanguageModel2022,
  title = {Discovering {{Language Model Behaviors}} with {{Model-Written Evaluations}}},
  author = {Perez, Ethan and Ringer, Sam and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and {Tran-Johnson}, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noem{\'i} and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and {Telleen-Lawton}, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and {Hatfield-Dodds}, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09251},
  eprint = {arXiv:2212.09251},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.09251},
  urldate = {2023-02-08},
  abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/XZTBMMB5/Perez et al. - 2022 - Discovering Language Model Behaviors with Model-Wr.pdf;/home/xav/Zotero/storage/KTDRYLW9/2212.html}
}

@misc{perezRedTeamingLanguage2022,
  title = {Red {{Teaming Language Models}} with {{Language Models}}},
  author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  year = {2022},
  month = feb,
  number = {arXiv:2202.03286},
  eprint = {arXiv:2202.03286},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2202.03286},
  urldate = {2023-01-16},
  abstract = {Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/Q4XM4QUR/Perez et al. - 2022 - Red Teaming Language Models with Language Models.pdf;/home/xav/Zotero/storage/VAX6H5E3/2202.html}
}

@article{perezUnsupervisedQuestionDecomposition2020,
  title = {Unsupervised {{Question Decomposition}} for {{Question Answering}}},
  author = {Perez, Ethan and Lewis, Patrick and Yih, Wen-tau and Cho, Kyunghyun and Kiela, Douwe},
  year = {2020},
  month = oct,
  journal = {arXiv:2002.09758 [cs]},
  eprint = {2002.09758},
  primaryclass = {cs},
  urldate = {2022-05-02},
  abstract = {We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/RI8IEKAJ/Perez et al. - 2020 - Unsupervised Question Decomposition for Question A.pdf;/home/xav/Zotero/storage/2EE62T4Z/2002.html}
}

@misc{petroniImprovingWikipediaVerifiability2022,
  title = {Improving {{Wikipedia Verifiability}} with {{AI}}},
  author = {Petroni, Fabio and Broscheit, Samuel and Piktus, Aleksandra and Lewis, Patrick and Izacard, Gautier and Hosseini, Lucas and {Dwivedi-Yu}, Jane and Lomeli, Maria and Schick, Timo and Mazar{\'e}, Pierre-Emmanuel and Joulin, Armand and Grave, Edouard and Riedel, Sebastian},
  year = {2022},
  month = jul,
  number = {arXiv:2207.06220},
  eprint = {arXiv:2207.06220},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.06220},
  urldate = {2022-08-17},
  abstract = {Verifiability is a core content policy of Wikipedia: claims that are likely to be challenged need to be backed by citations. There are millions of articles available online and thousands of new articles are released each month. For this reason, finding relevant sources is a difficult task: many claims do not have any references that support them. Furthermore, even existing citations might not support a given claim or become obsolete once the original source is updated or deleted. Hence, maintaining and improving the quality of Wikipedia references is an important challenge and there is a pressing need for better tools to assist humans in this effort. Here, we show that the process of improving references can be tackled with the help of artificial intelligence (AI). We develop a neural network based system, called Side, to identify Wikipedia citations that are unlikely to support their claims, and subsequently recommend better ones from the web. We train this model on existing Wikipedia references, therefore learning from the contributions and combined wisdom of thousands of Wikipedia editors. Using crowd-sourcing, we observe that for the top 10\% most likely citations to be tagged as unverifiable by our system, humans prefer our system's suggested alternatives compared to the originally cited reference 70\% of the time. To validate the applicability of our system, we built a demo to engage with the English-speaking Wikipedia community and find that Side's first citation recommendation collects over 60\% more preferences than existing Wikipedia citations for the same top 10\% most likely unverifiable claims according to Side. Our results indicate that an AI-based system could be used, in tandem with humans, to improve the verifiability of Wikipedia. More generally, we hope that our work can be used to assist fact checking efforts and increase the general trustworthiness of information online.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/WYIRVT4G/Petroni et al. - 2022 - Improving Wikipedia Verifiability with AI.pdf}
}

@inproceedings{pfeifferMADXAdapterBasedFramework2020,
  title = {{{MAD-X}}: {{An Adapter-Based Framework}} for {{Multi-Task Cross-Lingual Transfer}}},
  shorttitle = {{{MAD-X}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pfeiffer, Jonas and Vuli{\'c}, Ivan and Gurevych, Iryna and Ruder, Sebastian},
  year = {2020},
  month = nov,
  pages = {7654--7673},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.617},
  urldate = {2023-03-24},
  abstract = {The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language. MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering. Our code and adapters are available at AdapterHub.ml.},
  file = {/home/xav/Zotero/storage/RKTJF85R/Pfeiffer et al. - 2020 - MAD-X An Adapter-Based Framework for Multi-Task C.pdf}
}

@misc{pfistererHumanCenteredAutoML2019,
  title = {Towards {{Human Centered AutoML}}},
  author = {Pfisterer, Florian and Thomas, Janek and Bischl, Bernd},
  year = {2019},
  month = nov,
  number = {arXiv:1911.02391},
  eprint = {arXiv:1911.02391},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Building models from data is an integral part of the majority of data science work flows. While data scientists are often forced to spend the majority of the time available for a given project on data cleaning and exploratory analysis, the time available to practitioners to build actual models from data is often rather short due to time constraints for a given project. AutoML systems are currently rising in popularity, as they can build powerful models without human oversight. In this position paper, we aim to discuss the impact of the rising popularity of such systems and how a user-centered interface for such systems could look like. More importantly, we also want to point out features that are currently missing in those systems and start to explore better usability of such systems from a data-scientists perspective.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/home/xav/Zotero/storage/VGILB8D7/Pfisterer et al. - 2019 - Towards Human Centered AutoML.pdf}
}

@misc{phangHyperTuningAdaptingLarge2022,
  title = {{{HyperTuning}}: {{Toward Adapting Large Language Models}} without {{Back-propagation}}},
  shorttitle = {{{HyperTuning}}},
  author = {Phang, Jason and Mao, Yi and He, Pengcheng and Chen, Weizhu},
  year = {2022},
  month = nov,
  number = {arXiv:2211.12485},
  eprint = {arXiv:2211.12485},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Fine-tuning large language models for different tasks can be costly and inefficient, and even methods that reduce the number of tuned parameters still require full gradient-based optimization. We propose HyperTuning, a novel approach to model adaptation that uses a hypermodel to generate task-specific parameters for a fixed downstream model. We demonstrate a simple setup for hypertuning with HyperT5, a T5-based hypermodel that produces soft prefixes or LoRA parameters for a frozen T5 model from few-shot examples. We train HyperT5 in two stages: first, hyperpretraining with a modified conditional language modeling objective that trains a hypermodel to generate parameters; second, multi-task fine-tuning (MTF) on a large number of diverse language tasks. We evaluate HyperT5 on P3, MetaICL and Super-NaturalInstructions datasets, and show that it can effectively generate parameters for unseen tasks. Moreover, we show that using hypermodel-generated parameters as initializations for further parameter-efficient fine-tuning improves performance. HyperTuning can thus be a flexible and efficient way to leverage large language models for diverse downstream applications.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/UYUVHHH9/Phang et al. - 2022 - HyperTuning Toward Adapting Large Language Models.pdf}
}

@article{philippDecisionMakingMultiStepExpert2019,
  title = {Decision-{{Making}} with {{Multi-Step Expert Advice}} on the {{Web}}},
  author = {Philipp, Patrick Raoul},
  year = {2019},
  publisher = {{Karlsruhe}},
  doi = {10.5445/IR/1000093522},
  urldate = {2022-03-25},
  abstract = {This thesis deals with solving multi-step tasks by using advice from experts, which are algorithms to solve individual steps of such tasks.  We contribute with methods for maximizing the number of correct task solutions by selecting and combining experts for individual task instances and methods for automating the process of solving tasks on the Web, where experts are available as Web services.    Multi-step tasks frequently occur in Natural Language Processing (NLP) or Computer Vision, and as research progresses an increasing amount of exchangeable experts for the same steps are available on the Web.   Service provider platforms such as Algorithmia monetize expert access by making expert services available via their platform and having customers pay for single executions.  Such experts can be used to solve diverse tasks, which often consist of multiple steps and thus require pipelines of experts to generate hypotheses.    We perceive two distinct problems for solving multi-step tasks with expert services:  (1) Given that the task is sufficiently complex, no single pipeline generates correct solutions for all possible task instances.  One thus must learn how to construct individual expert pipelines for individual task instances in order to maximize the number of correct solutions, while also taking into account the costs adhered to executing an expert.   (2) To automatically solve multi-step tasks with expert services, we need to discover, execute and compose expert pipelines.  With mostly textual descriptions of complex functionalities and input parameters, Web automation entails to integrate available expert services and data, interpreting user-specified task goals or efficiently finding correct service configurations.     In this thesis, we present solutions to both problems:  (1) We enable to learn well-performing expert pipelines assuming available reference data sets (comprising a number of task instances and solutions), where we distinguish between centralized and decentralized decision-making.   We formalize the problem as specialization of a Markov Decision Process (MDP), which we refer to as Expert Process (EP) and integrate techniques from Statistical Relational Learning (SRL) or Multiagent coordination.   (2) We develop a framework for automatically discovering, executing and composing expert pipelines by exploiting methods developed for the Semantic Web.  We lift the representations of experts with structured vocabularies modeled with the Resource Description Framework (RDF) and extend EPs to Semantic Expert Processes (SEPs) to enable the data-driven execution of experts in Web-based architectures.    We evaluate our methods in different domains, namely Medical Assistance with tasks in Image Processing and Surgical Phase Recognition, and NLP for textual data on the Web, where we deal with the task of Named Entity Recognition and Disambiguation (NERD).},
  copyright = {Open Access, KITopen License},
  langid = {english},
  keywords = {Machine Learning,Markov Decision Process,Multi-Step Expert Advice,Natural Language Processing,Reinforcement Learning},
  file = {/home/xav/Zotero/storage/L3R3TKX6/Philipp - 2019 - Decision-Making with Multi-Step Expert Advice on t.pdf}
}

@misc{phuongFormalAlgorithmsTransformers2022,
  title = {Formal {{Algorithms}} for {{Transformers}}},
  author = {Phuong, Mary and Hutter, Marcus},
  year = {2022},
  month = jul,
  number = {arXiv:2207.09238},
  eprint = {2207.09238},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-28},
  abstract = {This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (*not* results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/xav/Zotero/storage/WJKYKZB4/Phuong et Hutter - 2022 - Formal Algorithms for Transformers.pdf}
}

@misc{piergiovanniAnswerMeMultiTaskOpenVocabulary2022,
  title = {Answer-{{Me}}: {{Multi-Task Open-Vocabulary Visual Question Answering}}},
  shorttitle = {Answer-{{Me}}},
  author = {Piergiovanni, A. J. and Li, Wei and Kuo, Weicheng and Saffar, Mohammad and Bertsch, Fred and Angelova, Anelia},
  year = {2022},
  month = nov,
  number = {arXiv:2205.00949},
  eprint = {arXiv:2205.00949},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {We present Answer-Me, a task-aware multi-task framework which unifies a variety of question answering tasks, such as, visual question answering, visual entailment, visual reasoning. In contrast to previous works using contrastive or generative captioning training, we propose a novel and simple recipe to pre-train a vision-language joint model, which is multi-task as well. The pre-training uses only noisy image captioning data, and is formulated to use the entire architecture end-toend with both a strong language encoder and decoder. Our results show state-of-the-art performance, zero-shot generalization, robustness to forgetting, and competitive single-task results across a variety of question answering tasks. Our multi-task mixture training learns from tasks of various question intents and thus generalizes better, including on zeroshot vision-language tasks. We conduct experiments in the challenging multi-task and open-vocabulary settings and across a variety of datasets and tasks, such as VQA2.0, SNLI-VE, NLVR2, GQA. We observe that the proposed approach is able to generalize to unseen tasks and that more diverse mixtures lead to higher accuracy in both known and novel tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/NWA72B7V/Piergiovanni et al. - 2022 - Answer-Me Multi-Task Open-Vocabulary Visual Quest.pdf}
}

@misc{piktusWebYourOyster2022,
  title = {The {{Web Is Your Oyster}} - {{Knowledge-Intensive NLP}} against a {{Very Large Web Corpus}}},
  author = {Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Okhonko, Dmytro and Broscheit, Samuel and Izacard, Gautier and Lewis, Patrick and O{\u g}uz, Barlas and Grave, Edouard and Yih, Wen-tau and Riedel, Sebastian},
  year = {2022},
  month = may,
  number = {arXiv:2112.09924},
  eprint = {arXiv:2112.09924},
  publisher = {{arXiv}},
  urldate = {2022-08-07},
  abstract = {In order to address increasing demands of real-world applications, the research for knowledge-intensive NLP (KI-NLP) should advance by capturing the challenges of a truly open-domain environment: web-scale knowledge, lack of structure, inconsistent quality and noise. To this end, we propose a new setup for evaluating existing knowledge intensive tasks in which we generalize the background corpus to a universal web snapshot. We investigate a slate of NLP tasks which rely on knowledge - either factual or common sense, and ask systems to use a subset of CCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia, otherwise a common background corpus in KI-NLP, Sphere is orders of magnitude larger and better reflects the full diversity of knowledge on the web. Despite potential gaps in coverage, challenges of scale, lack of structure and lower quality, we find that retrieval from Sphere enables a state of the art system to match and even outperform Wikipedia-based models on several tasks. We also observe that while a dense index can outperform a sparse BM25 baseline on Wikipedia, on Sphere this is not yet possible. To facilitate further research and minimise the community's reliance on proprietary, black-box search engines, we share our indices, evaluation metrics and infrastructure.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ZSVJFMC6/Piktus et al. - 2022 - The Web Is Your Oyster - Knowledge-Intensive NLP a.pdf}
}

@misc{pillutlaMAUVEMeasuringGap2021,
  title = {{{MAUVE}}: {{Measuring}} the {{Gap Between Neural Text}} and {{Human Text}} Using {{Divergence Frontiers}}},
  shorttitle = {{{MAUVE}}},
  author = {Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  year = {2021},
  month = nov,
  number = {arXiv:2102.01454},
  eprint = {arXiv:2102.01454},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.01454},
  urldate = {2023-01-22},
  abstract = {As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/5SAE2ZHX/Pillutla et al. - 2021 - MAUVE Measuring the Gap Between Neural Text and H.pdf;/home/xav/Zotero/storage/8NJNIPGR/2102.html}
}

@article{pinheiroWebExplainUPMLExtension2006,
  title = {{{WebExplain}}: {{A UPML Extension}} to {{Support}} the {{Development}} of {{Explanations}} on the {{Web}} for {{Knowledge-Based Systems}}},
  shorttitle = {{{WebExplain}}},
  author = {Pinheiro, V. and Furtado, V. and Pinheiro, Paulo and McGuinness, D.},
  year = {2006},
  journal = {undefined},
  urldate = {2022-11-15},
  abstract = {WebExplain is an extension to Unified Problem-Solving Method Description Language (UPML), a KBS development framework that is integrated with UPML generic components and can be easily reused during the development of other problem-solving methods and KBSs. Knowledge-based systems (KBS) should be able to explain their results to improve the understanding and credibility of their answers by users. However, most KBS explanation components cannot be easily reused by other applications, thus increasing the effort of implementing KBSs with explanation capabilities. In this paper we present WebExplain, an extension to Unified Problem-Solving Method Description Language (UPML), a KBS development framework. WebExplain is integrated with UPML generic components and can be easily reused during the development of other problem-solving methods and KBSs. WebExplain uses the Inference Web for enabling proof and explanation interoperability between distributed applications. We exemplify our approach by describing WebExplain's use in the development of a problem-solving method and a KBS with explanation capabilities.},
  langid = {english}
}

@misc{pipekIntroducingLMDebugger2022,
  title = {Introducing {{LM-Debugger}}},
  author = {Pipek, Mor Geva},
  year = {2022},
  month = apr,
  journal = {Medium},
  urldate = {2022-05-17},
  abstract = {An interactive tool for inspection and intervention in transformer-based language models},
  howpublished = {https://blog.allenai.org/introducing-lm-debugger-d34e94444dc2},
  langid = {english},
  file = {/home/xav/Zotero/storage/XVPCWSI8/introducing-lm-debugger-d34e94444dc2.html}
}

@misc{piReasoningProgramExecutors2022,
  title = {Reasoning {{Like Program Executors}}},
  author = {Pi, Xinyu and Liu, Qian and Chen, Bei and Ziyadi, Morteza and Lin, Zeqi and Fu, Qiang and Gao, Yan and Lou, Jian-Guang and Chen, Weizhu},
  year = {2022},
  month = oct,
  number = {arXiv:2201.11473},
  eprint = {arXiv:2201.11473},
  publisher = {{arXiv}},
  urldate = {2022-12-07},
  abstract = {Reasoning over natural language is a longstanding goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pretraining language models with programs and their execution results, POET empowers language models to harvest the reasoning knowledge possessed by program executors via a data-driven approach. POET is conceptually simple and can be instantiated by different kinds of program executors. In this paper, we showcase two simple instances POET-Math and POET-Logic, in addition to a complex instance, POET-SQL. Experimental results on six benchmarks demonstrate that POET can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on reasoningenhancement pre-training, and we hope our analysis would shed light on the future research of reasoning like program executors.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Symbolic Computation},
  file = {/home/xav/Zotero/storage/XMK5ADDE/Pi et al. - 2022 - Reasoning Like Program Executors.pdf}
}

@misc{pooleIntrinsicInteractiveReinforcement2022,
  title = {Towards {{Intrinsic Interactive Reinforcement Learning}}},
  author = {Poole, Benjamin and Lee, Minwoo},
  year = {2022},
  month = jan,
  number = {arXiv:2112.01575},
  eprint = {arXiv:2112.01575},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.01575},
  urldate = {2022-10-28},
  abstract = {Reinforcement learning (RL) and brain-computer interfaces (BCI) are two fields that have been growing over the past decade. Until recently, these fields have operated independently of one another. With the rising interest in human-in-the-loop (HITL) applications, RL algorithms have been adapted to account for human guidance giving rise to the sub-field of interactive reinforcement learning (IRL). Adjacently, BCI applications have been long interested in extracting intrinsic feedback from neural activity during human-computer interactions. These two ideas have set RL and BCI on a collision course for one another through the integration of BCI into the IRL framework where intrinsic feedback can be utilized to help train an agent. This intersection has created a new and emerging paradigm denoted as intrinsic IRL. To further help facilitate deeper ingratiation of BCI and IRL, we provide a tutorial and review of intrinsic IRL so far with an emphasis on its parent field of feedback-driven IRL along with discussions concerning validity, challenges, and open problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/QGDMEF3X/Poole et Lee - 2022 - Towards Intrinsic Interactive Reinforcement Learni.pdf}
}

@misc{popeEfficientlyScalingTransformer2022,
  title = {Efficiently {{Scaling Transformer Inference}}},
  author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  year = {2022},
  month = nov,
  number = {arXiv:2211.05102},
  eprint = {arXiv:2211.05102},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.05102},
  urldate = {2023-02-09},
  abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76\% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/RG95YV96/Pope et al. - 2022 - Efficiently Scaling Transformer Inference.pdf;/home/xav/Zotero/storage/TQSHK9U9/2211.html}
}

@article{pramanikUNIQORNUnifiedQuestion2022,
  title = {{{UNIQORN}}: {{Unified Question Answering}} over {{RDF Knowledge Graphs}} and {{Natural Language Text}}},
  shorttitle = {{{UNIQORN}}},
  author = {Pramanik, Soumajit and Alabi, Jesujoba and Roy, Rishiraj Saha and Weikum, Gerhard},
  year = {2022},
  month = apr,
  journal = {arXiv:2108.08614 [cs]},
  eprint = {2108.08614},
  primaryclass = {cs},
  urldate = {2022-05-09},
  abstract = {Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first QA system that can seamlessly operate over RDF datasets and text corpora, or both together, in a unified framework. Our method, called Uniqorn, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. Uniqorn copes with this input by a graph algorithm for Group Steiner Trees, that identifies the best answer candidates in the context graph. Experimental results on several benchmarks of complex questions with multiple entities and relations, show that Uniqorn significantly outperforms state-of-the-art methods for QA over heterogeneous sources. The graph-based methodology provides user-interpretable evidence for the complete answering process. CCS Concepts: \textbullet{} Information systems \textrightarrow{} Question answering.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/J37RDSNB/Pramanik et al. - 2022 - UNIQORN Unified Question Answering over RDF Knowl.pdf}
}

@misc{prasadGrIPSGradientfreeEditbased2022,
  title = {{{GrIPS}}: {{Gradient-free}}, {{Edit-based Instruction Search}} for {{Prompting Large Language Models}}},
  shorttitle = {{{GrIPS}}},
  author = {Prasad, Archiki and Hase, Peter and Zhou, Xiang and Bansal, Mohit},
  year = {2022},
  month = mar,
  number = {arXiv:2203.07281},
  eprint = {2203.07281},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2203.07281},
  urldate = {2022-05-22},
  abstract = {Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and requires full access to model weights, which may not be available for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. The instructions in our search are iteratively edited using four operations (delete, add, swap, paraphrase) on text at the phrase-level. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural-Instructions dataset. We see improvements for both instruction-only prompts and for k-shot example+instruction prompts. Notably, GrIPS outperforms manual rewriting following the guidelines in Mishra et al. (2022) and also outperforms purely example-based prompts while controlling for the available compute and data budget. Lastly, we provide qualitative analysis of the edited instructions across several scales of GPT models. Our code is available at: https://github.com/archiki/GrIPS},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/EA9S9KM9/Prasad et al. - 2022 - GrIPS Gradient-free, Edit-based Instruction Searc.pdf;/home/xav/Zotero/storage/5XLLU4LI/2203.html}
}

@misc{prattDesignPatternsResearch2009,
  title = {Design {{Patterns}} for {{Research Methods}} : {{Iterative Field Research}}},
  shorttitle = {Design {{Patterns}} for {{Research Methods}}},
  author = {Pratt, K.},
  year = {2009},
  urldate = {2022-09-07},
  abstract = {Two examples of this Iterative Research method will be presented: the first investigating the vehicle, interface, and team CONOPS for small Unmanned Aerial Systems used during Urban Search And Rescue (USAR) operations, and the second working to develop a multi-operator team HRI metric and robot usability evaluation method. For the last two decades the idea of design patterns has been a useful abstraction for computer scientists and programmers. As computer scientists, and scientists of all fields, are more than just programmers, we can apply the patterns concept to more than just program design. Indeed, the meta-creative processes and research methods which generate the code can also be viewed through the patterning abstraction to identify research method patterns and the contexts where they can be applied. One example of a research pattern is Iterative Research. Two examples of this Iterative Research method will be presented: the first investigating the vehicle, interface, and team CONOPS for small Unmanned Aerial Systems (sUAS) used during Urban Search And Rescue (USAR) operations, and the second working to develop a multi-operator team HRI metric and robot usability evaluation method.},
  howpublished = {https://www.semanticscholar.org/paper/Design-Patterns-for-Research-Methods-\%3A-Iterative-Pratt/9c38e6612877f66c9a2c9644a1733cc399bcdb8a},
  langid = {english},
  file = {/home/xav/Zotero/storage/FWKB6J48/Pratt - 2009 - Design Patterns for Research Methods  Iterative F.pdf}
}

@misc{pressMeasuringNarrowingCompositionality2022,
  title = {Measuring and {{Narrowing}} the {{Compositionality Gap}} in {{Language Models}}},
  author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03350},
  eprint = {arXiv:2210.03350},
  publisher = {{arXiv}},
  urldate = {2022-11-20},
  abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all subproblems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multihop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/SLVFUDR8/Press et al. - 2022 - Measuring and Narrowing the Compositionality Gap i.pdf}
}

@misc{ProblemSolvingGuidelines,
  title = {Problem {{Solving Guidelines}}},
  urldate = {2022-09-09},
  abstract = {The following guidelines are based on studies that compare experts and novices1 and use the recorded differences to help students move through the problem-solving process:  A general strategy can be summarized in terms of five steps2. Each step uses information gathered in the previous step to translate the problem into more quantitative terms.     Comprehend the problem. "What's going on?"     Getting started is the most difficult step. In this step, you need to accurately assess the situation, identify, and comprehend the problem. This enables you to decide what information is important, what information can be ignored, and what additional information may be needed.     Develop a qualitative description of the problem. Write down a simple statement of what you want to find out. Write down the engineering ideas that might be useful in the problem.     Represent the problem in formal terms. "What are the key concepts and variables?"     This step, allows you to simplify a complex problem to its essential parts, making the search for a solution easier. Your aim here is to determine the relationships between the unknown and known. Your qualitative understanding from step 1 prepares you for the quantitative solution.     Simplify the problem situation by visualizing the events described in the problem and then describing it using a sketch or diagram. Restate what you want to find by identifying the desired unknown and naming specific variables.     Plan a solution. "How are we going to solve this?"     Write down an outline of how you will solve problem to see if it will yield a reasonable solution before you go through the effort of doing any calculations. In many cases, the logical steps can be conveniently expressed mathematically. Select an equation that specifies how the variables are related.     Execute the plan. "What is our answer?"     In this step, you execute the solution you have planned. Insert all of the known quantities into the solution to determine a value.     Evaluate and interpret the solution. "Is this solution correct and reasonable?"     How well does the solution resolve the original problem? Check your work to see that it is properly stated, reasonable, and that it answers the question asked. Explain or restate the solution in terms that relate to the original problem. 1  Bransford, J.D., A.L. Brown, and R.R. Cocking (eds.), 1999, Chapter 2, How People Learn: Brain, Mind, Experience, and School. Committee on Developments in the Science of Learning, Commission on Behavioral and Social Sciences and Education, National Research Council. National Academy Press, Washington D. C. http://books.nap.edu/html/howpeople1/ 2 Adapted from: Physics Education Research and Development, University of Minnesota, Cooperative Group Problems Solving, http://www.physics.umn.edu/groups/physed/Research/CGPS/CGPSintro.htm . See also: "The A thru E Approach to Problem Solving in Chemistry" by D. Woodcock, http://people.ouc.bc.ca/woodcock/probsol/ps\_A-E.html .},
  howpublished = {https://courses.cs.washington.edu/courses/cse473/03au/Lectures/problem\_solving\_guidelines.htm},
  file = {/home/xav/Zotero/storage/7GJ4S9QM/problem_solving_guidelines.html}
}

@misc{pruksachatkunIntermediateTaskTransferLearning2020,
  title = {Intermediate-{{Task Transfer Learning}} with {{Pretrained Models}} for {{Natural Language Understanding}}: {{When}} and {{Why Does It Work}}?},
  shorttitle = {Intermediate-{{Task Transfer Learning}} with {{Pretrained Models}} for {{Natural Language Understanding}}},
  author = {Pruksachatkun, Yada and Phang, Jason and Liu, Haokun and Htut, Phu Mon and Zhang, Xiaoyi and Pang, Richard Yuanzhe and Vania, Clara and Kann, Katharina and Bowman, Samuel R.},
  year = {2020},
  month = may,
  number = {arXiv:2005.00628},
  eprint = {arXiv:2005.00628},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.00628},
  urldate = {2023-02-02},
  abstract = {While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/XML2PYE6/Pruksachatkun et al. - 2020 - Intermediate-Task Transfer Learning with Pretraine.pdf;/home/xav/Zotero/storage/69X6B9H3/2005.html}
}

@misc{PSMLibrarianProtege2002,
  title = {{{PSM Librarian}} - {{Protege Wiki}}},
  year = {2002},
  urldate = {2022-05-14},
  abstract = {The PSM Librarian supports you in building a knowledge-based application out of reusable knowledge components known as Problem-Solving Methods (PSMs). PSMs are generic, formalized and implemented algorithms that encode domain-independent strategies to realize common cognitive tasks (known as generic tasks, e.g., classification, diagnosis, design) by processing domain knowledge. As such, PSMs serve as reasoning building blocks (often collected in libraries) that can be assembled with a domain knowledge base to create a running system. The PSM Librarian tab plug-in for Protege serves as a front-end interface between one of your domain projects and a set of PSM libraries (modeled with the UPML language). The PSM Librarian allows you to browse several PSM libraries in a uniform way and thus to make an informed selection of a PSM that suits your domain task and knowledge. The PSM Librarian then allows you to configure a PSM to fit the specifics of your domain application, by creating interactively a set of mapping relations between the entities of your domain and the input-output entities specified by the PSM. The PSM Librarian provides an integrated and simultaneous support for you to view and manage all three Prot\'eg\'e projects involved in the configuration process (domain, method and mappings). The PSM Librarian includes an interpreter that processes those mapping relations to re-formulate your domain knowledge in terms that are appropriate for activating the PSM. (Note that the current version of the PSM Librarian tab does not support the actual activation of the configured PSM with case data.)},
  howpublished = {https://protegewiki.stanford.edu/wiki/PSM\_Librarian},
  file = {/home/xav/Zotero/storage/CMGHE8H8/PSM_Librarian.html}
}

@inproceedings{puDeepKnowledgeTracing2020,
  title = {Deep {{Knowledge Tracing}} with {{Transformers}}},
  booktitle = {Artificial {{Intelligence}} in {{Education}}},
  author = {Pu, Shi and Yudelson, Michael and Ou, Lu and Huang, Yuchi},
  editor = {Bittencourt, Ig Ibert and Cukurova, Mutlu and Muldner, Kasia and Luckin, Rose and Mill{\'a}n, Eva},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {252--256},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-52240-7_46},
  abstract = {In this work, we propose a Transformer-based model to trace students' knowledge acquisition. We modified the Transformer structure to utilize 1) the association between questions and skills and 2) the elapsed time between question steps. The use of question-skill associations allows the model to learn specific representation for frequently encountered questions while representing rare questions with their underline skill representations. The inclusion of elapsed time opens the opportunity to address forgetting. Our approach outperforms the state-of-the-art methods in the literature by roughly 10\% in AUC with frequently used public datasets.},
  isbn = {978-3-030-52240-7},
  langid = {english},
  keywords = {Bayesian Knowledge Tracing,Deep Knowledge Tracing,Transformer},
  file = {/home/xav/Zotero/storage/VZ4FLETM/Pu et al. - 2020 - Deep Knowledge Tracing with Transformers.pdf}
}

@misc{pyatkinReinforcedClarificationQuestion2022,
  title = {Reinforced {{Clarification Question Generation}} with {{Defeasibility Rewards}} for {{Disambiguating Social}} and {{Moral Situations}}},
  author = {Pyatkin, Valentina and Hwang, Jena D. and Srikumar, Vivek and Lu, Ximing and Jiang, Liwei and Choi, Yejin and Bhagavatula, Chandra},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10409},
  eprint = {arXiv:2212.10409},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10409},
  urldate = {2023-02-07},
  abstract = {Context is vital for commonsense moral reasoning. "Lying to a friend" is wrong if it is meant to deceive them, but may be morally okay if it is intended to protect them. Such nuanced but salient contextual information can potentially flip the moral judgment of an action. Thus, we present ClarifyDelphi, an interactive system that elicits missing contexts of a moral situation by generating clarification questions such as "Why did you lie to your friend?". Our approach is inspired by the observation that questions whose potential answers lead to diverging moral judgments are the most informative. We learn to generate questions using Reinforcement Learning, by maximizing the divergence between moral judgements of hypothetical answers to a question. Human evaluation shows that our system generates more relevant, informative and defeasible questions compared to other question generation baselines. ClarifyDelphi assists informed moral reasoning processes by seeking additional morally consequential context to disambiguate social and moral situations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/7NQJCYGU/Pyatkin et al. - 2022 - Reinforced Clarification Question Generation with .pdf;/home/xav/Zotero/storage/QU2M7PTD/2212.html}
}

@misc{qiaoReasoningLanguageModel2022,
  title = {Reasoning with {{Language Model Prompting}}: {{A Survey}}},
  shorttitle = {Reasoning with {{Language Model Prompting}}},
  author = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09597},
  eprint = {arXiv:2212.09597},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/H24VV7PG/Qiao et al. - 2022 - Reasoning with Language Model Prompting A Survey.pdf}
}

@misc{qinExploringUniversalIntrinsic2022,
  title = {Exploring {{Universal Intrinsic Task Subspace}} via {{Prompt Tuning}}},
  author = {Qin, Yujia and Wang, Xiaozhi and Su, Yusheng and Lin, Yankai and Ding, Ning and Yi, Jing and Chen, Weize and Liu, Zhiyuan and Li, Juanzi and Hou, Lei and Li, Peng and Sun, Maosong and Zhou, Jie},
  year = {2022},
  month = nov,
  number = {arXiv:2110.07867},
  eprint = {arXiv:2110.07867},
  publisher = {{arXiv}},
  urldate = {2023-02-08},
  abstract = {Why can pre-trained language models (PLMs) learn universal representations and effectively adapt to broad NLP tasks differing a lot superficially? In this work, we empirically find evidence indicating that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parameters in a unified low-dimensional intrinsic task subspace, which may help us understand why PLMs could easily adapt to various NLP tasks with small-scale data. To find such a subspace and examine its universality, we propose an analysis pipeline called intrinsic prompt tuning (IPT). Specifically, we resort to the recent success of prompt tuning and decompose the soft prompts of multiple NLP tasks into the same low-dimensional nonlinear subspace, then we learn to adapt the PLM to unseen data or tasks by only tuning parameters in this subspace. In the experiments, we study diverse few-shot NLP tasks and surprisingly find that in a 250-dimensional subspace found with 100 tasks, by only tuning 250 free parameters, we can recover 97\% and 83\% of the full prompt tuning performance for 100 seen tasks (using different training data) and 20 unseen tasks, respectively, showing great generalization ability of the found intrinsic task subspace. Besides being an analysis tool, IPT could further help us improve the prompt tuning stability.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/NIYMHX4N/Qin et al. - 2022 - Exploring Universal Intrinsic Task Subspace via Pr.pdf}
}

@misc{qinKnowledgeInheritancePretrained2022,
  title = {Knowledge {{Inheritance}} for {{Pre-trained Language Models}}},
  author = {Qin, Yujia and Lin, Yankai and Yi, Jing and Zhang, Jiajie and Han, Xu and Zhang, Zhengyan and Su, Yusheng and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  year = {2022},
  month = apr,
  number = {arXiv:2105.13880},
  eprint = {arXiv:2105.13880},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Recent explorations of large-scale pre-trained language models (PLMs) have revealed the power of PLMs with huge amounts of parameters, setting off a wave of training everlarger PLMs. However, it requires tremendous computational resources to train a largescale PLM, which may be practically unaffordable. In addition, existing large-scale PLMs are mainly trained from scratch individually, ignoring that many well-trained PLMs are available. To this end, we explore the question how could existing PLMs benefit training large-scale PLMs in future. Specifically, we introduce a pre-training framework named ``knowledge inheritance'' (KI) and explore how could knowledge distillation serve as auxiliary supervision during pre-training to efficiently learn larger PLMs. Experimental results demonstrate the superiority of KI in training efficiency. We also conduct empirical analyses to explore the effects of teacher PLMs' pre-training settings, including model architecture, pre-training data, etc. Finally, we show that KI could be applied to domain adaptation and knowledge transfer. The implementation is publicly available at https://github.com/thunlp/ Knowledge-Inheritance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/VD4ETW3W/Qin et al. - 2022 - Knowledge Inheritance for Pre-trained Language Mod.pdf}
}

@misc{qinT5ScoreDiscriminativeFinetuning2022,
  title = {{{T5Score}}: {{Discriminative Fine-tuning}} of {{Generative Evaluation Metrics}}},
  shorttitle = {{{T5Score}}},
  author = {Qin, Yiwei and Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  year = {2022},
  month = dec,
  number = {arXiv:2212.05726},
  eprint = {arXiv:2212.05726},
  publisher = {{arXiv}},
  urldate = {2023-01-31},
  abstract = {Modern embedding-based metrics for evaluation of generated text generally fall into one of two paradigms: discriminative metrics that are trained to directly predict which outputs are of higher quality according to supervised human annotations, and generative metrics that are trained to evaluate text based on the probabilities of a generative model. Both have their advantages; discriminative metrics are able to directly optimize for the problem of distinguishing between good and bad outputs, while generative metrics can be trained using abundant raw text. In this paper, we present a framework that combines the best of both worlds, using both supervised and unsupervised signals from whatever data we have available. We operationalize this idea by training T5SCORE, a metric that uses these training signals with mT5 as backbone1 We perform an extensive empirical comparison with other existing metrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility of our method.2 Experimental results show that: T5SCOREachieves the best performance on all datasets against existing topscoring metrics at the segment level. We release our code and models at https:// github.com/qinyiwei/T5Score.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/ZPBVPF2Z/Qin et al. - 2022 - T5Score Discriminative Fine-tuning of Generative .pdf}
}

@misc{qinTIMEDIALTemporalCommonsense2021,
  title = {{{TIMEDIAL}}: {{Temporal Commonsense Reasoning}} in {{Dialog}}},
  shorttitle = {{{TIMEDIAL}}},
  author = {Qin, Lianhui and Gupta, Aditya and Upadhyay, Shyam and He, Luheng and Choi, Yejin and Faruqui, Manaal},
  year = {2021},
  month = jun,
  number = {arXiv:2106.04571},
  eprint = {arXiv:2106.04571},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.04571},
  urldate = {2023-02-01},
  abstract = {Everyday conversations require understanding everyday events, which in turn, requires understanding temporal commonsense concepts interwoven with those events. Despite recent progress with massive pre-trained language models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains largely under-explored. In this paper, we present the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TIMEDIAL. We formulate TIME-DIAL as a multiple-choice cloze task with over 1.1K carefully curated dialogs. Empirical results demonstrate that even the best performing models struggle on this task compared to humans, with 23 absolute points of gap in accuracy. Furthermore, our analysis reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context, motivating future research for modeling temporal concepts in text and robust contextual reasoning about them. The dataset is publicly available at: https://github.com/google-research-datasets/timedial.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/DXMFWTNZ/Qin et al. - 2021 - TIMEDIAL Temporal Commonsense Reasoning in Dialog.pdf;/home/xav/Zotero/storage/R4V85JED/2106.html}
}

@misc{qiTegFormerTopictoEssayGeneration2022,
  title = {{{TegFormer}}: {{Topic-to-Essay Generation}} with {{Good Topic Coverage}} and {{High Text Coherence}}},
  shorttitle = {{{TegFormer}}},
  author = {Qi, Wang and Liu, Rui and Zuo, Yuan and Chen, Yong and Zhang, Dell},
  year = {2022},
  month = dec,
  number = {arXiv:2212.13456},
  eprint = {arXiv:2212.13456},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.13456},
  urldate = {2023-01-03},
  abstract = {Creating an essay based on a few given topics is a challenging NLP task. Although several effective methods for this problem, topic-to-essay generation, have appeared recently, there is still much room for improvement, especially in terms of the coverage of the given topics and the coherence of the generated text. In this paper, we propose a novel approach called TegFormer which utilizes the Transformer architecture where the encoder is enriched with domain-specific contexts while the decoder is enhanced by a large-scale pre-trained language model. Specifically, a \textbackslash emph\{Topic-Extension\} layer capturing the interaction between the given topics and their domain-specific contexts is plugged into the encoder. Since the given topics are usually concise and sparse, such an additional layer can bring more topic-related semantics in to facilitate the subsequent natural language generation. Moreover, an \textbackslash emph\{Embedding-Fusion\} module that combines the domain-specific word embeddings learnt from the given corpus and the general-purpose word embeddings provided by a GPT-2 model pre-trained on massive text data is integrated into the decoder. Since GPT-2 is at a much larger scale, it contains a lot more implicit linguistic knowledge which would help the decoder to produce more grammatical and readable text. Extensive experiments have shown that the pieces of text generated by TegFormer have better topic coverage and higher text coherence than those from SOTA topic-to-essay techniques, according to automatic and human evaluations. As revealed by ablation studies, both the Topic-Extension layer and the Embedding-Fusion module contribute substantially to TegFormer's performance advantage.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/S6QSIC5E/Qi et al. - 2022 - TegFormer Topic-to-Essay Generation with Good Top.pdf}
}

@inproceedings{quinnHumanComputationSurvey2011,
  title = {Human Computation: A Survey and Taxonomy of a Growing Field},
  shorttitle = {Human Computation},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Quinn, Alexander J. and Bederson, Benjamin B.},
  year = {2011},
  month = may,
  series = {{{CHI}} '11},
  pages = {1403--1412},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1978942.1979148},
  urldate = {2022-10-03},
  abstract = {The rapid growth of human computation within research and industry has produced many novel ideas aimed at organizing web users to do great things. However, the growth is not adequately supported by a framework with which to understand each new system in the context of the old. We classify human computation systems to help identify parallels between different systems and reveal "holes" in the existing work as opportunities for new research. Since human computation is often confused with "crowdsourcing" and other terms, we explore the position of human computation with respect to these related topics.},
  isbn = {978-1-4503-0228-9},
  keywords = {crowdsourcing,data mining,human computation,literature review,social computing,survey,taxonomy},
  file = {/home/xav/Zotero/storage/T769VVCC/Quinn et Bederson - 2011 - Human computation a survey and taxonomy of a grow.pdf}
}

@inproceedings{quNaturalLanguageUnderstanding2021,
  title = {Natural {{Language Understanding}} with {{Privacy-Preserving BERT}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Qu, Chen and Kong, Weize and Yang, Liu and Zhang, Mingyang and Bendersky, Michael and Najork, Marc},
  year = {2021},
  month = oct,
  eprint = {2104.07504},
  primaryclass = {cs},
  pages = {1488--1497},
  doi = {10.1145/3459637.3482281},
  urldate = {2023-02-08},
  abstract = {Privacy preservation remains a key challenge in data mining and Natural Language Understanding (NLU). Previous research shows that the input text or even text embeddings can leak private information. This concern motivates our research on effective privacy preservation approaches for pretrained Language Models (LMs). We investigate the privacy and utility implications of applying {$\mathsl{d}$} {$\mathsl{X}$}-privacy, a variant of Local Differential Privacy, to BERT finetuning in NLU applications. More importantly, we further propose privacy-adaptive LM pretraining methods and show that our approach can boost the utility of BERT dramatically while retaining the same level of privacy protection. We also quantify the level of privacy preservation and provide guidance on privacy configuration. Our experiments and findings lay the groundwork for future explorations of privacy-preserving NLU with pretrained LMs.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/JYNKYTSW/Qu et al. - 2021 - Natural Language Understanding with Privacy-Preser.pdf}
}

@article{quPrivacyAdaptiveBERTNatural2021,
  title = {Privacy-{{Adaptive BERT}} for {{Natural Language Understanding}}},
  author = {Qu, Chen and Kong, Weize and Yang, Liu and Zhang, Mingyang and Bendersky, Michael and Najork, Marc-Alexander},
  year = {2021},
  journal = {ArXiv},
  urldate = {2023-02-08},
  abstract = {When trying to apply the recent advance of Natural Language Understanding (NLU) technologies to real-world applications, privacy preservation imposes a crucial challenge, which, unfortunately, has not been well resolved. To address this issue, we study how to improve the effectiveness of NLUmodels under a Local Privacy setting, using BERT [9], a widely-used pretrained Language Model (LM), as an example. We systematically study the strengths and weaknesses of imposing d{$\chi$}-privacy [12], a relaxed variant of Local Differential Privacy, at different stages of language modeling: input text, token embeddings, and sequence representations. We then focus on the former two with privacy-constrained fine-tuning experiments to reveal the utility of BERT under local privacy constraints. More importantly, to the best of our knowledge, we are the first to propose privacy-adaptive LM pretraining methods and demonstrate that they can significantly improve model performance on privatized text input. We also interpret the level of privacy preservation and provide our guidance on privacy parameter selections.},
  file = {/home/xav/Zotero/storage/W9JTRQR8/Qu et al. - 2021 - Privacy-Adaptive BERT for Natural Language Underst.pdf}
}

@article{raeScalingLanguageModels2022,
  title = {Scaling {{Language Models}}: {{Methods}}, {{Analysis}} \& {{Insights}} from {{Training Gopher}}},
  shorttitle = {Scaling {{Language Models}}},
  author = {Rae, Jack W. and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and Rutherford, Eliza and Hennigan, Tom and Menick, Jacob and Cassirer, Albin and Powell, Richard and van den Driessche, George and Hendricks, Lisa Anne and Rauh, Maribeth and Huang, Po-Sen and Glaese, Amelia and Welbl, Johannes and Dathathri, Sumanth and Huang, Saffron and Uesato, Jonathan and Mellor, John and Higgins, Irina and Creswell, Antonia and McAleese, Nat and Wu, Amy and Elsen, Erich and Jayakumar, Siddhant and Buchatskaya, Elena and Budden, David and Sutherland, Esme and Simonyan, Karen and Paganini, Michela and Sifre, Laurent and Martens, Lena and Li, Xiang Lorraine and Kuncoro, Adhiguna and Nematzadeh, Aida and Gribovskaya, Elena and Donato, Domenic and Lazaridou, Angeliki and Mensch, Arthur and Lespiau, Jean-Baptiste and Tsimpoukelli, Maria and Grigorev, Nikolai and Fritz, Doug and Sottiaux, Thibault and Pajarskas, Mantas and Pohlen, Toby and Gong, Zhitao and Toyama, Daniel and {d'Autume}, Cyprien de Masson and Li, Yujia and Terzi, Tayfun and Mikulik, Vladimir and Babuschkin, Igor and Clark, Aidan and Casas, Diego de Las and Guy, Aurelia and Jones, Chris and Bradbury, James and Johnson, Matthew and Hechtman, Blake and Weidinger, Laura and Gabriel, Iason and Isaac, William and Lockhart, Ed and Osindero, Simon and Rimell, Laura and Dyer, Chris and Vinyals, Oriol and Ayoub, Kareem and Stanway, Jeff and Bennett, Lorrayne and Hassabis, Demis and Kavukcuoglu, Koray and Irving, Geoffrey},
  year = {2022},
  month = jan,
  journal = {arXiv:2112.11446 [cs]},
  eprint = {2112.11446},
  primaryclass = {cs},
  urldate = {2022-03-29},
  abstract = {Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {QID: Q110802987},
  file = {/home/xav/Zotero/storage/F8ZKB7U3/Rae et al. - 2022 - Scaling Language Models Methods, Analysis & Insig.pdf;/home/xav/Zotero/storage/9JDKWAWF/2112.html}
}

@misc{raffelExploringLimitsTransfer2020,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text-to-Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.10683},
  eprint = {arXiv:1910.10683},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.10683},
  urldate = {2023-02-02},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/K24XGJXR/Raffel et al. - 2020 - Exploring the Limits of Transfer Learning with a U.pdf;/home/xav/Zotero/storage/H7EGKJ7K/1910.html}
}

@article{RaisonnementPartirCas2021,
  title = {{Raisonnement \`a partir de cas}},
  year = {2021},
  month = may,
  journal = {Wikip\'edia},
  urldate = {2022-05-09},
  abstract = {Le raisonnement \`a partir de cas (R\`aPC) (nomm\'e en anglais case-based reasoning (CBR)) est un type de raisonnement qui copie le comportement humain qui consiste \`a faire naturellement appel \`a l'exp\'erience pour r\'esoudre les probl\`emes de la vie quotidienne, en se rem\'emorant les situations semblables d\'ej\`a rencontr\'ees et en les comparant \`a la situation actuelle pour construire une nouvelle solution qui, \`a son tour, s'ajoutera \`a l'exp\'erience. Ce type de raisonnement r\'esout les probl\`emes en retrouvant des cas analogues dans sa base de connaissances et en les adaptant au cas consid\'er\'e. Cette technologie est apparue il y a une quinzaine d'ann\'ees[Quand ?] mais les travaux initiaux sur le sujet remontent cependant aux exp\'eriences de Schank et Abelson en 1977 \`a l'Universit\'e Yale. Elle reste pourtant encore assez m\'econnue par rapport \`a d'autres technologies appartenant au domaine des sciences cognitives comme le data mining. Elle diff\`ere de cette derni\`ere par son approche. En effet, ici, on n'utilise qu'indirectement les donn\'ees pour retrouver les cas proches, \`a partir desquels on va g\'en\'erer une solution.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 183331357},
  file = {/home/xav/Zotero/storage/XQ7MYLHI/Raisonnement_√†_partir_de_cas.html}
}

@misc{rajagopalTemplateFillingControllable2022,
  title = {Template {{Filling}} for {{Controllable Commonsense Reasoning}}},
  author = {Rajagopal, Dheeraj and Khetan, Vivek and Sacaleanu, Bogdan and Gershman, Anatole and Fano, Andrew and Hovy, Eduard},
  year = {2022},
  month = oct,
  number = {arXiv:2111.00539},
  eprint = {arXiv:2111.00539},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.00539},
  urldate = {2023-02-07},
  abstract = {Large-scale sequence-to-sequence models have shown to be adept at both multiple-choice and open-domain commonsense reasoning tasks. However, the current systems do not provide the ability to control the various attributes of the reasoning chain. To enable better controllability, we propose to study the commonsense reasoning as a template filling task (TemplateCSR) -- where the language models fills reasoning templates with the given constraints as control factors. As an approach to TemplateCSR, we (i) propose a dataset of commonsense reasoning template-expansion pairs and (ii) introduce POTTER, a pretrained sequence-to-sequence model using prompts to perform commonsense reasoning across concepts. Our experiments show that our approach outperforms baselines both in generation metrics and factuality metrics. We also present a detailed error analysis on our approach's ability to reliably perform commonsense reasoning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/8SAGAZR7/Rajagopal et al. - 2022 - Template Filling for Controllable Commonsense Reas.pdf;/home/xav/Zotero/storage/IQ7DXLJG/2111.html}
}

@misc{rajpurkarKnowWhatYou2018,
  title = {Know {{What You Don}}'t {{Know}}: {{Unanswerable Questions}} for {{SQuAD}}},
  shorttitle = {Know {{What You Don}}'t {{Know}}},
  author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  year = {2018},
  month = jun,
  number = {arXiv:1806.03822},
  eprint = {arXiv:1806.03822},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.03822},
  urldate = {2023-01-23},
  abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/RJQZHDTI/Rajpurkar et al. - 2018 - Know What You Don't Know Unanswerable Questions f.pdf;/home/xav/Zotero/storage/NIEJLVJN/1806.html}
}

@inproceedings{rajpurkarSQuAD1000002016,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  booktitle = {Proceedings of the 2016 {{Conference}} on {{Empirical Methods}} in {{Natural}}           {{Language Processing}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year = {2016},
  pages = {2383--2392},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, Texas}},
  doi = {10.18653/v1/D16-1264},
  urldate = {2023-01-23},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.},
  langid = {english},
  file = {/home/xav/Zotero/storage/532SP9GV/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf}
}

@inproceedings{ramInContextRetrievalAugmentedLanguage2023,
  title = {In-{{Context Retrieval-Augmented Language Models}}},
  author = {Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and {Leyton-Brown}, Kevin and Shoham, Yoav},
  year = {2023},
  month = jan,
  urldate = {2023-02-03},
  abstract = {Retrieval-Augmented Language Modeling (RALM) methods, that condition a language model (LM) on relevant documents from a grounding corpus during generation, have been shown to significantly improve language modeling while also providing a natural source attribution mechanism. Existing RALM approaches focus on modifying the LM architecture in order to facilitate the incorporation of external information, significantly complicating deployment. This paper proposes an under-explored alternative, which we dub In-Context RALM : leaving the LM architecture unchanged and prepending grounding documents to the input. We show that in-context RALM which uses off-the-shelf general purpose retrievers provides surprisingly large LM gains across model sizes and diverse corpora. We also demonstrate that the document retrieval and ranking mechanism can be specialized to the RALM setting to further boost performance. We conclude that in-context RALM has considerable potential to increase the prevalence of LM grounding, particularly in settings where a pretrained LM must be used without modification or even via API access. To that end, we make our code publicly available. 1},
  file = {/home/xav/Zotero/storage/T9GQSM5V/Ram et al. - 2023 - In-Context Retrieval-Augmented Language Models.pdf}
}

@inproceedings{rashkinEvent2MindCommonsenseInference2018,
  title = {{{Event2Mind}}: {{Commonsense Inference}} on {{Events}}, {{Intents}}, and {{Reactions}}},
  shorttitle = {{{Event2Mind}}},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Rashkin, Hannah and Sap, Maarten and Allaway, Emily and Smith, Noah A. and Choi, Yejin},
  year = {2018},
  month = jul,
  pages = {463--473},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1043},
  urldate = {2023-02-01},
  abstract = {We investigate a new commonsense inference task: given an event described in a short free-form text (``X drinks coffee in the morning''), a system reasons about the likely intents (``X wants to stay awake'') and reactions (``X feels alert'') of the event's participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people's intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.},
  file = {/home/xav/Zotero/storage/LFVJPLJ7/Rashkin et al. - 2018 - Event2Mind Commonsense Inference on Events, Inten.pdf}
}

@article{rasovskaContributionMethodologieCapitalisation2006,
  title = {Contribution \`a Une M\'ethodologie de Capitalisation Des Connaissances Bas\'ee Sur Le Raisonnement \`a Partir de Cas : {{Application}} Au Diagnostic Dans Une Plateforme d'e-Maintenance.},
  shorttitle = {Contribution \`a Une M\'ethodologie de Capitalisation Des Connaissances Bas\'ee Sur Le Raisonnement \`a Partir de Cas},
  author = {Rasovska, Ivana},
  year = {2006},
  journal = {undefined},
  urldate = {2022-05-07},
  abstract = {Face aux evolutions technologiques, a la complexite croissante des equipements industriels et a la dynamique des processus ainsi qu\&\#39;aux changements organisationnels et a la mobilite du personnel, les responsables de maintenance cherchent a formaliser et perenniser le savoir et le savoir-faire des employes. Pour repondre a cette problematique, notre objectif est de fournir un service d\&\#39;aide a la maintenance qui utilise et capitalise les connaissances. Nos travaux se situent dans le cadre du projet Europeen Proteus qui a permis de developper une plateforme distribuee d\&\#39;e-maintenance integrant les differents systemes et applications existants en maintenance. Nous avons determine quatre niveaux d\&\#39;applications associes chacun a un ensemble d\&\#39;outils d\&\#39;aide : le niveau d\&\#39;analyse d\&\#39;equipement, celui de diagnostic et d\&\#39;expertise, celui de gestion des ressources et celui de gestion des strategies de maintenance. Ces outils d\&\#39;aide necessitent une expertise capitalisee que nous proposons de preserver dans une memoire d\&\#39;entreprise. Afin d\&\#39;elaborer la memoire ainsi que notre outil d\&\#39;aide au diagnostic et a la reparation, nous avons introduit une demarche de capitalisation des connaissances articulee autour d\&\#39;une methodologie de raisonnement a partir de cas (RaPC) guide par les connaissances. La conception de l\&\#39;outil passe par la modelisation des connaissances qui se decline en un modele de representation (une ontologie du domaine) et en un modele de resolution de problemes (RaPC). Les modeles proposes utilisent des technologies emergeantes du Web semantique permettant de faire evoluer le concept d\&\#39;emaintenance vers un nouveau concept de s-maintenance (maintenance semantique).},
  langid = {english},
  file = {/home/xav/Zotero/storage/CQ83B3HG/Rasovska - 2006 - Contribution √† une m√©thodologie de capitalisation .pdf;/home/xav/Zotero/storage/EDHJGPAE/7c6da254c6817c4cf6af86f0f69092c2b22a42ff.html}
}

@misc{razdaibiedinaProgressivePromptsContinual2023,
  title = {Progressive {{Prompts}}: {{Continual Learning}} for {{Language Models}}},
  shorttitle = {Progressive {{Prompts}}},
  author = {Razdaibiedina, Anastasia and Mao, Yuning and Hou, Rui and Khabsa, Madian and Lewis, Mike and Almahairi, Amjad},
  year = {2023},
  month = jan,
  number = {arXiv:2301.12314},
  eprint = {arXiv:2301.12314},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.12314},
  urldate = {2023-03-24},
  abstract = {We introduce Progressive Prompts - a simple and efficient approach for continual learning in language models. Our method allows forward transfer and resists catastrophic forgetting, without relying on data replay or a large number of task-specific parameters. Progressive Prompts learns a new soft prompt for each task and sequentially concatenates it with the previously learned prompts, while keeping the base model frozen. Experiments on standard continual learning benchmarks show that our approach outperforms state-of-the-art methods, with an improvement {$>$}20\% in average test accuracy over the previous best-preforming method on T5 model. We also explore a more challenging continual learning setup with longer sequences of tasks and show that Progressive Prompts significantly outperforms prior methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/TUGEJZMR/Razdaibiedina et al. - 2023 - Progressive Prompts Continual Learning for Langua.pdf;/home/xav/Zotero/storage/HDRRX8AF/2301.html}
}

@misc{reedGeneralistAgent2022a,
  title = {A {{Generalist Agent}}},
  author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and {Barth-Maron}, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and {de Freitas}, Nando},
  year = {2022},
  month = may,
  number = {arXiv:2205.06175},
  eprint = {arXiv:2205.06175},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.06175},
  urldate = {2022-10-29},
  abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/xav/Zotero/storage/Q6BZB3R8/Reed et al. - 2022 - A Generalist Agent.pdf;/home/xav/Zotero/storage/33I4WTEI/2205.html}
}

@misc{reidCanWikipediaHelp2022,
  title = {Can {{Wikipedia Help Offline Reinforcement Learning}}?},
  author = {Reid, Machel and Yamada, Yutaro and Gu, Shixiang Shane},
  year = {2022},
  month = jul,
  number = {arXiv:2201.12122},
  eprint = {arXiv:2201.12122},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Fine-tuning reinforcement learning (RL) models has been challenging because of a lack of large scale off-the-shelf datasets as well as high variance in transferability among different environments. Recent work has looked at tackling offline RL from the perspective of sequence modeling with improved results as result of the introduction of the Transformer architecture. However, when the model is trained from scratch, it suffers from slow convergence speeds. In this paper, we look to take advantage of this formulation of reinforcement learning as sequence modeling and investigate the transferability of pre-trained sequence models on other domains (vision, language) when finetuned on offline RL tasks (control, games). To this end, we also propose techniques to improve transfer between these domains. Results show consistent performance gains in terms of both convergence speed and reward on a variety of environments, accelerating training by 3-6x and achieving state-of-the-art performance in a variety of tasks using Wikipedia-pretrained and GPT2 language models. We hope that this work not only brings light to the potentials of leveraging generic sequence modeling techniques and pre-trained models for RL, but also inspires future work on sharing knowledge between generative modeling tasks of completely different domains.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/FBVXU8HA/Reid et al. - 2022 - Can Wikipedia Help Offline Reinforcement Learning.pdf}
}

@misc{reifRecipeArbitraryText2022,
  title = {A {{Recipe For Arbitrary Text Style Transfer}} with {{Large Language Models}}},
  author = {Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy and {Callison-Burch}, Chris and Wei, Jason},
  year = {2022},
  month = mar,
  number = {arXiv:2109.03910},
  eprint = {arXiv:2109.03910},
  publisher = {{arXiv}},
  urldate = {2022-06-02},
  abstract = {In this paper, we leverage large language models (LMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as "make this melodramatic" or "insert a metaphor."},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/T9NZR563/Reif et al. - 2022 - A Recipe For Arbitrary Text Style Transfer with La.pdf}
}

@article{renSMOREKnowledgeGraph2021,
  title = {{{SMORE}}: {{Knowledge Graph Completion}} and {{Multi-hop Reasoning}} in {{Massive Knowledge Graphs}}},
  shorttitle = {{{SMORE}}},
  author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Zhou, Denny and Leskovec, Jure and Schuurmans, Dale},
  year = {2021},
  month = nov,
  journal = {arXiv:2110.14890 [cs]},
  eprint = {2110.14890},
  primaryclass = {cs},
  urldate = {2022-05-09},
  abstract = {Knowledge graphs (KGs) capture knowledge in the form of head\textendash relation\textendash tail triples and are a crucial component in many AI systems. There are two important reasoning tasks on KGs: (1) single-hop knowledge graph completion, which involves predicting individual links in the KG; and (2), multi-hop reasoning, where the goal is to predict which KG entities satisfy a given logical query. Embedding-based methods solve both tasks by first computing an embedding for each entity and relation, then using them to form predictions. However, existing scalable KG embedding frameworks only support single-hop knowledge graph completion and cannot be applied to the more challenging multi-hop reasoning task. Here we present Scalable Multi-hOp REasoning (SMORE), the first general framework for both single-hop and multi-hop reasoning in KGs. Using a single machine SMORE can perform multi-hop reasoning in Freebase KG (86M entities, 338M edges), which is 1,500\texttimes{} larger than previously considered KGs. The key to SMORE's runtime performance is a novel bidirectional rejection sampling that achieves a square root reduction of the complexity of online training data generation. Furthermore, SMORE exploits asynchronous scheduling, overlapping CPU-based data sampling, GPU-based embedding computation, and frequent CPU\textendash GPU IO. SMORE increases throughput (i.e., training speed) over prior multi-hop KG frameworks by 2.2\texttimes{} with minimal GPU memory requirements (2GB for training 400-dim embeddings on 86M-node Freebase) and achieves near linear speed-up with the number of GPUs. Moreover, on the simpler single-hop knowledge graph completion task SMORE achieves comparable or even better runtime performance to state-of-the-art frameworks on both single GPU and multi-GPU settings.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/B8MTCPAY/Ren et al. - 2021 - SMORE Knowledge Graph Completion and Multi-hop Re.pdf}
}

@misc{reppertIteratedDecompositionImproving2023,
  title = {Iterated {{Decomposition}}: {{Improving Science Q}}\&{{A}} by {{Supervising Reasoning Processes}}},
  shorttitle = {Iterated {{Decomposition}}},
  author = {Reppert, Justin and Rachbach, Ben and George, Charlie and Stebbing, Luke and Byun, Jungwon and Appleton, Maggie and Stuhlm{\"u}ller, Andreas},
  year = {2023},
  month = jan,
  number = {arXiv:2301.01751},
  eprint = {arXiv:2301.01751},
  publisher = {{arXiv}},
  urldate = {2023-02-07},
  abstract = {Language models (LMs) can perform complex reasoning either end-to-end, with hidden latent state, or compositionally, with transparent intermediate state. Composition offers benefits for interpretability and safety, but may need workflow support and infrastructure to remain competitive. We describe iterated decomposition, a human-in-the-loop workflow for developing and refining compositional LM programs. We improve the performance of compositions by zooming in on failing components and refining them through decomposition, additional context, chain of thought, etc. To support this workflow, we develop ICE, an open-source tool for visualizing the execution traces of LM programs. We apply iterated decomposition to three real-world tasks and improve the accuracy of LM programs over less compositional baselines on held-out test sets: describing the placebo used in a randomized controlled trial (25\% \textrightarrow{} 65\%), evaluating participant adherence to a medical intervention (53\% \textrightarrow{} 70\%), and answering NLP questions on the QASPER dataset (38\% \textrightarrow{} 69\%). These applications serve as case studies for a workflow that, if automated, could keep ML systems interpretable and safe even as they scale to increasingly complex tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/home/xav/Zotero/storage/GRAZ5NHJ/Reppert et al. - 2023 - Iterated Decomposition Improving Science Q&A by S.pdf}
}

@article{rexwinkleDevelopmentGamePurpose2019,
  title = {Development of a {{Game}} with a {{Purpose}} for {{Acquisition}} of {{Brain-Computer Interface Data}}},
  author = {Rexwinkle, Joe T. and Lieberman, Gregory and Jaswa, Matthew and Lance, Brent J.},
  year = {2019},
  month = sep,
  journal = {arXiv:1910.00106 [cs]},
  eprint = {1910.00106},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Brain-computer interfaces (BCIs) have the potential to significantly change the ways in which humans interact with technology, the environment, and even each other. Unfortunately, BCI technologies are seldom robust enough for use in real-world applications, in part due to the large amount of data that must be collected, processed, and classified in order to develop models of task-related neural activity that account for two of the most important and least-understood drivers of BCI illiteracy: individual differences in neural signals and intra-individual differences across interdependent, time-varying neural states. This paper describes the feasibility of using a game with a purpose (GWAP) as a viable instrument for collecting data from BCI-relevant research tasks. By leveraging game-related reward processes to maintain participant interest and engagement, this approach will enable large amounts of BCI data to be acquired, both across many individuals and longitudinally from specific individuals as neural states vary naturally over time. Pilot and technical testing results are presented here to demonstrate that the BCI-relevant tasks embedded within the research game elicit neural signals similar to those that would be expected from more traditional BCI tasks. These preliminary data provide support and validation of the use of GWAPs as promising tools to enable long-term collection of BCI-relevant data in an engaging environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/xav/Zotero/storage/IT7LITJH/Rexwinkle et al. - 2019 - Development of a Game with a Purpose for Acquisiti.pdf;/home/xav/Zotero/storage/IWTR5HAS/1910.html}
}

@inproceedings{rhyscoxDirectedDiversityLeveraging2021,
  title = {Directed {{Diversity}}: {{Leveraging Language Embedding Distances}} for {{Collective Creativity}} in {{Crowd Ideation}}},
  shorttitle = {Directed {{Diversity}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rhys Cox, Samuel and Wang, Yunlong and Abdul, Ashraf and {von der Weth}, Christian and Y. Lim, Brian},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--35},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3411764.3445782},
  urldate = {2023-01-04},
  abstract = {Crowdsourcing can collect many diverse ideas by prompting ideators individually, but this can generate redundant ideas. Prior methods reduce redundancy by presenting peers' ideas or peer-proposed prompts, but these require much human coordination. We introduce Directed Diversity, an automatic prompt selection approach that leverages language model embedding distances to maximize diversity. Ideators can be directed towards diverse prompts and away from prior ideas, thus improving their collective creativity. Since there are diverse metrics of diversity, we present a Diversity Prompting Evaluation Framework consolidating metrics from several research disciplines to analyze along the ideation chain \textemdash{} prompt selection, prompt creativity, prompt-ideation mediation, and ideation creativity. Using this framework, we evaluated Directed Diversity in a series of a simulation study and four user studies for the use case of crowdsourcing motivational messages to encourage physical activity. We show that automated diverse prompting can variously improve collective creativity across many nuanced metrics of diversity.},
  isbn = {978-1-4503-8096-6},
  keywords = {Collective Creativity,Collective Intelligence,Creativity Support Tool,Crowdsourcing,Diversity,Ideation,Motivational messaging},
  file = {/home/xav/Zotero/storage/I3A3X2TD/Rhys Cox et al. - 2021 - Directed Diversity Leveraging Language Embedding .pdf}
}

@misc{ribeiroEntailmentTreeExplanations2022,
  title = {Entailment {{Tree Explanations}} via {{Iterative Retrieval-Generation Reasoner}}},
  author = {Ribeiro, Danilo and Wang, Shen and Ma, Xiaofei and Dong, Rui and Wei, Xiaokai and Zhu, Henry and Chen, Xinchi and Huang, Zhiheng and Xu, Peng and Arnold, Andrew and Roth, Dan},
  year = {2022},
  month = jul,
  number = {arXiv:2205.09224},
  eprint = {arXiv:2205.09224},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.09224},
  urldate = {2023-02-08},
  abstract = {Large language models have achieved high performance on various question answering (QA) benchmarks, but the explainability of their output remains elusive. Structured explanations, called entailment trees, were recently suggested as a way to explain and inspect a QA system's answer. In order to better generate such entailment trees, we propose an architecture called Iterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a given hypothesis by systematically generating a step-by-step explanation from textual premises. The IRGR model iteratively searches for suitable premises, constructing a single entailment step at a time. Contrary to previous approaches, our method combines generation steps and retrieval of premises, allowing the model to leverage intermediate conclusions, and mitigating the input size limit of baseline encoder-decoder models. We conduct experiments using the EntailmentBank dataset, where we outperform existing benchmarks on premise retrieval and entailment tree generation, with around 300\% gain in overall correctness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,I.2.7},
  file = {/home/xav/Zotero/storage/CDSEIHJN/Ribeiro et al. - 2022 - Entailment Tree Explanations via Iterative Retriev.pdf;/home/xav/Zotero/storage/7USUI7FW/2205.html}
}

@misc{rivelandGeneralizationSensorimotorNetworks2022,
  title = {Generalization in {{Sensorimotor Networks Configured}} with {{Natural Language Instructions}}},
  author = {Riveland, Reidar and Pouget, Alexandre},
  year = {2022},
  month = nov,
  primaryclass = {New Results},
  pages = {2022.02.22.481293},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.02.22.481293},
  urldate = {2023-02-06},
  abstract = {One of humans' most fundamental cognitive feats is the ability to interpret linguistic instructions in order to perform novel tasks without any explicit experience with the task. Yet, the computations that the brain might use to accomplish such a feat remains poorly understood. Here we use the latest advances in Natural Language Processing to create a neural model of generalization based on linguistic instructions. Models are trained on a set of commonly studied psychophysical tasks, and receive instructions embedded by a pre-trained language model. Our best models can perform a previously unseen task with a performance of 85\% correct on average based solely on linguistic instructions (i.e. 0-shot learning). We found that language scaffolds sensorimotor representations such that activity for interrelated tasks share a common geometry with the semantic representations of instructions, allowing language to cue the proper composition of practiced skills in unseen settings. Finally, we show how this model can generate a linguistic description of a novel task it has identified using only motor feedback, which can subsequently guide a partner model to perform the task. Our models offer several experimentally testable predictions outlining how linguistic information must be represented in order to facilitate flexible and general cognition in the human brain.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english},
  file = {/home/xav/Zotero/storage/Y8IEDXVI/Riveland et Pouget - 2022 - Generalization in Sensorimotor Networks Configured.pdf}
}

@article{rogersGettingCloserAI2020,
  title = {Getting {{Closer}} to {{AI Complete Question Answering}}: {{A Set}} of {{Prerequisite Real Tasks}}},
  shorttitle = {Getting {{Closer}} to {{AI Complete Question Answering}}},
  author = {Rogers, Anna and Kovaleva, Olga and Downey, Matthew and Rumshisky, Anna},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {05},
  pages = {8722--8731},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i05.6398},
  urldate = {2023-02-01},
  abstract = {The recent explosion in question answering research produced a wealth of both factoid reading comprehension (RC) and commonsense reasoning datasets. Combining them presents a different kind of task: deciding not simply whether information is present in the text, but also whether a confident guess could be made for the missing information. We present QuAIL, the first RC dataset to combine text-based, world knowledge and unanswerable questions, and to provide question type annotation that would enable diagnostics of the reasoning strategies by a given QA system. QuAIL contains 15K multi-choice questions for 800 texts in 4 domains. Crucially, it offers both general and text-specific questions, unlikely to be found in pretraining data. We show that QuAIL poses substantial challenges to the current state-of-the-art systems, with a 30\% drop in accuracy compared to the most similar existing dataset.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/home/xav/Zotero/storage/GCU36BLK/Rogers et al. - 2020 - Getting Closer to AI Complete Question Answering .pdf}
}

@article{rogersQADatasetExplosion2022,
  title = {{{QA Dataset Explosion}}: {{A Taxonomy}} of {{NLP Resources}} for {{Question Answering}} and {{Reading Comprehension}}},
  shorttitle = {{{QA Dataset Explosion}}},
  author = {Rogers, Anna and Gardner, Matt and Augenstein, Isabelle},
  year = {2022},
  month = sep,
  journal = {ACM Computing Surveys},
  eprint = {2107.12708},
  primaryclass = {cs},
  pages = {3560260},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3560260},
  urldate = {2023-01-24},
  abstract = {Alongside huge volumes of research on deep learning models in NLP in the recent years, there has been also much work on benchmark datasets needed to track modeling progress. Question answering and reading comprehension have been particularly prolific in this regard, with over 80 new datasets appearing in the past two years. This study is the largest survey of the field to date. We provide an overview of the various formats and domains of the current resources, highlighting the current lacunae for future work. We further discuss the current classifications of "skills" that question answering/reading comprehension systems are supposed to acquire, and propose a new taxonomy. The supplementary materials survey the current multilingual resources and monolingual resources for languages other than English, and we discuss the implications of over-focusing on English. The study is aimed at both practitioners looking for pointers to the wealth of existing data, and at researchers working on new resources.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {QID: Q114857014},
  file = {/home/xav/Zotero/storage/A8U5QVFP/Rogers et al. - 2022 - QA Dataset Explosion A Taxonomy of NLP Resources .pdf}
}

@article{royReasoningQuantitiesNatural2015,
  title = {Reasoning about {{Quantities}} in {{Natural Language}}},
  author = {Roy, Subhro and Vieira, Tim and Roth, Dan},
  year = {2015},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {3},
  pages = {1--13},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00118},
  urldate = {2023-02-07},
  abstract = {Little work from the Natural Language Processing community has targeted the role of quantities in Natural Language Understanding. This paper takes some key steps towards facilitating reasoning about quantities expressed in natural language. We investigate two different tasks of numerical reasoning. First, we consider Quantity Entailment, a new task formulated to understand the role of quantities in general textual inference tasks. Second, we consider the problem of automatically understanding and solving elementary school math word problems. In order to address these quantitative reasoning problems we first develop a computational approach which we show to successfully recognize and normalize textual expressions of quantities. We then use these capabilities to further develop algorithms to assist reasoning in the context of the aforementioned tasks.},
  file = {/home/xav/Zotero/storage/GR8B3PZH/Roy et al. - 2015 - Reasoning about Quantities in Natural Language.pdf;/home/xav/Zotero/storage/3H6D8IT3/Reasoning-about-Quantities-in-Natural-Language.html}
}

@inproceedings{roySolvingGeneralArithmetic2015,
  title = {Solving {{General Arithmetic Word Problems}}},
  booktitle = {Proceedings of the 2015 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Roy, Subhro and Roth, Dan},
  year = {2015},
  month = sep,
  pages = {1743--1752},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/D15-1202},
  urldate = {2023-02-07},
  file = {/home/xav/Zotero/storage/8PUU78IL/Roy et Roth - 2015 - Solving General Arithmetic Word Problems.pdf}
}

@misc{rubinLearningRetrievePrompts2022,
  title = {Learning {{To Retrieve Prompts}} for {{In-Context Learning}}},
  author = {Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  year = {2022},
  month = may,
  number = {arXiv:2112.08633},
  eprint = {arXiv:2112.08633},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.08633},
  urldate = {2023-01-02},
  abstract = {In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompt). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and a LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/F6L4KKGN/Rubin et al. - 2022 - Learning To Retrieve Prompts for In-Context Learni.pdf;/home/xav/Zotero/storage/ZDYSS9QY/2112.html}
}

@article{rudolphChatGPTBullshitSpewer2023,
  title = {{{ChatGPT}}: {{Bullshit}} Spewer or the End of Traditional Assessments in Higher Education?},
  shorttitle = {{{ChatGPT}}},
  author = {Rudolph, J{\"u}rgen and Tan, Samson and Tan, Shannon},
  year = {2023},
  month = jan,
  journal = {Journal of Applied Learning and Teaching},
  volume = {6},
  number = {1},
  issn = {2591-801X},
  doi = {10.37074/jalt.2023.6.1.9},
  urldate = {2023-02-16},
  abstract = {ChatGPT is the world's most advanced chatbot thus far. Unlike other chatbots, it can create impressive prose within seconds, and it has created much hype and doomsday predictions when it comes to student assessment in higher education and a host of other matters. ChatGPT is a state-of-the-art language model (a variant of OpenAI's Generative Pretrained Transformer (GPT) language model) designed to generate text that can be indistinguishable from text written by humans. It can engage in conversation with users in a seemingly natural and intuitive way. In this article, we briefly tell the story of OpenAI, the organisation behind ChatGPT. We highlight the fundamental change from a not-for-profit organisation to a commercial business model. In terms of our methods, we conducted an extensive literature review and experimented with this artificial intelligence (AI) software. Our literature review shows our review to be amongst the first peer-reviewed academic journal articles to explore ChatGPT and its relevance for higher education (especially assessment, learning and teaching). After a description of ChatGPT's functionality and a summary of its strengths and limitations, we focus on the technology's implications for higher education and discuss what is the future of learning, teaching and assessment in higher education in the context of AI chatbots such as ChatGPT. We position ChatGPT in the context of current Artificial Intelligence in Education (AIEd) research, discuss student-facing, teacher-facing and system-facing applications, and analyse opportunities and threats. We conclude the article with recommendations for students, teachers and higher education institutions. Many of them focus on assessment.},
  copyright = {Copyright (c) 2023 Journal of Applied Learning and Teaching},
  langid = {english},
  file = {/home/xav/Zotero/storage/LVVJKDAK/Rudolph et al. - 2023 - ChatGPT Bullshit spewer or the end of traditional.pdf}
}

@article{ruizRaisonnementCollaboratifPartir2012,
  title = {Raisonnement Collaboratif \`a Partir de Cas Dans La R\'esolution de Probl\`emes En Maintenance},
  author = {Ruiz, P. and Noyes, D. and {Kamsu-Foguem}, Bernard},
  year = {2012},
  journal = {undefined},
  urldate = {2022-05-07},
  abstract = {Nous nous interessons dans cette etude a la realisation de la fonction maintenance en contexte industriel et, plus particulierement, a l'aide qui peut etre apportee aux processus decisionnels sous-jacents par la reutilisation des connaissances. La resolution de problemes complexes en maintenance necessite souvent la collaboration d'experts pour prendre les decisions necessaires, parfois en situation d'urgence. Nos travaux visent l'amelioration des performances des actions de maintenance par l'exploitation de systeme de retour d'experiences en contexte collaboratif. Plusieurs idees sont developpees, dans le domaine de la maintenance industrielle, sur les processus de resolution de problemes complexes et, plus particulierement, une proposition de developpement de Raisonnement Collaboratif a Partir de Cas (RCaPC).},
  langid = {english},
  file = {/home/xav/Zotero/storage/F6KLFUTP/Ruiz et al. - 2012 - Raisonnement collaboratif √† partir de cas dans la .pdf;/home/xav/Zotero/storage/MAJ4U2J9/5fa53d14ecbeb960f9b6ee19a921fde1641efcc7.html}
}

@misc{sachanEndtoEndTrainingMultiDocument2021,
  title = {End-to-{{End Training}} of {{Multi-Document Reader}} and {{Retriever}} for {{Open-Domain Question Answering}}},
  author = {Sachan, Devendra Singh and Reddy, Siva and Hamilton, William and Dyer, Chris and Yogatama, Dani},
  year = {2021},
  month = dec,
  number = {arXiv:2106.05346},
  eprint = {arXiv:2106.05346},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.05346},
  urldate = {2023-01-16},
  abstract = {We present an end-to-end differentiable training method for retrieval-augmented open-domain question answering systems that combine information from multiple retrieved documents when generating answers. We model retrieval decisions as latent variables over sets of relevant documents. Since marginalizing over sets of retrieved documents is computationally hard, we approximate this using an expectation-maximization algorithm. We iteratively estimate the value of our latent variable (the set of relevant documents for a given question) and then use this estimate to update the retriever and reader parameters. We hypothesize that such end-to-end training allows training signals to flow to the reader and then to the retriever better than staged-wise training. This results in a retriever that is able to select more relevant documents for a question and a reader that is trained on more accurate documents to generate an answer. Experiments on three benchmark datasets demonstrate that our proposed method outperforms all existing approaches of comparable size by 2-3\% absolute exact match points, achieving new state-of-the-art results. Our results also demonstrate the feasibility of learning to retrieve to improve answer generation without explicit supervision of retrieval decisions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/8L43WDHE/Sachan et al. - 2021 - End-to-End Training of Multi-Document Reader and R.pdf}
}

@article{sahaExplaGraphsExplanationGraph2021,
  title = {{{ExplaGraphs}}: {{An Explanation Graph Generation Task}} for {{Structured Commonsense Reasoning}}},
  shorttitle = {{{ExplaGraphs}}},
  author = {Saha, Swarnadeep and Yadav, Prateek and Bauer, Lisa and Bansal, Mohit},
  year = {2021},
  month = oct,
  journal = {arXiv:2104.07644 [cs]},
  eprint = {2104.07644},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Recent commonsense-reasoning tasks are typically discriminative in nature, where a model answers a multiple-choice question for a certain context. Discriminative tasks are limiting because they fail to adequately evaluate the model's ability to reason and explain predictions with underlying commonsense knowledge. They also allow such models to use reasoning shortcuts and not be "right for the right reasons". In this work, we present ExplaGraphs, a new generative and structured commonsense-reasoning task (and an associated dataset) of explanation graph generation for stance prediction. Specifically, given a belief and an argument, a model has to predict if the argument supports or counters the belief and also generate a commonsense-augmented graph that serves as non-trivial, complete, and unambiguous explanation for the predicted stance. We collect explanation graphs through a novel Create-Verify-And-Refine graph collection framework that improves the graph quality (up to 90\%) via multiple rounds of verification and refinement. A significant 79\% of our graphs contain external commonsense nodes with diverse structures and reasoning depths. Next, we propose a multi-level evaluation framework, consisting of automatic metrics and human evaluation, that check for the structural and semantic correctness of the generated graphs and their degree of match with ground-truth graphs. Finally, we present several structured, commonsense-augmented, and text generation models as strong starting points for this explanation graph generation task, and observe that there is a large gap with human performance, thereby encouraging future work for this new challenging task. ExplaGraphs will be publicly available at https://explagraphs.github.io.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/Y6ZW9ZT2/Saha et al. - 2021 - ExplaGraphs An Explanation Graph Generation Task .pdf}
}

@inproceedings{saharoyQuestionAnsweringCurated2020,
  title = {Question {{Answering}} over {{Curated}} and {{Open Web Sources}}},
  booktitle = {Proceedings of the 43rd {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Saha Roy, Rishiraj and Anand, Avishek},
  year = {2020},
  month = jul,
  pages = {2432--2435},
  publisher = {{ACM}},
  address = {{Virtual Event China}},
  doi = {10.1145/3397271.3401421},
  urldate = {2022-05-04},
  abstract = {The last few years have seen an explosion of research on the topic of automated question answering (QA), spanning the communities of information retrieval, natural language processing, and artificial intelligence. This tutorial would cover the highlights of this really active period of growth for QA to give the audience a grasp over the families of algorithms that are currently being used. We partition research contributions by the underlying source from where answers are retrieved: curated knowledge graphs, unstructured text, or hybrid corpora. We choose this dimension of partitioning as it is the most discriminative when it comes to algorithm design. Other key dimensions are covered within each sub-topic: like the complexity of questions addressed, and degrees of explainability and interactivity introduced in the systems. We would conclude the tutorial with the most promising emerging trends in the expanse of QA, that would help new entrants into this field make the best decisions to take the community forward. Much has changed in the community since the last tutorial on QA in SIGIR 2016, and we believe that this timely overview will indeed benefit a large number of conference participants.},
  isbn = {978-1-4503-8016-4},
  langid = {english},
  file = {/home/xav/Zotero/storage/E8K4IYMT/Saha Roy et Anand - 2020 - Question Answering over Curated and Open Web Sourc.pdf}
}

@misc{sakaguchiWinoGrandeAdversarialWinograd2019,
  title = {{{WinoGrande}}: {{An Adversarial Winograd Schema Challenge}} at {{Scale}}},
  shorttitle = {{{WinoGrande}}},
  author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  year = {2019},
  month = nov,
  number = {arXiv:1907.10641},
  eprint = {arXiv:1907.10641},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.10641},
  urldate = {2023-02-01},
  abstract = {The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90\% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1\%, which are 15-35\% below human performance of 94.0\%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1\%), DPR (93.1\%), COPA (90.6\%), KnowRef (85.6\%), and Winogender (97.1\%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/4VIYDK7K/Sakaguchi et al. - 2019 - WinoGrande An Adversarial Winograd Schema Challen.pdf;/home/xav/Zotero/storage/P7ZA6P5G/1907.html}
}

@misc{sanhMultitaskPromptedTraining2022,
  title = {Multitask {{Prompted Training Enables Zero-Shot Task Generalization}}},
  author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H. and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and Dey, Manan and Bari, M. Saiful and Xu, Canwen and Thakker, Urmish and Sharma, Shanya Sharma and Szczechla, Eliza and Kim, Taewoon and Chhablani, Gunjan and Nayak, Nihal and Datta, Debajyoti and Chang, Jonathan and Jiang, Mike Tian-Jian and Wang, Han and Manica, Matteo and Shen, Sheng and Yong, Zheng Xin and Pandey, Harshit and Bawden, Rachel and Wang, Thomas and Neeraj, Trishala and Rozen, Jos and Sharma, Abheesht and Santilli, Andrea and Fevry, Thibault and Fries, Jason Alan and Teehan, Ryan and Bers, Tali and Biderman, Stella and Gao, Leo and Wolf, Thomas and Rush, Alexander M.},
  year = {2022},
  month = mar,
  number = {arXiv:2110.08207},
  eprint = {arXiv:2110.08207},
  publisher = {{arXiv}},
  urldate = {2023-02-01},
  abstract = {Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks (Brown et al., 2020). It has been hypothesized that this is a consequence of implicit multitask learning in language models' pretraining (Radford et al., 2019). Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping any natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts with diverse wording. These prompted datasets allow for benchmarking the ability of a model to perform completely held-out tasks. We fine-tune a pretrained encoder-decoder model (Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16\texttimes{} its size. Further, our approach attains strong performance on a subset of tasks from the BIG-bench benchmark, outperforming models up to 6\texttimes{} its size. All trained models are available at https://github.com/bigscience-workshop/t-zero, and all prompts are available at https://github.com/bigscience-workshop/promptsource.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/P7696SRL/Sanh et al. - 2022 - Multitask Prompted Training Enables Zero-Shot Task.pdf}
}

@phdthesis{sanjaykamathramachandraraoQuestionAnsweringHybrid2020,
  title = {Question {{Answering}} with {{Hybrid Data}} and {{Models}}},
  author = {Sanjay Kamath Ramachandra Rao},
  year = {2020},
  school = {Universit\'e Paris-Saclay},
  file = {/home/xav/Zotero/storage/4528NS85/Sanjay Kamath Ramachandra Rao - 2020 - Question Answering with Hybrid Data and Models.pdf}
}

@misc{saparovLanguageModelsAre2022,
  title = {Language {{Models Are Greedy Reasoners}}: {{A Systematic Formal Analysis}} of {{Chain-of-Thought}}},
  shorttitle = {Language {{Models Are Greedy Reasoners}}},
  author = {Saparov, Abulhair and He, He},
  year = {2022},
  month = oct,
  number = {arXiv:2210.01240},
  eprint = {arXiv:2210.01240},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.01240},
  urldate = {2022-10-12},
  abstract = {Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/F3VUNRLC/Saparov et He - 2022 - Language Models Are Greedy Reasoners A Systematic.pdf}
}

@inproceedings{sapSocialIQaCommonsense2019,
  title = {Social {{IQa}}: {{Commonsense Reasoning}} about {{Social Interactions}}},
  shorttitle = {Social {{IQa}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Sap, Maarten and Rashkin, Hannah and Chen, Derek and Le Bras, Ronan and Choi, Yejin},
  year = {2019},
  month = nov,
  pages = {4463--4473},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1454},
  urldate = {2023-02-01},
  abstract = {We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: ``Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?'' A: ``Make sure no one else could hear''). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (\textbackslash textgreater20\% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).},
  file = {/home/xav/Zotero/storage/95BHEMCJ/Sap et al. - 2019 - Social IQa Commonsense Reasoning about Social Int.pdf}
}

@misc{SchemaOrgSchema,
  title = {Schema.Org - {{Schema}}.Org},
  urldate = {2022-05-04},
  howpublished = {https://schema.org/},
  file = {/home/xav/Zotero/storage/QEQKDR6J/schema.org.html}
}

@misc{schickPEERCollaborativeLanguage2022,
  title = {{{PEER}}: {{A Collaborative Language Model}}},
  shorttitle = {{{PEER}}},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Jiang, Zhengbao and Petroni, Fabio and Lewis, Patrick and Izacard, Gautier and You, Qingfei and Nalmpantis, Christoforos and Grave, Edouard and Riedel, Sebastian},
  year = {2022},
  month = aug,
  number = {arXiv:2208.11663},
  eprint = {arXiv:2208.11663},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.11663},
  urldate = {2022-09-02},
  abstract = {Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/GZUMGZ7V/Schick et al. - 2022 - PEER A Collaborative Language Model.pdf}
}

@misc{schickToolformerLanguageModels2023,
  title = {Toolformer: {{Language Models Can Teach Themselves}} to {{Use Tools}}},
  shorttitle = {Toolformer},
  author = {Schick, Timo and {Dwivedi-Yu}, Jane and Dess{\`i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04761},
  eprint = {arXiv:2302.04761},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04761},
  urldate = {2023-02-13},
  abstract = {Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\textbackslash\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/X7TUSBG2/Schick et al. - 2023 - Toolformer Language Models Can Teach Themselves t.pdf;/home/xav/Zotero/storage/CDCV8LWU/2302.html}
}

@misc{schickUsingBigLanguage2021,
  title = {Using {{Big Language Models To Generate Entire Datasets From Scratch}}},
  author = {Schick, Timo},
  year = {2021},
  month = may,
  journal = {Timo Schick},
  urldate = {2022-05-04},
  abstract = {This post discusses how DINO (Datasets From Instructions) can be used to distill the zero-shot knowledge of big language models like GPT3 into much smaller models without requiring any data or access to model internals.},
  howpublished = {http://timoschick.com/research/2021/05/19/dino.html},
  langid = {english},
  file = {/home/xav/Zotero/storage/MR2BVN8Y/Schick - 2021 -  Using Big Language Models To Generate Entire Da.pdf;/home/xav/Zotero/storage/F7ZU7J2E/dino.html}
}

@misc{schillerAspectControlledNeuralArgument2020,
  title = {Aspect-{{Controlled Neural Argument Generation}}},
  author = {Schiller, Benjamin and Daxenberger, Johannes and Gurevych, Iryna},
  year = {2020},
  month = apr,
  number = {arXiv:2005.00084},
  eprint = {arXiv:2005.00084},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.00084},
  urldate = {2023-01-03},
  abstract = {We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we train a language model for argument generation that can be controlled on a fine-grained level to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that our generation model is able to generate high-quality, aspect-specific arguments. Moreover, these arguments can be used to improve the performance of stance detection models via data augmentation and to generate counter-arguments. We publish all datasets and code to fine-tune the language model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/YEKEBEJS/Schiller et al. - 2020 - Aspect-Controlled Neural Argument Generation.pdf;/home/xav/Zotero/storage/EQENQ69Y/2005.html}
}

@article{schonUsingConceptNetTeach2019,
  title = {Using {{ConceptNet}} to {{Teach Common Sense}} to an {{Automated Theorem Prover}}},
  author = {Schon, Claudia and Siebert, Sophie and Stolzenburg, Frieder},
  year = {2019},
  month = dec,
  journal = {Electronic Proceedings in Theoretical Computer Science},
  volume = {311},
  pages = {19--24},
  issn = {2075-2180},
  doi = {10.4204/EPTCS.311.3},
  urldate = {2022-05-04},
  langid = {english},
  file = {/home/xav/Zotero/storage/6C37HUZ9/Schon et al. - 2019 - Using ConceptNet to Teach Common Sense to an Autom.pdf}
}

@book{schreiberKnowledgeEngineeringManagement2001,
  title = {Knowledge {{Engineering}} and {{Management}} - {{The CommonKADS Methodology}}},
  author = {Schreiber, Guus and Akkermans, Hans and Anjewierden, Anjo and Hoog, Robert and Shadbolt, Nigel and Velde, Walter and Wielinga, Bob},
  year = {2001},
  month = jan,
  volume = {24},
  doi = {10.7551/mitpress/4073.001.0001},
  isbn = {978-0-262-28323-6},
  file = {/home/xav/Zotero/storage/VZXHTMVJ/Schreiber et al. - 2001 - Knowledge Engineering and Management - The CommonK.pdf}
}

@misc{schusterConfidentAdaptiveLanguage2022,
  title = {Confident {{Adaptive Language Modeling}}},
  author = {Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh Q. and Tay, Yi and Metzler, Donald},
  year = {2022},
  month = oct,
  number = {arXiv:2207.07061},
  eprint = {arXiv:2207.07061},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2207.07061},
  urldate = {2023-02-09},
  abstract = {Recent advances in Transformer-based large language models (LLMs) have led to significant performance improvements across many tasks. These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time. In practice, however, the series of generations made by LLMs is composed of varying levels of difficulty. While certain predictions truly benefit from the models' full capacity, other continuations are more trivial and can be solved with reduced compute. In this work, we introduce Confident Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep. Early exit decoding involves several challenges that we address here, such as: (1) what confidence measure to use; (2) connecting sequence-level constraints to local per-token exit decisions; and (3) attending back to missing hidden representations due to early exits in previous tokens. Through theoretical analysis and empirical experiments on three diverse text generation tasks, we demonstrate the efficacy of our framework in reducing compute -- potential speedup of up to \$\textbackslash times 3\$ -- while provably maintaining high performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/DGH2EA4S/Schuster et al. - 2022 - Confident Adaptive Language Modeling.pdf;/home/xav/Zotero/storage/HYNI6JMZ/2207.html}
}

@misc{schuurmansMemoryAugmentedLarge2023,
  title = {Memory {{Augmented Large Language Models}} Are {{Computationally Universal}}},
  author = {Schuurmans, Dale},
  year = {2023},
  month = jan,
  number = {arXiv:2301.04589},
  eprint = {arXiv:2301.04589},
  publisher = {{arXiv}},
  urldate = {2023-02-06},
  abstract = {We show that transformer-based large language models are computationally universal when augmented with an external memory. Any deterministic language model that conditions on strings of bounded length is equivalent to a finite automaton, hence computationally limited. However, augmenting such models with a read-write memory creates the possibility of processing arbitrarily large inputs and, potentially, simulating any algorithm. We establish that an existing large language model, Flan-U-PaLM 540B, can be combined with an associative read-write memory to exactly simulate the execution of a universal Turing machine, U15 2. A key aspect of the finding is that it does , not require any modification of the language model weights. Instead, the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory},
  file = {/home/xav/Zotero/storage/2VIB2W28/Schuurmans - 2023 - Memory Augmented Large Language Models are Computa.pdf}
}

@misc{scialomFinetunedLanguageModels2022,
  title = {Fine-Tuned {{Language Models}} Are {{Continual Learners}}},
  author = {Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  year = {2022},
  month = oct,
  number = {arXiv:2205.12393},
  eprint = {arXiv:2205.12393},
  publisher = {{arXiv}},
  urldate = {2023-02-16},
  abstract = {Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions. Language models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets. To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning we show that Language Models can be continual learners. We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn diverse new tasks, while still maintaining good performance on previous tasks, spanning remarkably through 70 datasets in total. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some compositionality.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/ENQCVM2V/Scialom et al. - 2022 - Fine-tuned Language Models are Continual Learners.pdf}
}

@inproceedings{scialomQuestEvalSummarizationAsks2021,
  title = {{{QuestEval}}: {{Summarization Asks}} for {{Fact-based Evaluation}}},
  shorttitle = {{{QuestEval}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Scialom, Thomas and Dray, Paul-Alexis and Lamprier, Sylvain and Piwowarski, Benjamin and Staiano, Jacopo and Wang, Alex and Gallinari, Patrick},
  year = {2021},
  month = nov,
  pages = {6594--6604},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.529},
  urldate = {2023-01-24},
  abstract = {Summarization evaluation remains an open research problem: current metrics such as ROUGE are known to be limited and to correlate poorly with human judgments. To alleviate this issue, recent work has proposed evaluation metrics which rely on question answering models to assess whether a summary contains all the relevant information in its source document. Though promising, the proposed approaches have so far failed to correlate better than ROUGE with human judgments. In this paper, we extend previous approaches and propose a unified framework, named QuestEval. In contrast to established metrics such as ROUGE or BERTScore, QuestEval does not require any ground-truth reference. Nonetheless, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions (consistency, coherence, fluency, and relevance), as shown in extensive experiments.},
  file = {/home/xav/Zotero/storage/UINF3KVI/Scialom et al. - 2021 - QuestEval Summarization Asks for Fact-based Evalu.pdf}
}

@misc{SemEvalInternationalWorkshop,
  title = {{{SemEval}} | {{International Workshop}} on {{Semantic Evaluation}}},
  journal = {SemEval},
  urldate = {2023-02-15},
  abstract = {International Workshop on Semantic Evaluation},
  howpublished = {https://semeval.github.io/},
  langid = {american},
  file = {/home/xav/Zotero/storage/97EDGJ69/semeval.github.io.html}
}

@inproceedings{shaoCompositionalTaskRepresentations2023,
  title = {Compositional {{Task Representations}} for {{Large Language Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Shao, Nan and Cai, Zefan and Xu, Hanwei and Liao, Chonghua and Zheng, Yanan and Yang, Zhilin},
  year = {2023},
  month = feb,
  urldate = {2023-02-07},
  abstract = {Large language models have shown a remarkable cross-task generalization ability. Most prior work assumed that prompts effectively extract knowledge from language models to facilitate generalization to new tasks. This perspective led to numerous studies on improving prompts. In contrast, we introduce a new perspective, compositional generalization, that views each task as a composition of latent codes and generalizes to test tasks by a new composition of seen codes. To this end, we propose a novel prompt-free approach, Compositional Task Representations (CTR), that employs multi-task training to learn a discrete, compositional codebook. Empirically, our CTR substantially outperforms prompt-based methods in zero-label learning on average. According to our analysis, some of the learned CTR codes are interpretable to human and demonstrate a certain degree of controllability.},
  langid = {english},
  file = {/home/xav/Zotero/storage/JUVJ396K/Shao et al. - 2023 - Compositional Task Representations for Large Langu.pdf}
}

@inproceedings{shaoSyntheticPromptingGenerating2023,
  title = {Synthetic {{Prompting}}: {{Generating Chain-of-Thought Demonstrations}} for {{Large Language Models}}},
  shorttitle = {Synthetic {{Prompting}}},
  author = {Shao, Zhihong and Gong, Yeyun and Shen, Yelong and Huang, Minlie and Duan, Nan and Chen, Weizhu},
  year = {2023},
  month = feb,
  urldate = {2023-02-03},
  abstract = {Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce S YNTHETIC PROMPTING , a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process gen-erates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.},
  file = {/home/xav/Zotero/storage/HDYDB4R9/Shao et al. - 2023 - Synthetic Prompting Generating Chain-of-Thought D.pdf}
}

@misc{sharirCostTrainingNLP2020,
  title = {The {{Cost}} of {{Training NLP Models}}: {{A Concise Overview}}},
  shorttitle = {The {{Cost}} of {{Training NLP Models}}},
  author = {Sharir, Or and Peleg, Barak and Shoham, Yoav},
  year = {2020},
  month = apr,
  number = {arXiv:2004.08900},
  eprint = {arXiv:2004.08900},
  publisher = {{arXiv}},
  urldate = {2023-02-09},
  abstract = {We review the cost of training large-scale language models, and the drivers of these costs. The intended audience includes engineers and scientists budgeting their model-training experiments, as well as non-practitioners trying to make sense of the economics of modern-day Natural Language Processing (NLP).},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/xav/Zotero/storage/Q5A793TH/Sharir et al. - 2020 - The Cost of Training NLP Models A Concise Overvie.pdf}
}

@misc{shenHuggingGPTSolvingAI2023,
  title = {{{HuggingGPT}}: {{Solving AI Tasks}} with {{ChatGPT}} and Its {{Friends}} in {{HuggingFace}}},
  shorttitle = {{{HuggingGPT}}},
  author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  year = {2023},
  month = apr,
  number = {arXiv:2303.17580},
  eprint = {arXiv:2303.17580},
  publisher = {{arXiv}},
  urldate = {2023-04-06},
  abstract = {Solving complicated AI tasks with different domains and modalities is a key step toward advanced artificial intelligence. While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., Hugging Face) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in Hugging Face, execute each subtask with the selected AI model, and summarize the response according to the execution results. By leveraging the strong language capability of ChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks, which paves a new way towards advanced artificial intelligence 2.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/N4BNGGCA/Shen et al. - 2023 - HuggingGPT Solving AI Tasks with ChatGPT and its .pdf}
}

@misc{shenMultiLexSumRealWorldSummaries2022,
  title = {Multi-{{LexSum}}: {{Real-World Summaries}} of {{Civil Rights Lawsuits}} at {{Multiple Granularities}}},
  shorttitle = {Multi-{{LexSum}}},
  author = {Shen, Zejiang and Lo, Kyle and Yu, Lauren and Dahlberg, Nathan and Schlanger, Margo and Downey, Doug},
  year = {2022},
  month = jul,
  number = {arXiv:2206.10883},
  eprint = {2206.10883},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-25},
  abstract = {With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC) (https://clearinghouse.net),which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence "extreme" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further research in summarization methods as well as to facilitate development of applications to assist in the CRLC's mission at https://multilexsum.github.io.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/home/xav/Zotero/storage/NK4IZHSL/Shen et al. - 2022 - Multi-LexSum Real-World Summaries of Civil Rights.pdf}
}

@article{shenRevisitingEvaluationMetrics2022,
  title = {Revisiting the {{Evaluation Metrics}} of {{Paraphrase Generation}}},
  author = {Shen, Lingfeng and Jiang, Haiyun and Liu, Lemao and Shi, Shuming},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.08479 [cs]},
  eprint = {2202.08479},
  primaryclass = {cs},
  urldate = {2022-05-14},
  abstract = {Paraphrase generation is an important NLP task that has achieved significant progress recently. However, one crucial problem is overlooked, `how to evaluate the quality of paraphrase?'. Most existing paraphrase generation models use reference-based metrics (e.g., BLEU) from neural machine translation (NMT) to evaluate their generated paraphrase. Such metrics' reliability is hardly evaluated, and they are only plausible when there exists a standard reference. Therefore, this paper first answers one fundamental question, `Are existing metrics reliable for paraphrase generation?'. We present two conclusions that disobey conventional wisdom in paraphrasing generation: (1) existing metrics poorly align with human annotation in system-level and segment-level paraphrase evaluation. (2) reference-free metrics outperform referencebased metrics, indicating that the standard references are unnecessary to evaluate the paraphrase's quality. Such empirical findings expose a lack of reliable automatic evaluation metrics. Therefore, this paper proposes BBScore, a reference-free metric that can reflect the generated paraphrase's quality. BBScore consists of two sub-metrics: S3C score and SelfBLEU, which correspond to two criteria for paraphrase evaluation: semantic preservation and diversity. By connecting two submetrics, BBScore significantly outperforms existing paraphrase evaluation metrics.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/78IFWUU8/Shen et al. - 2022 - Revisiting the Evaluation Metrics of Paraphrase Ge.pdf}
}

@misc{shiJustFinetuneTwice2022,
  title = {Just {{Fine-tune Twice}}: {{Selective Differential Privacy}} for {{Large Language Models}}},
  shorttitle = {Just {{Fine-tune Twice}}},
  author = {Shi, Weiyan and Chen, Si and Zhang, Chiyuan and Jia, Ruoxi and Yu, Zhou},
  year = {2022},
  month = apr,
  number = {arXiv:2204.07667},
  eprint = {arXiv:2204.07667},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.07667},
  urldate = {2022-07-19},
  abstract = {With the increasing adoption of NLP models in real-world products, it becomes more and more important to protect these models from privacy leakage. Because private information in language data is sparse, previous research formalized a Selective-Differential-Privacy (SDP) notion to provide protection for sensitive tokens detected by policy functions, and prove its effectiveness on RNN-based models. But the previous mechanism requires separating the private and public model parameters and thus cannot be applied on large attention-based models. In this paper, we propose a simple yet effective just-fine-tune-twice privacy mechanism to first fine-tune on in-domain redacted data and then on in-domain private data, to achieve SDP for large Transformer-based language models. We also design explicit and contextual policy functions to provide protections at different levels. Experiments show that our models achieve strong performance while staying robust to the canary insertion attack. We further show that even under low-resource settings with a small amount of in-domain data, SDP can still improve the model utility. We will release the code, data and models to facilitate future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security},
  file = {/home/xav/Zotero/storage/UTSQXDJ8/Shi et al. - 2022 - Just Fine-tune Twice Selective Differential Priva.pdf;/home/xav/Zotero/storage/T6A99BQR/2204.html}
}

@misc{shiLanguageModelsAre2022,
  title = {Language {{Models}} Are {{Multilingual Chain-of-Thought Reasoners}}},
  author = {Shi, Freda and Suzgun, Mirac and Freitag, Markus and Wang, Xuezhi and Srivats, Suraj and Vosoughi, Soroush and Chung, Hyung Won and Tay, Yi and Ruder, Sebastian and Zhou, Denny and Das, Dipanjan and Wei, Jason},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03057},
  eprint = {arXiv:2210.03057},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03057},
  urldate = {2022-10-12},
  abstract = {We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We find that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/QUH5EPHF/Shi et al. - 2022 - Language Models are Multilingual Chain-of-Thought .pdf}
}

@misc{shiREPLUGRetrievalAugmentedBlackBox2023,
  title = {{{REPLUG}}: {{Retrieval-Augmented Black-Box Language Models}}},
  shorttitle = {{{REPLUG}}},
  author = {Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  year = {2023},
  month = jan,
  number = {arXiv:2301.12652},
  eprint = {arXiv:2301.12652},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.12652},
  urldate = {2023-02-01},
  abstract = {We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model. Unlike prior retrieval-augmented LMs that train language models with special cross attention mechanisms to encode the retrieved text, REPLUG simply prepends retrieved documents to the input for the frozen black-box LM. This simple design be can easily applied to any existing retrieval and language models. Furthermore, we show that the LM can be used to supervise the retrieval model, which can then find documents that help the LM make better predictions. Our experiments demonstrate that REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3\%, as well as the performance of Codex on five-shot MMLU by 5.1\%.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/8Q42K59F/Shi et al. - 2023 - REPLUG Retrieval-Augmented Black-Box Language Mod.pdf}
}

@article{shiTalk2DataHighLevelQuestion2021,
  title = {{{Talk2Data}}: {{High-Level Question Decomposition}} for {{Data-Oriented Question}} and {{Answering}}},
  shorttitle = {{{Talk2Data}}},
  author = {Shi, Danqing and Guo, Yi and Guo, Mingjuan and Wu, Yanqiu and Chen, Qing and Cao, Nan},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.14420 [cs]},
  eprint = {2107.14420},
  primaryclass = {cs},
  urldate = {2022-03-28},
  abstract = {Through a data-oriented question and answering system, users can directly "ask" the system for the answers to their analytical questions about the input tabular data. This process greatly improves user experience and lowers the technical barriers of data analysis. Existing techniques focus on providing a concrete query for users or untangling the ambiguities in a specific question so that the system could better understand questions and provide more correct and precise answers. However, when users have little knowledge about the data, it is difficult for them to ask concrete questions. Instead, high-level questions are frequently asked, which cannot be easily solved with the existing techniques. To address the issue, in this paper, we introduce Talk2Data, a data-oriented online question and answering system that supports answering both low-level and high-level questions. It leverages a novel deep-learning model to resolve high-level questions into a series of low-level questions that can be answered by data facts. These low-level questions could be used to gradually elaborate the users' requirements. We design a set of annotated and captioned visualizations to represent the answers in a form that supports interpretation and narration. We evaluate the effectiveness of the Talk2Data system via a series of evaluations including case studies, performance validation, and a controlled user study. The results show the power of the system.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/home/xav/Zotero/storage/JEHH4F42/Shi et al. - 2021 - Talk2Data High-Level Question Decomposition for D.pdf;/home/xav/Zotero/storage/TFRI45FA/2107.html}
}

@misc{shridharAutomaticGenerationSocratic2022,
  title = {Automatic {{Generation}} of {{Socratic Subquestions}} for {{Teaching Math Word Problems}}},
  author = {Shridhar, Kumar and Macina, Jakub and {El-Assady}, Mennatallah and Sinha, Tanmay and Kapur, Manu and Sachan, Mrinmaya},
  year = {2022},
  month = nov,
  number = {arXiv:2211.12835},
  eprint = {arXiv:2211.12835},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.12835},
  urldate = {2023-02-07},
  abstract = {Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers. In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning. On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/Z654BHSG/Shridhar et al. - 2022 - Automatic Generation of Socratic Subquestions for .pdf;/home/xav/Zotero/storage/ACZCSCR6/2211.html}
}

@misc{shridharDistillingMultiStepReasoning2022,
  title = {Distilling {{Multi-Step Reasoning Capabilities}} of {{Large Language Models}} into {{Smaller Models}} via {{Semantic Decompositions}}},
  author = {Shridhar, Kumar and Stolfo, Alessandro and Sachan, Mrinmaya},
  year = {2022},
  month = nov,
  number = {arXiv:2212.00193},
  eprint = {arXiv:2212.00193},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.00193},
  urldate = {2022-12-17},
  abstract = {Step-by-step reasoning approaches like chain-of-thought (CoT) have proved to be a very effective technique to induce reasoning capabilities in large language models. However, the success of the CoT approach depends primarily on model size, and often billion parameter-scale models are needed to get CoT to work. In this paper, we propose a knowledge distillation approach, that leverages the step-by-step CoT reasoning capabilities of larger models and distils these reasoning abilities into smaller models. Our approach Decompositional Distillation learns a semantic decomposition of the original problem into a sequence of subproblems and uses it to train two models: a) a problem decomposer that learns to decompose the complex reasoning problem into a sequence of simpler sub-problems and b) a problem solver that uses the intermediate subproblems to solve the overall problem. On a multi-step math word problem dataset (GSM8K), we boost the performance of GPT-2 variants up to 35\% when distilled with our approach compared to CoT. We show that using our approach, it is possible to train a GPT-2-large model (775M) that can outperform a 10X larger GPT-3 (6B) model trained using CoT reasoning. Finally, we also demonstrate that our approach of problem decomposition can also be used as an alternative to CoT prompting, which boosts the GPT-3 performance by 40\% compared to CoT prompts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ZWVH5XPJ/Shridhar et al. - 2022 - Distilling Multi-Step Reasoning Capabilities of La.pdf}
}

@article{shusterLanguageModelsThat2022,
  title = {Language {{Models}} That {{Seek}} for {{Knowledge}}: {{Modular Search}} \& {{Generation}} for {{Dialogue}} and {{Prompt Completion}}},
  shorttitle = {Language {{Models}} That {{Seek}} for {{Knowledge}}},
  author = {Shuster, Kurt and Komeili, Mojtaba and Adolphs, Leonard and Roller, Stephen and Szlam, Arthur and Weston, Jason},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.13224 [cs]},
  eprint = {2203.13224},
  primaryclass = {cs},
  urldate = {2022-04-25},
  abstract = {Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine-{$>$}Knowledge-{$>$}Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/UIGVY7JH/Shuster et al. - 2022 - Language Models that Seek for Knowledge Modular S.pdf;/home/xav/Zotero/storage/N5CHQBE8/2203.html}
}

@article{siddharthNaturalLanguageProcessing,
  title = {Natural {{Language Processing}} In-and-for {{Design Research}}},
  author = {Siddharth, L and Blessing, Lucienne T M and Luo, Jianxi},
  pages = {77},
  abstract = {We review the scholarly contributions that utilise Natural Language Processing (NLP) techniques to support the design process. Using a heuristic approach, we gathered 223 articles that are published in 32 journals within the period 1991-present. We present state-of-the-art NLP in-and-for design research by reviewing these articles according to the type of natural language text sources: internal reports, design concepts, discourse transcripts, technical publications, consumer opinions, and others. Upon summarizing and identifying the gaps in these contributions, we utilise an existing design innovation framework to identify the applications that are currently being supported by NLP. We then propose a few methodological and theoretical directions for future NLP in-and-for design research.},
  langid = {english},
  file = {/home/xav/Zotero/storage/XBIW3YNR/Siddharth et al. - Natural Language Processing in-and-for Design Rese.pdf}
}

@misc{SIGIRSpecialInterest,
  title = {{{SIGIR}} | {{Special Interest Group}} on {{Information Retrieval}}},
  urldate = {2023-02-15},
  howpublished = {https://sigir.org/},
  langid = {american},
  file = {/home/xav/Zotero/storage/SNJ6C9T7/sigir.org.html}
}

@misc{SIGKDDSpecialInterest,
  title = {{{SIGKDD}} | {{Special Interest Group}} on {{Knowledge Discovery}} and {{Data Mining}}},
  urldate = {2023-02-15},
  howpublished = {https://www.kdd.org/},
  file = {/home/xav/Zotero/storage/3GT9MCHP/www.kdd.org.html}
}

@article{singhBRRQABoostingRanking2023,
  title = {{{BRR-QA}}: {{Boosting Ranking}} and {{Reading}} in {{Open-Domain Question Answering}}},
  shorttitle = {{{BRR-QA}}},
  author = {Singh, Manish and Shrivastava, Manish},
  year = {2023},
  journal = {Proceedings of the 6th Joint International Conference on Data Science \& Management of Data (10th ACM IKDD CODS and 28th COMAD)},
  urldate = {2023-01-08},
  abstract = {A Passage Ranker model that captures local-context information through cross-passage interaction and uses modified attention in the cross- passage interaction to compute a better confidence score for each passage is presented. Open-domain question qnswering (OpenQA) involves a retriever for selecting relevant passages from large text corpora (e.g. Wikipedia) and a reading comprehension (RC) model for extracting answers from these retrieved passages. The retrieved passages are often noisy. Since OpenQA relies heavily on efficient passages for better answer prediction, many passage ranker models have been proposed to filter out noisy passages. However, their performance is limited because their ranker model scores each passage separately by modelling only the relationship between query and passage. Thus, they could not capture local context information. Their ranker model also ignored the rich initial rank of passages ranked by a search engine. This paper presents a Passage Ranker model that captures local-context information through cross-passage interaction. Our ranker model integrates initial ranking and uses modified attention in the cross-passage interaction to compute a better confidence score for each passage. Moreover, we integrate SRL into our passage reader and train it on proposed sampled data. Our semantic reader can absorb contextual semantics. Experimental results on four public OpenQA datasets show that our model significantly outperforms recent OpenQA baselines.},
  langid = {english}
}

@misc{solaimanProcessAdaptingLanguage2021,
  title = {Process for {{Adapting Language Models}} to {{Society}} ({{PALMS}}) with {{Values-Targeted Datasets}}},
  author = {Solaiman, Irene and Dennison, Christy},
  year = {2021},
  month = nov,
  number = {arXiv:2106.10328},
  eprint = {arXiv:2106.10328},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.10328},
  urldate = {2023-01-16},
  abstract = {Language models can generate harmful and biased outputs and exhibit undesirable behavior according to a given cultural context. We propose a Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to significantly change model behavior by crafting and fine-tuning on a dataset that reflects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs significantly better on all metrics compared to baseline and control models for a broad range of GPT-3 language model sizes without compromising capability integrity. We find that the effectiveness of PALMS increases with model size. We show that significantly adjusting language model behavior is feasible with a small, hand-curated dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/home/xav/Zotero/storage/BNAIMTBZ/Solaiman et Dennison - 2021 - Process for Adapting Language Models to Society (P.pdf;/home/xav/Zotero/storage/2VZMWMPA/2106.html}
}

@article{sopranoManyDimensionsTruthfulness2021,
  title = {The {{Many Dimensions}} of {{Truthfulness}}: {{Crowdsourcing Misinformation Assessments}} on a {{Multidimensional Scale}}},
  shorttitle = {The {{Many Dimensions}} of {{Truthfulness}}},
  author = {Soprano, Michael and Roitero, Kevin and La Barbera, David and Ceolin, Davide and Spina, Damiano and Mizzaro, Stefano and Demartini, Gianluca},
  year = {2021},
  month = nov,
  journal = {Information Processing \& Management},
  volume = {58},
  number = {6},
  eprint = {2108.01222},
  primaryclass = {cs},
  pages = {102710},
  issn = {03064573},
  doi = {10.1016/j.ipm.2021.102710},
  urldate = {2023-02-14},
  abstract = {Recent work has demonstrated the viability of using crowdsourcing as a tool for evaluating the truthfulness of public statements. Under certain conditions such as: (1) having a balanced set of workers with different backgrounds and cognitive abilities; (2) using an adequate set of mechanisms to control the quality of the collected data; and (3) using a coarse grained assessment scale, the crowd can provide reliable identification of fake news. However, fake news are a subtle matter: statements can be just biased (``cherrypicked''), imprecise, wrong, etc. and the unidimensional truth scale used in existing work cannot account for such differences. In this paper we propose a multidimensional notion of truthfulness and we ask the crowd workers to assess seven different dimensions of truthfulness selected based on existing literature: Correctness, Neutrality, Comprehensibility, Precision, Completeness, Speaker's Trustworthiness, and Informativeness. We deploy a set of quality control mechanisms to ensure that the thousands of assessments collected on 180 publicly available fact-checked statements distributed over two datasets are of adequate quality, including a custom search engine used by the crowd workers to find web pages supporting their truthfulness assessments. A comprehensive analysis of crowdsourced judgments shows that: (1) the crowdsourced assessments are reliable when compared to an expert-provided gold standard; (2) the proposed dimensions of truthfulness capture independent pieces of information; (3) the crowdsourcing task can be easily learned by the workers; and (4) the resulting assessments provide a useful basis for a more complete estimation of statement truthfulness.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {68P20,Computer Science - Information Retrieval,H.3},
  file = {/home/xav/Zotero/storage/WPU9MB5P/Soprano et al. - 2021 - The Many Dimensions of Truthfulness Crowdsourcing.pdf}
}

@misc{sorscherNeuralScalingLaws2022,
  title = {Beyond Neural Scaling Laws: Beating Power Law Scaling via Data Pruning},
  shorttitle = {Beyond Neural Scaling Laws},
  author = {Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S.},
  year = {2022},
  month = nov,
  number = {arXiv:2206.14486},
  eprint = {arXiv:2206.14486},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.14486},
  urldate = {2023-02-09},
  abstract = {Widely observed neural scaling laws, in which error falls off as a power of the training set size, model size, or both, have driven substantial performance improvements in deep learning. However, these improvements through scaling alone require considerable costs in compute and energy. Here we focus on the scaling of error with dataset size and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned dataset size. We then test this improved scaling prediction with pruned dataset size empirically, and indeed observe better than power law scaling in practice on ResNets trained on CIFAR-10, SVHN, and ImageNet. Next, given the importance of finding high-quality pruning metrics, we perform the first large-scale benchmarking study of ten different data pruning metrics on ImageNet. We find most existing high performing metrics scale poorly to ImageNet, while the best are computationally intensive and require labels for every image. We therefore developed a new simple, cheap and scalable self-supervised pruning metric that demonstrates comparable performance to the best supervised metrics. Overall, our work suggests that the discovery of good data-pruning metrics may provide a viable path forward to substantially improved neural scaling laws, thereby reducing the resource costs of modern deep learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/78JQBFKK/Sorscher et al. - 2022 - Beyond neural scaling laws beating power law scal.pdf;/home/xav/Zotero/storage/HI88F88E/2206.html}
}

@misc{sovianyCurriculumLearningSurvey2022,
  title = {Curriculum {{Learning}}: {{A Survey}}},
  shorttitle = {Curriculum {{Learning}}},
  author = {Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  year = {2022},
  month = apr,
  number = {arXiv:2101.10382},
  eprint = {arXiv:2101.10382},
  publisher = {{arXiv}},
  urldate = {2023-01-29},
  abstract = {Training machine learning models in a meaningful order, from the easy samples to the hard ones, using curriculum learning can provide performance improvements over the standard training approach based on random data shuffling, without any additional computational costs. Curriculum learning strategies have been successfully employed in all areas of machine learning, in a wide range of tasks. However, the necessity of finding a way to rank the samples from easy to hard, as well as the right pacing function for introducing more difficult data can limit the usage of the curriculum approaches. In this survey, we show how these limits have been tackled in the literature, and we present different curriculum learning instantiations for various tasks in machine learning. We construct a multi-perspective taxonomy of curriculum learning approaches by hand, considering various classification criteria. We further build a hierarchical tree of curriculum learning methods using an agglomerative clustering algorithm, linking the discovered clusters with our taxonomy. At the end, we provide some interesting directions for future work.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/NXS47REV/Soviany et al. - 2022 - Curriculum Learning A Survey.pdf}
}

@misc{srivastavaCollectiveDecisionMakingIdeal2014,
  title = {Collective {{Decision-Making}} in {{Ideal Networks}}: {{The Speed-Accuracy Tradeoff}}},
  shorttitle = {Collective {{Decision-Making}} in {{Ideal Networks}}},
  author = {Srivastava, Vaibhav and Leonard, Naomi Ehrich},
  year = {2014},
  month = feb,
  number = {arXiv:1402.3634},
  eprint = {arXiv:1402.3634},
  publisher = {{arXiv}},
  urldate = {2022-10-07},
  abstract = {We study collective decision-making in a model of human groups, with network interactions, performing two alternative choice tasks. We focus on the speed-accuracy tradeoff, i.e., the tradeoff between a quick decision and a reliable decision, for individuals in the network. We model the evidence aggregation process across the network using a coupled drift diffusion model (DDM) and consider the free response paradigm in which individuals take their time to make the decision. We develop reduced DDMs as decoupled approximations to the coupled DDM and characterize their efficiency. We determine high probability bounds on the error rate and the expected decision time for the reduced DDM. We show the effect of the decision-maker's location in the network on their decision-making performance under several threshold selection criteria. Finally, we extend the coupled DDM to the coupled Ornstein-Uhlenbeck model for decision-making in two alternative choice tasks with recency effects, and to the coupled race model for decision-making in multiple alternative choice tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Multiagent Systems,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {/home/xav/Zotero/storage/G4GJPKC4/Srivastava et Leonard - 2014 - Collective Decision-Making in Ideal Networks The .pdf}
}

@inproceedings{steinbergProblemSolvingApproachData2001,
  title = {Problem-{{Solving Approach}} to {{Data Fusion}}},
  author = {Steinberg, Alan N.},
  year = {2001},
  file = {/home/xav/Zotero/storage/SWUM5NWX/Steinberg - 2001 - Problem-Solving Approach to Data Fusion.pdf}
}

@misc{suGenerativeLongformQuestion2022,
  title = {Generative {{Long-form Question Answering}}: {{Relevance}}, {{Faithfulness}} and {{Succinctness}}},
  shorttitle = {Generative {{Long-form Question Answering}}},
  author = {Su, Dan},
  year = {2022},
  month = nov,
  number = {arXiv:2211.08386},
  eprint = {arXiv:2211.08386},
  publisher = {{arXiv}},
  urldate = {2022-11-28},
  abstract = {In this thesis, we investigated the relevance, faithfulness, and succinctness aspects of Long Form Question Answering (LFQA). LFQA aims to generate an in-depth, paragraph-length answer for a given question, to help bridge the gap between real scenarios and the existing open-domain QA models which can only extract short-span answers. LFQA is quite challenging and under-explored. Few works have been done to build an effective LFQA system. It is even more challenging to generate a good-quality long-form answer relevant to the query and faithful to facts, since a considerable amount of redundant, complementary, or contradictory information will be contained in the retrieved documents. Moreover, no prior work has been investigated to generate succinct answers. We are among the first to research the LFQA task. We pioneered the research direction to improve the answer quality in terms of 1) query-relevance, 2) answer faithfulness, and 3) answer succinctness.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/5I4YA2M3/Su - 2022 - Generative Long-form Question Answering Relevance.pdf}
}

@article{suharaOpinionDigestSimpleFramework2020,
  title = {{{OpinionDigest}}: {{A Simple Framework}} for {{Opinion Summarization}}},
  shorttitle = {{{OpinionDigest}}},
  author = {Suhara, Yoshihiko and Wang, Xiaolan and Angelidis, Stefanos and Tan, Wang-Chiew},
  year = {2020},
  month = may,
  journal = {arXiv:2005.01901 [cs]},
  eprint = {2005.01901},
  primaryclass = {cs},
  urldate = {2022-05-13},
  abstract = {We present OPINIONDIGEST, an abstractive opinion summarization framework, which does not rely on gold-standard summaries for training. The framework uses an Aspect-based Sentiment Analysis model to extract opinion phrases from reviews, and trains a Transformer model to reconstruct the original reviews from these extractions. At summarization time, we merge extractions from multiple reviews and select the most popular ones. The selected opinions are used as input to the trained Transformer model, which verbalizes them into an opinion summary. OPINIONDIGEST can also generate customized summaries, tailored to specific user needs, by filtering the selected opinions according to their aspect and/or sentiment. Automatic evaluation on YELP data shows that our framework outperforms competitive baselines. Human studies on two corpora verify that OPINIONDIGEST produces informative summaries and shows promising customization capabilities1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/G24GPJPT/Suhara et al. - 2020 - OpinionDigest A Simple Framework for Opinion Summ.pdf}
}

@book{suiTriggerGNNTriggerBasedGraph2022,
  title = {Trigger-{{GNN}}: {{A Trigger-Based Graph Neural Network}} for {{Nested Named Entity Recognition}}},
  shorttitle = {Trigger-{{GNN}}},
  author = {Sui, Yuan and Bu, Fanyang and Hu, Yingting and Yan, Wei and Zhang, Liang},
  year = {2022},
  month = apr,
  abstract = {Nested named entity recognition (NER) aims to identify the entity boundaries and recognize categories of the named entities in a complex hierarchical sentence. Some works have been done using character-level, word-level, or lexicon-level based models. However, such researches ignore the role of the complementary annotations. In this paper, we propose a trigger-based graph neural network (Trigger-GNN) to leverage the nested NER. It obtains the complementary annotation embeddings through entity trigger encoding and semantic matching, and tackle nested entity utilizing an efficient graph message passing architecture, aggregation-update mode. We posit that using entity triggers as external annotations can add in complementary supervision signals on the whole sentences. It helps the model to learn and generalize more efficiently and cost-effectively. Experiments show that the Trigger-GNN consistently outperforms the baselines on four public NER datasets, and it can effectively alleviate the nested NER.},
  file = {/home/xav/Zotero/storage/5F5HBEPT/Sui et al. - 2022 - Trigger-GNN A Trigger-Based Graph Neural Network .pdf}
}

@misc{sultanLifeCircusWe2022,
  title = {Life Is a {{Circus}} and {{We}} Are the {{Clowns}}: {{Automatically Finding Analogies}} between {{Situations}} and {{Processes}}},
  shorttitle = {Life Is a {{Circus}} and {{We}} Are the {{Clowns}}},
  author = {Sultan, Oren and Shahaf, Dafna},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12197},
  eprint = {arXiv:2210.12197},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.12197},
  urldate = {2023-01-17},
  abstract = {Analogy-making gives rise to reasoning, abstraction, flexible categorization and counterfactual inference -- abilities lacking in even the best AI systems today. Much research has suggested that analogies are key to non-brittle systems that can adapt to new domains. Despite their importance, analogies received little attention in the NLP community, with most research focusing on simple word analogies. Work that tackled more complex analogies relied heavily on manually constructed, hard-to-scale input representations. In this work, we explore a more realistic, challenging setup: our input is a pair of natural language procedural texts, describing a situation or a process (e.g., how the heart works/how a pump works). Our goal is to automatically extract entities and their relations from the text and find a mapping between the different domains based on relational similarity (e.g., blood is mapped to water). We develop an interpretable, scalable algorithm and demonstrate that it identifies the correct mappings 87\% of the time for procedural texts and 94\% for stories from cognitive-psychology literature. We show it can extract analogies from a large dataset of procedural texts, achieving 79\% precision (analogy prevalence in data: 3\%). Lastly, we demonstrate that our algorithm is robust to paraphrasing the input texts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/C2WQ98PN/Sultan et Shahaf - 2022 - Life is a Circus and We are the Clowns Automatica.pdf}
}

@misc{sunBBTv2GradientFreeFuture2022,
  title = {{{BBTv2}}: {{Towards}} a {{Gradient-Free Future}} with {{Large Language Models}}},
  shorttitle = {{{BBTv2}}},
  author = {Sun, Tianxiang and He, Zhengfu and Qian, Hong and Zhou, Yunhua and Huang, Xuanjing and Qiu, Xipeng},
  year = {2022},
  month = oct,
  number = {arXiv:2205.11200},
  eprint = {arXiv:2205.11200},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11200},
  urldate = {2022-12-21},
  abstract = {Most downstream adaptation methods tune all or part of the parameters of pre-trained models (PTMs) through gradient descent, where the tuning cost increases linearly with the growth of the model size. By contrast, gradient-free methods only require the forward computation of the PTM to tune the prompt, retaining the benefits of efficient tuning and deployment. Though, past work on gradient-free tuning often introduces gradient descent to seek a good initialization of prompt and lacks versatility across tasks and PTMs. In this paper, we present BBTv2, an improved version of Black-Box Tuning, to drive PTMs for few-shot learning. We prepend continuous prompts to every layer of the PTM and propose a divide-and-conquer gradient-free algorithm to optimize the prompts at different layers alternately. Extensive experiments across various tasks and PTMs show that BBTv2 can achieve comparable performance to full model tuning and state-of-the-art parameter-efficient methods (e.g., Adapter, LoRA, BitFit, etc.) under few-shot settings while maintaining much fewer tunable parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/3Q2YNYEC/Sun et al. - 2022 - BBTv2 Towards a Gradient-Free Future with Large L.pdf}
}

@misc{sunContrastiveLearningReduces2022,
  title = {Contrastive {{Learning Reduces Hallucination}} in {{Conversations}}},
  author = {Sun, Weiwei and Shi, Zhengliang and Gao, Shen and Ren, Pengjie and {de Rijke}, Maarten and Ren, Zhaochun},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10400},
  eprint = {arXiv:2212.10400},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10400},
  urldate = {2023-02-09},
  abstract = {Pre-trained language models (LMs) store knowledge in their parameters and can generate informative responses when used in conversational systems. However, LMs suffer from the problem of "hallucination:" they may generate plausible-looking statements that are irrelevant or factually incorrect. To address this problem, we propose a contrastive learning scheme, named MixCL. A novel mixed contrastive objective is proposed to explicitly optimize the implicit knowledge elicitation process of LMs, and thus reduce their hallucination in conversations. We also examine negative sampling strategies of retrieved hard negatives and model-generated negatives. We conduct experiments on Wizard-of-Wikipedia, a public, open-domain knowledge-grounded dialogue benchmark, and assess the effectiveness of MixCL. MixCL effectively reduces the hallucination of LMs in conversations and achieves the highest performance among LM-based dialogue agents in terms of relevancy and factuality. We show that MixCL achieves comparable performance to state-of-the-art KB-based approaches while enjoying notable advantages in terms of efficiency and scalability.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/YWIPML9I/Sun et al. - 2022 - Contrastive Learning Reduces Hallucination in Conv.pdf;/home/xav/Zotero/storage/HKMZSQLH/2212.html}
}

@article{sunERNIELargescaleKnowledge2021,
  title = {{{ERNIE}} 3.0: {{Large-scale Knowledge Enhanced Pre-training}} for {{Language Understanding}} and {{Generation}}},
  shorttitle = {{{ERNIE}} 3.0},
  author = {Sun, Yu and Wang, Shuohuan and Feng, Shikun and Ding, Siyu and Pang, Chao and Shang, Junyuan and Liu, Jiaxiang and Chen, Xuyi and Zhao, Yanbin and Lu, Yuxiang and Liu, Weixin and Wu, Zhihua and Gong, Weibao and Liang, Jianzhong and Shang, Zhizhou and Sun, Peng and Liu, Wei and Ouyang, Xuan and Yu, Dianhai and Tian, Hao and Wu, Hua and Wang, Haifeng},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.02137 [cs]},
  eprint = {2107.02137},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Pre-trained models have achieved state-of-the-art results in various Natural Language Processing (NLP) tasks. Recent works such as T5 and GPT-3 have shown that scaling up pre-trained language models can improve their generalization abilities. Particularly, the GPT-3 model with 175 billion parameters shows its strong task-agnostic zero-shot/few-shot learning capabilities. Despite their success, these large-scale models are trained on plain texts without introducing knowledge such as linguistic knowledge and world knowledge. In addition, most large-scale models are trained in an auto-regressive way. As a result, this kind of traditional fine-tuning approach demonstrates relatively weak performance when solving downstream language understanding tasks. In order to solve the above problems, we propose a unified framework named ERNIE 3.0 for pre-training large-scale knowledge enhanced models. It fuses auto-regressive network and auto-encoding network, so that the trained model can be easily tailored for both natural language understanding and generation tasks with zero-shot learning, few-shot learning or fine-tuning. We trained the model with 10 billion parameters on a 4TB corpus consisting of plain texts and a large-scale knowledge graph. Empirical results show that the model outperforms the state-of-the-art models on 54 Chinese NLP tasks, and its English version achieves the first place on the SuperGLUE benchmark (July 3, 2021), surpassing the human performance by +0.8\% (90.6\% vs. 89.8\%).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/TE5U7LBG/Sun et al. - 2021 - ERNIE 3.0 Large-scale Knowledge Enhanced Pre-trai.pdf;/home/xav/Zotero/storage/JHBVXDC3/2107.html}
}

@misc{sungOptimizingTestTimeQuery2022,
  title = {Optimizing {{Test-Time Query Representations}} for {{Dense Retrieval}}},
  author = {Sung, Mujeen and Park, Jungsoo and Kang, Jaewoo and Chen, Danqi and Lee, Jinhyuk},
  year = {2022},
  month = dec,
  number = {arXiv:2205.12680},
  eprint = {arXiv:2205.12680},
  publisher = {{arXiv}},
  urldate = {2023-03-21},
  abstract = {Recent developments of dense retrieval rely on quality representations of queries and contexts coming from pre-trained query and context encoders. In this paper, we introduce TouR (test-time optimization of query representations), which further optimizes instance-level query representations guided by signals from test-time retrieval results. We leverage a cross-encoder re-ranker to provide fine-grained pseudo labels over retrieval results and iteratively optimize query representations with the gradient descent method. Our theoretical analysis reveals that TouR can be viewed as a generalization of the classical Rocchio's algorithm for pseudo relevance feedback, and we present two variants leveraging psuedo labels as either hard binary or soft continuous labels. We first apply TouR on phrase retrieval with our proposed phrase re-ranker. On passage retrieval, we demonstrate its effectiveness with an off-the-shelf re-ranker. TouR improves the end-to-end open-domain QA accuracy significantly, as well as passage retrieval performance. Compared to re-ranker, TouR requires a smaller number of candidates, and achieves consistently better performance and runs up to 4x faster with our efficient implementation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/S3FSK5VY/Sung et al. - 2022 - Optimizing Test-Time Query Representations for Den.pdf}
}

@misc{sunImportanceBuildingHighquality2022,
  title = {On the {{Importance}} of {{Building High-quality Training Datasets}} for {{Neural Code Search}}},
  author = {Sun, Zhensu and Li, Li and Liu, Yan and Du, Xiaoning and Li, Li},
  year = {2022},
  month = feb,
  number = {arXiv:2202.06649},
  eprint = {arXiv:2202.06649},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {The performance of neural code search is significantly influenced by the quality of the training data from which the neural models are derived. A large corpus of high-quality query and code pairs is demanded to establish a precise mapping from the natural language to the programming language. Due to the limited availability, most widely-used code search datasets are established with compromise, such as using code comments as a replacement of queries. Our empirical study on a famous code search dataset reveals that over one-third of its queries contain noises that make them deviate from natural user queries. Models trained through noisy data are faced with severe performance degradation when applied in real-world scenarios. To improve the dataset quality and make the queries of its samples semantically identical to real user queries is critical for the practical usability of neural code search. In this paper, we propose a data cleaning framework consisting of two subsequent filters: a rule-based syntactic filter and a model-based semantic filter. This is the first framework that applies semantic query cleaning to code search datasets. Experimentally, we evaluated the effectiveness of our framework on two widely-used code search models and three manually-annotated code retrieval benchmarks. Training the popular DeepCS model with the filtered dataset from our framework improves its performance by 19.2\% MRR and 21.3\% Answer@1, on average with the three validation benchmarks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/home/xav/Zotero/storage/HAUCHW9S/Sun et al. - 2022 - On the Importance of Building High-quality Trainin.pdf}
}

@article{sunParadigmShiftNatural2021,
  title = {Paradigm {{Shift}} in {{Natural Language Processing}}},
  author = {Sun, Tianxiang and Liu, Xiangyang and Qiu, Xipeng and Huang, Xuanjing},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.12575 [cs]},
  eprint = {2109.12575},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {In the era of deep learning, modeling for most NLP tasks has converged to several mainstream paradigms. For example, we usually adopt the sequence labeling paradigm to solve a bundle of tasks such as POS-tagging, NER, Chunking, and adopt the classification paradigm to solve tasks like sentiment analysis. With the rapid progress of pre-trained language models, recent years have observed a rising trend of Paradigm Shift, which is solving one NLP task by reformulating it as another one. Paradigm shift has achieved great success on many tasks, becoming a promising way to improve model performance. Moreover, some of these paradigms have shown great potential to unify a large number of NLP tasks, making it possible to build a single model to handle diverse tasks. In this paper, we review such phenomenon of paradigm shifts in recent years, highlighting several paradigms that have the potential to solve different NLP tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/E4DAXMCX/Sun et al. - 2021 - Paradigm Shift in Natural Language Processing.pdf}
}

@misc{sunTSGPTwoStageGenerative2022,
  title = {{{TSGP}}: {{Two-Stage Generative Prompting}} for {{Unsupervised Commonsense Question Answering}}},
  shorttitle = {{{TSGP}}},
  author = {Sun, Yueqing and Zhang, Yu and Qi, Le and Shi, Qi},
  year = {2022},
  month = nov,
  number = {arXiv:2211.13515},
  eprint = {arXiv:2211.13515},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.13515},
  urldate = {2023-02-07},
  abstract = {Unsupervised commonsense question answering requires mining effective commonsense knowledge without the rely on the labeled task data. Previous methods typically retrieved from traditional knowledge bases or used pre-trained language models (PrLMs) to generate fixed types of knowledge, which have poor generalization ability. In this paper, we aim to address the above limitation by leveraging the implicit knowledge stored in PrLMs and propose a two-stage prompt-based unsupervised commonsense question answering framework (TSGP). Specifically, we first use knowledge generation prompts to generate the knowledge required for questions with unlimited types and possible candidate answers independent of specified choices. Then, we further utilize answer generation prompts to generate possible candidate answers independent of specified choices. Experimental results and analysis on three different commonsense reasoning tasks, CommonsenseQA, OpenBookQA, and SocialIQA, demonstrate that TSGP significantly improves the reasoning ability of language models in unsupervised settings. Our code is available at: https://github.com/Yueqing-Sun/TSGP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/RWWAGRYT/Sun et al. - 2022 - TSGP Two-Stage Generative Prompting for Unsupervi.pdf;/home/xav/Zotero/storage/NUE3BZS8/2211.html}
}

@misc{suSelectiveAnnotationMakes2022,
  title = {Selective {{Annotation Makes Language Models Better Few-Shot Learners}}},
  author = {Su, Hongjin and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
  year = {2022},
  month = sep,
  number = {arXiv:2209.01975},
  eprint = {arXiv:2209.01975},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.01975},
  urldate = {2022-09-15},
  abstract = {Many recent approaches to natural language tasks are built on the remarkable abilities of large language models. Large language models can perform in-context learning, where they learn a new task from a few task demonstrations, without any parameter updates. This work examines the implications of in-context learning for the creation of datasets for new natural language tasks. Departing from recent in-context learning methods, we formulate an annotation-efficient, two-step framework: selective annotation that chooses a pool of examples to annotate from unlabeled data in advance, followed by prompt retrieval that retrieves task examples from the annotated pool at test time. Based on this framework, we propose an unsupervised, graph-based selective annotation method, voke-k, to select diverse, representative examples to annotate. Extensive experiments on 10 datasets (covering classification, commonsense reasoning, dialogue, and text/code generation) demonstrate that our selective annotation method improves the task performance by a large margin. On average, vote-k achieves a 12.9\%/11.4\% relative gain under an annotation budget of 18/100, as compared to randomly selecting examples to annotate. Compared to state-of-the-art supervised finetuning approaches, it yields similar performance with 10-100x less annotation cost across 10 tasks. We further analyze the effectiveness of our framework in various scenarios: language models with varying sizes, alternative selective annotation methods, and cases where there is a test data domain shift. We hope that our studies will serve as a basis for data annotations as large language models are increasingly applied to new tasks. Our code is available at https://github.com/HKUNLP/icl-selective-annotation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/3QUSEM8G/Su et al. - 2022 - Selective Annotation Makes Language Models Better .pdf}
}

@article{susskindNeuroSymbolicAIEmerging2021,
  title = {Neuro-{{Symbolic AI}}: {{An Emerging Class}} of {{AI Workloads}} and Their {{Characterization}}},
  shorttitle = {Neuro-{{Symbolic AI}}},
  author = {Susskind, Zachary and Arden, Bryce and John, Lizy K. and Stockton, Patrick and John, Eugene B.},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.06133 [cs]},
  eprint = {2109.06133},
  primaryclass = {cs},
  urldate = {2022-05-11},
  abstract = {Neuro-symbolic artificial intelligence is a novel area of AI research which seeks to combine traditional rules-based AI approaches with modern deep learning techniques. Neuro-symbolic models have already demonstrated the capability to outperform state-of-the-art deep learning models in domains such as image and video reasoning. They have also been shown to obtain high accuracy with significantly less training data than traditional models. Due to the recency of the field's emergence and relative sparsity of published results, the performance characteristics of these models are not well understood. In this paper, we describe and analyze the performance characteristics of three recent neuro-symbolic models. We find that symbolic models have less potential parallelism than traditional neural models due to complex control flow and low-operational-intensity operations, such as scalar multiplication and tensor addition. However, the neural aspect of computation dominates the symbolic part in cases where they are clearly separable. We also find that data movement poses a potential bottleneck, as it does in many ML workloads.},
  archiveprefix = {arxiv},
  keywords = {C.4,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Performance,I.2.m},
  file = {/home/xav/Zotero/storage/S7UNGF96/Susskind et al. - 2021 - Neuro-Symbolic AI An Emerging Class of AI Workloa.pdf;/home/xav/Zotero/storage/4RPWWCAD/2109.html}
}

@inproceedings{suttonDigitizedTrustHumanintheLoop2018,
  title = {Digitized {{Trust}} in {{Human-in-the-Loop Health Research}}},
  booktitle = {2018 16th {{Annual Conference}} on {{Privacy}}, {{Security}} and {{Trust}} ({{PST}})},
  author = {Sutton, Andrew and Samavi, Reza and Doyle, Thomas E. and Koff, David},
  year = {2018},
  month = aug,
  pages = {1--10},
  publisher = {{IEEE}},
  address = {{Belfast}},
  doi = {10.1109/PST.2018.8514168},
  urldate = {2022-03-21},
  abstract = {In this paper, we propose an architecture that utilizes blockchain technology for enabling verifiable trust in collaborative health research environments. The architecture supports the human-in-the-loop paradigm for health research by establishing trust between participants, including human researchers and AI systems, by making all data transformations transparent and verifiable by all participants. We define the trustworthiness of the system and provide an analysis of the architecture in terms of trust requirements. We then evaluate our architecture by analyzing its resiliency to common security threats and through an experimental realization.},
  isbn = {978-1-5386-7493-2},
  langid = {english},
  file = {/home/xav/Zotero/storage/5QIS8YJC/Sutton et al. - 2018 - Digitized Trust in Human-in-the-Loop Health Resear.pdf}
}

@book{suttonReinforcementLearningSecond2018,
  title = {Reinforcement {{Learning}}, Second Edition: {{An Introduction}}},
  shorttitle = {Reinforcement {{Learning}}, Second Edition},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  month = nov,
  publisher = {{MIT Press}},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics.Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
  googlebooks = {uWV0DwAAQBAJ},
  isbn = {978-0-262-35270-3},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / General}
}

@misc{suzgunChallengingBIGBenchTasks2022a,
  title = {Challenging {{BIG-Bench Tasks}} and {{Whether Chain-of-Thought Can Solve Them}}},
  author = {Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V. and Chi, Ed H. and Zhou, Denny and Wei, Jason},
  year = {2022},
  month = oct,
  number = {arXiv:2210.09261},
  eprint = {arXiv:2210.09261},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.09261},
  urldate = {2022-11-23},
  abstract = {BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65\% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard (BBH). These are the task for which prior language model evaluations did not outperform the average human-rater. We find that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, finding that CoT enables emergent task performance on several BBH tasks with otherwise flat scaling curves.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/25INUR7V/Suzgun et al. - 2022 - Challenging BIG-Bench Tasks and Whether Chain-of-T.pdf}
}

@article{syedSurveyStateoftheArtModels2021,
  title = {A {{Survey}} of the {{State-of-the-Art Models}} in {{Neural Abstractive Text Summarization}}},
  author = {Syed, Ayesha Ayub and Gaol, Ford Lumban and Matsuo, Tokuro},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {13248--13265},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3052783},
  abstract = {Dealing with vast amounts of textual data requires the use of efficient systems. Automatic summarization systems are capable of addressing this issue. Therefore, it becomes highly essential to work on the design of existing automatic summarization systems and innovate them to make them capable of meeting the demands of continuously increasing data, based on user needs. This study tends to survey the scientific literature to obtain information and knowledge about the recent research in automatic text summarization specifically abstractive summarization based on neural networks. A review of various neural networks based abstractive summarization models have been presented. The proposed conceptual framework includes five key elements identified as encoder-decoder architecture, mechanisms, training strategies and optimization algorithms, dataset, and evaluation metric. A description of these elements is also included in this article. The purpose of this research is to provide an overall understanding and familiarity with the elements of recent neural networks based abstractive text summarization models with an up-to-date review as well as to render an awareness of the challenges and issues with these systems. Analysis has been performed qualitatively with the help of a concept matrix indicating common trends in the design of recent neural abstractive summarization systems. Models employing a transformer-based encoder-decoder architecture are found to be the new state-of-the-art. Based on the knowledge acquired from the survey, this article suggests the use of pre-trained language models in complement with neural network architecture for abstractive summarization task.},
  keywords = {Abstractive text summarization,attention,Context modeling,Data models,decoder,encoder,evaluation,Mathematical model,Neural networks,Neurons,optimization,Semantics,Task analysis,training,transformer},
  file = {/home/xav/Zotero/storage/BPC346KX/Syed et al. - 2021 - A Survey of the State-of-the-Art Models in Neural .pdf;/home/xav/Zotero/storage/PTQ6DHZ2/9328413.html}
}

@misc{tafjordEntailerAnsweringQuestions2022,
  title = {Entailer: {{Answering Questions}} with {{Faithful}} and {{Truthful Chains}} of {{Reasoning}}},
  shorttitle = {Entailer},
  author = {Tafjord, Oyvind and Mishra, Bhavana Dalvi and Clark, Peter},
  year = {2022},
  month = oct,
  number = {arXiv:2210.12217},
  eprint = {arXiv:2210.12217},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.12217},
  urldate = {2023-02-08},
  abstract = {Our goal is a question-answering (QA) system that can show how its answers are implied by its own internal beliefs via a systematic chain of reasoning. Such a capability would allow better understanding of why a model produced the answer it did. Our approach is to recursively combine a trained backward-chaining model, capable of generating a set of premises entailing an answer hypothesis, with a verifier that checks that the model itself believes those premises (and the entailment itself) through self-querying. To our knowledge, this is the first system to generate multistep chains that are both faithful (the answer follows from the reasoning) and truthful (the chain reflects the system's own internal beliefs). In evaluation using two different datasets, users judge that a majority (70\%+) of generated chains clearly show how an answer follows from a set of facts - substantially better than a high-performance baseline - while preserving answer accuracy. By materializing model beliefs that systematically support an answer, new opportunities arise for understanding the model's system of belief, and diagnosing and correcting its misunderstandings when an answer is wrong.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/TQN82742/Tafjord et al. - 2022 - Entailer Answering Questions with Faithful and Tr.pdf;/home/xav/Zotero/storage/Y7A9HGY4/2210.html}
}

@misc{tafjordProofWriterGeneratingImplications2021,
  title = {{{ProofWriter}}: {{Generating Implications}}, {{Proofs}}, and {{Abductive Statements}} over {{Natural Language}}},
  shorttitle = {{{ProofWriter}}},
  author = {Tafjord, Oyvind and Mishra, Bhavana Dalvi and Clark, Peter},
  year = {2021},
  month = jun,
  number = {arXiv:2012.13048},
  eprint = {arXiv:2012.13048},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.13048},
  urldate = {2023-01-30},
  abstract = {Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9\% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/JHVAGC8W/Tafjord et al. - 2021 - ProofWriter Generating Implications, Proofs, and .pdf;/home/xav/Zotero/storage/BGAWH3KV/2012.html}
}

@misc{tafjordQuaRelDatasetModels2018,
  title = {{{QuaRel}}: {{A Dataset}} and {{Models}} for {{Answering Questions}} about {{Qualitative Relationships}}},
  shorttitle = {{{QuaRel}}},
  author = {Tafjord, Oyvind and Clark, Peter and Gardner, Matt and Yih, Wen-tau and Sabharwal, Ashish},
  year = {2018},
  month = nov,
  number = {arXiv:1811.08048},
  eprint = {arXiv:1811.08048},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.08048},
  urldate = {2023-02-01},
  abstract = {Many natural language questions require recognizing and reasoning with qualitative relationships (e.g., in science, economics, and medicine), but are challenging to answer with corpus-based methods. Qualitative modeling provides tools that support such reasoning, but the semantic parsing task of mapping questions into those models has formidable challenges. We present QuaRel, a dataset of diverse story questions involving qualitative relationships that characterize these challenges, and techniques that begin to address them. The dataset has 2771 questions relating 19 different types of quantities. For example, "Jenny observes that the robot vacuum cleaner moves slower on the living room carpet than on the bedroom carpet. Which carpet has more friction?" We contribute (1) a simple and flexible conceptual framework for representing these kinds of questions; (2) the QuaRel dataset, including logical forms, exemplifying the parsing challenges; and (3) two novel models for this task, built as extensions of type-constrained semantic parsing. The first of these models (called QuaSP+) significantly outperforms off-the-shelf tools on QuaRel. The second (QuaSP+Zero) demonstrates zero-shot capability, i.e., the ability to handle new qualitative relationships without requiring additional training data, something not possible with previous models. This work thus makes inroads into answering complex, qualitative questions that require reasoning, and scaling to new relationships at low cost. The dataset and models are available at http://data.allenai.org/quarel.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BYA8DY7C/Tafjord et al. - 2018 - QuaRel A Dataset and Models for Answering Questio.pdf;/home/xav/Zotero/storage/NDKBWLE9/1811.html}
}

@inproceedings{tafjordQuaRTzOpenDomainDataset2019,
  title = {{{QuaRTz}}: {{An Open-Domain Dataset}} of {{Qualitative Relationship Questions}}},
  shorttitle = {{{QuaRTz}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Tafjord, Oyvind and Gardner, Matt and Lin, Kevin and Clark, Peter},
  year = {2019},
  month = nov,
  pages = {5941--5946},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1608},
  urldate = {2023-02-01},
  abstract = {We introduce the first open-domain dataset, called QuaRTz, for reasoning about textual qualitative relationships. QuaRTz contains general qualitative statements, e.g., ``A sunscreen with a higher SPF protects the skin longer.'', twinned with 3864 crowdsourced situated questions, e.g., ``Billy is wearing sunscreen with a lower SPF than Lucy. Who will be best protected from the sun?'', plus annotations of the properties being compared. Unlike previous datasets, the general knowledge is textual and not tied to a fixed set of relationships, and tests a system's ability to comprehend and apply textual qualitative knowledge in a novel setting. We find state-of-the-art results are substantially (20\%) below human performance, presenting an open challenge to the NLP community.},
  file = {/home/xav/Zotero/storage/7A68VLUP/Tafjord et al. - 2019 - QuaRTz An Open-Domain Dataset of Qualitative Rela.pdf}
}

@article{talmorMultiModalQAComplexQuestion2021,
  title = {{{MultiModalQA}}: {{Complex Question Answering}} over {{Text}}, {{Tables}} and {{Images}}},
  shorttitle = {{{MultiModalQA}}},
  author = {Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.06039 [cs]},
  eprint = {2104.06039},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {When answering complex questions, people can seamlessly combine information from visual, textual and tabular sources. While interest in models that reason over multiple pieces of evidence has surged in recent years, there has been relatively little work on question answering models that reason across multiple modalities. In this paper, we present MultiModalQA(MMQA): a challenging question answering dataset that requires joint reasoning over text, tables and images. We create MMQA using a new framework for generating complex multi-modal questions at scale, harvesting tables from Wikipedia, and attaching images and text paragraphs using entities that appear in each table. We then define a formal language that allows us to take questions that can be answered from a single modality, and combine them to generate cross-modal questions. Last, crowdsourcing workers take these automatically-generated questions and rephrase them into more fluent language. We create 29,918 questions through this procedure, and empirically demonstrate the necessity of a multi-modal multi-hop approach to solve our task: our multi-hop model, ImplicitDecomp, achieves an average F1of 51.7 over cross-modal questions, substantially outperforming a strong baseline that achieves 38.2 F1, but still lags significantly behind human performance, which is at 90.1 F1},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ZXLXFNDD/Talmor et al. - 2021 - MultiModalQA Complex Question Answering over Text.pdf;/home/xav/Zotero/storage/D7YZZWDL/2104.html}
}

@article{talmorWebKnowledgebaseAnswering2018,
  title = {The {{Web}} as a {{Knowledge-base}} for {{Answering Complex Questions}}},
  author = {Talmor, Alon and Berant, Jonathan},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.06643 [cs]},
  eprint = {1803.06643},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, ComplexWebQuestions, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/PFIRFHVN/Talmor et Berant - 2018 - The Web as a Knowledge-base for Answering Complex .pdf;/home/xav/Zotero/storage/99WNC97L/1803.html}
}

@misc{tamParameterEfficientPromptTuning2022,
  title = {Parameter-{{Efficient Prompt Tuning Makes Generalized}} and {{Calibrated Neural Text Retrievers}}},
  author = {Tam, Weng Lam and Liu, Xiao and Ji, Kaixuan and Xue, Lilong and Zhang, Xingjian and Dong, Yuxiao and Liu, Jiahua and Hu, Maodi and Tang, Jie},
  year = {2022},
  month = jul,
  number = {arXiv:2207.07087},
  eprint = {arXiv:2207.07087},
  publisher = {{arXiv}},
  urldate = {2022-08-04},
  abstract = {Prompt tuning attempts to update few task-specific parameters in pre-trained models. It has achieved comparable performance to fine-tuning of the full parameter set on both language understanding and generation tasks. In this work, we study the problem of prompt tuning for neural text retrievers. We introduce parameter-efficient prompt tuning for text retrieval across in-domain, cross-domain, and cross-topic settings. Through an extensive analysis, we show that the strategy can mitigate the two issues -- parameter-inefficiency and weak generalizability -- faced by fine-tuning based retrieval methods. Notably, it can significantly improve the out-of-domain zero-shot generalization of the retrieval models. By updating only 0.1\% of the model parameters, the prompt tuning strategy can help retrieval models achieve better generalization performance than traditional methods in which all parameters are updated. Finally, to facilitate research on retrievers' cross-topic generalizability, we curate and release an academic retrieval dataset with 18K query-results pairs in 87 topics, making it the largest topic-specific one to date.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/73D9UZ2Q/Tam et al. - 2022 - Parameter-Efficient Prompt Tuning Makes Generalize.pdf}
}

@misc{tangYouNeedMultiple2022,
  title = {You {{Need Multiple Exiting}}: {{Dynamic Early Exiting}} for {{Accelerating Unified Vision Language Model}}},
  shorttitle = {You {{Need Multiple Exiting}}},
  author = {Tang, Shengkun and Wang, Yaqing and Kong, Zhenglun and Zhang, Tianchi and Li, Yao and Ding, Caiwen and Wang, Yanzhi and Liang, Yi and Xu, Dongkuan},
  year = {2022},
  month = nov,
  number = {arXiv:2211.11152},
  eprint = {arXiv:2211.11152},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.11152},
  urldate = {2023-02-09},
  abstract = {Large-scale Transformer models bring significant improvements for various downstream vision language tasks with a unified architecture. The performance improvements come with increasing model size, resulting in slow inference speed and increased cost for severing. While some certain predictions benefit from the full complexity of the large-scale model, not all of inputs need the same amount of computation to conduct, potentially leading to computation resource waste. To handle this challenge, early exiting is proposed to adaptively allocate computational power in term of input complexity to improve inference efficiency. The existing early exiting strategies usually adopt output confidence based on intermediate layers as a proxy of input complexity to incur the decision of skipping following layers. However, such strategies cannot apply to encoder in the widely-used unified architecture with both encoder and decoder due to difficulty of output confidence estimation in the encoder. It is suboptimal in term of saving computation power to ignore the early exiting in encoder component. To handle this challenge, we propose a novel early exiting strategy for unified visual language models, which allows dynamically skip the layers in encoder and decoder simultaneously in term of input layer-wise similarities with multiple times of early exiting, namely \textbackslash textbf\{MuE\}. By decomposing the image and text modalities in the encoder, MuE is flexible and can skip different layers in term of modalities, advancing the inference efficiency while minimizing performance drop. Experiments on the SNLI-VE and MS COCO datasets show that the proposed approach MuE can reduce expected inference time by up to 50\textbackslash\% and 40\textbackslash\% while maintaining 99\textbackslash\% and 96\textbackslash\% performance respectively.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/8UMASA3D/Tang et al. - 2022 - You Need Multiple Exiting Dynamic Early Exiting f.pdf;/home/xav/Zotero/storage/NRAJJGVP/2211.html}
}

@misc{tapaswiMovieQAUnderstandingStories2016,
  title = {{{MovieQA}}: {{Understanding Stories}} in {{Movies}} through {{Question-Answering}}},
  shorttitle = {{{MovieQA}}},
  author = {Tapaswi, Makarand and Zhu, Yukun and Stiefelhagen, Rainer and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  year = {2016},
  month = sep,
  number = {arXiv:1512.02902},
  eprint = {arXiv:1512.02902},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.02902},
  urldate = {2023-01-24},
  abstract = {We introduce the MovieQA dataset which aims to evaluate automatic story comprehension from both video and text. The dataset consists of 14,944 questions about 408 movies with high semantic diversity. The questions range from simpler "Who" did "What" to "Whom", to "Why" and "How" certain events occurred. Each question comes with a set of five possible answers; a correct one and four deceiving answers provided by human annotators. Our dataset is unique in that it contains multiple sources of information -- video clips, plots, subtitles, scripts, and DVS. We analyze our data through various statistics and methods. We further extend existing QA techniques to show that question-answering with such open-ended semantics is hard. We make this data set public along with an evaluation benchmark to encourage inspiring work in this challenging domain.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/JT3BXX8M/Tapaswi et al. - 2016 - MovieQA Understanding Stories in Movies through Q.pdf;/home/xav/Zotero/storage/BS3MHGUI/1512.html}
}

@misc{tayEfficientTransformersSurvey2022,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2022},
  month = mar,
  number = {arXiv:2009.06732},
  eprint = {arXiv:2009.06732},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.06732},
  urldate = {2023-01-05},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/5M4KMUW8/Tay et al. - 2022 - Efficient Transformers A Survey.pdf;/home/xav/Zotero/storage/U64AI6A5/2009.html}
}

@misc{taylorGalacticaLargeLanguage2022,
  title = {Galactica: {{A Large Language Model}} for {{Science}}},
  shorttitle = {Galactica},
  author = {Taylor, Ross and Kardas, Marcin and Cucurull, Guillem and Scialom, Thomas and Hartshorn, Anthony and Saravia, Elvis and Poulton, Andrew and Kerkez, Viktor and Stojnic, Robert},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09085},
  eprint = {arXiv:2211.09085},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.09085},
  urldate = {2023-01-22},
  abstract = {Information overload is a major obstacle to scientific progress. The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information. Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone. In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge. We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources. We outperform existing models on a range of scientific tasks. On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2\% versus 49.0\%. Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3\% to 35.7\%, and PaLM 540B on MATH with a score of 20.4\% versus 8.8\%. It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6\% and 52.9\%. And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench. We believe these results demonstrate the potential for language models as a new interface for science. We open source the model for the benefit of the scientific community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/PRAYYKG7/Taylor et al. - 2022 - Galactica A Large Language Model for Science.pdf;/home/xav/Zotero/storage/HHSWLAJE/2211.html}
}

@misc{tayTranscendingScalingLaws2022,
  title = {Transcending {{Scaling Laws}} with 0.1\% {{Extra Compute}}},
  author = {Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q. and So, David R. and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and Zhou, Denny and Metzler, Donald and Petrov, Slav and Houlsby, Neil and Le, Quoc V. and Dehghani, Mostafa},
  year = {2022},
  month = nov,
  number = {arXiv:2210.11399},
  eprint = {arXiv:2210.11399},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.11399},
  urldate = {2023-02-03},
  abstract = {Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving \$\textbackslash sim\$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/GYPU23D2/Tay et al. - 2022 - Transcending Scaling Laws with 0.1% Extra Compute.pdf;/home/xav/Zotero/storage/4N83FIXB/2210.html}
}

@misc{tayUL2UnifyingLanguage2022,
  title = {{{UL2}}: {{Unifying Language Learning Paradigms}}},
  shorttitle = {{{UL2}}},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  year = {2022},
  month = oct,
  number = {arXiv:2205.05131},
  eprint = {arXiv:2205.05131},
  publisher = {{arXiv}},
  urldate = {2023-02-03},
  abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives \textendash{} two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On zero-shot MMLU, UL2 20B outperforms T0 and T5 models. Finally, we show that UL2 20B works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. We release Flax-based T5X model checkpoints for the 20B model at https://github.com/google-research/google-research/tree/master/ul2.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/Q7WMYZQ9/Tay et al. - 2022 - UL2 Unifying Language Learning Paradigms.pdf}
}

@misc{tekiKnowledgeDistillationPrinciples2022,
  title = {Knowledge {{Distillation}}: {{Principles}}, {{Algorithms}}, {{Applications}}},
  shorttitle = {Knowledge {{Distillation}}},
  author = {Teki, Sundeep},
  year = {2022},
  month = jul,
  journal = {neptune.ai},
  urldate = {2023-01-16},
  abstract = {Large-scale machine learning and deep learning models are increasingly common. For instance, GPT-3 is trained on 570 GB of text and consists of 175 billion parameters. However, whilst training large models helps improve state-of-the-art performance, deploying such cumbersome models especially on edge devices is not straightforward. Additionally, the majority of data science modeling work focuses\ldots},
  howpublished = {https://neptune.ai/blog/knowledge-distillation},
  langid = {american}
}

@article{tessariRulesProblemSolving2015,
  title = {Rules for {{Problem Solving}}: {{Qualitative Analysis}} and {{Compilation}} of {{Existing Inventive Heuristics}} of {{TRIZ}}},
  shorttitle = {Rules for {{Problem Solving}}},
  author = {Tessari, Rodolfo and De Carvalho, Marco},
  year = {2015},
  month = mar,
  journal = {Applied Mechanics and Materials},
  volume = {741},
  pages = {827--849},
  doi = {10.4028/www.scientific.net/AMM.741.827},
  abstract = {Heuristics are widely accepted and used as tools for inventive problem solving. A problem when using heuristics is that their number is relatively high and ever growing (469 heuristics were analyzed only in this research). This amount of heuristics makes it necessary for problem solvers to spend a significant amount of time in understanding them, finding the most suitable ones to their specific situations and using them. This article presents a first step towards decreasing this complexity. The main heuristics to solve inventive problems were synthesized into a single list, whose preparation involved the identification of heuristics in literature, followed by a qualitative analysis through a comparison process, resulting in a final list of 263 heuristics. The authors hope that this list can save problem solvers' time and become a basis for a future, essential set of inventive problem solving heuristics.},
  file = {/home/xav/Zotero/storage/V7TH5PRD/Tessari et De Carvalho - 2015 - Rules for Problem Solving Qualitative Analysis an.pdf}
}

@misc{TextSummarizationApproaches2020,
  title = {Text {{Summarization Approaches}} for {{NLP}} - {{Practical Guide}} with {{Generative Examples}}},
  year = {2020},
  month = oct,
  journal = {Machine Learning Plus},
  urldate = {2022-05-13},
  abstract = {Text Summarization is summarizing the information in large texts for quicker consumption. In this article, I will walk you through the extractive as well as generative text summarization approaches.},
  langid = {american},
  file = {/home/xav/Zotero/storage/TE2BZZL8/text-summarization-approaches-nlp-example.html}
}

@misc{thaiCBRiKBCaseBasedReasoning2022,
  title = {{{CBR-iKB}}: {{A Case-Based Reasoning Approach}} for {{Question Answering}} over {{Incomplete Knowledge Bases}}},
  shorttitle = {{{CBR-iKB}}},
  author = {Thai, Dung and Ravishankar, Srinivas and Abdelaziz, Ibrahim and Chaudhary, Mudit and Mihindukulasooriya, Nandana and Naseem, Tahira and Das, Rajarshi and Kapanipathi, Pavan and Fokoue, Achille and McCallum, Andrew},
  year = {2022},
  month = apr,
  number = {arXiv:2204.08554},
  eprint = {arXiv:2204.08554},
  publisher = {{arXiv}},
  urldate = {2022-09-01},
  abstract = {Knowledge bases (KBs) are often incomplete and constantly changing in practice. Yet, in many question answering applications coupled with knowledge bases, the sparse nature of KBs is often overlooked. To this end, we propose a case-based reasoning approach, CBRiKB, for knowledge base question answering (KBQA) with incomplete-KB as our main focus. Our method ensembles decisions from multiple reasoning chains with a novel nonparametric reasoning algorithm. By design, CBR-iKB can seamlessly adapt to changes in KBs without any task-specific training or fine-tuning. Our method achieves 100\% accuracy on MetaQA and establishes new stateof-the-art on multiple benchmarks. For instance, CBR-iKB achieves an accuracy of 70\% on WebQSP under the incomplete-KB setting, outperforming the existing state-of-theart method by 22.3\%.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/9JEINKIQ/Thai et al. - 2022 - CBR-iKB A Case-Based Reasoning Approach for Quest.pdf}
}

@misc{thakurBEIRHeterogenousBenchmark2021,
  title = {{{BEIR}}: {{A Heterogenous Benchmark}} for {{Zero-shot Evaluation}} of {{Information Retrieval Models}}},
  shorttitle = {{{BEIR}}},
  author = {Thakur, Nandan and Reimers, Nils and R{\"u}ckl{\'e}, Andreas and Srivastava, Abhishek and Gurevych, Iryna},
  year = {2021},
  month = oct,
  number = {arXiv:2104.08663},
  eprint = {arXiv:2104.08663},
  publisher = {{arXiv}},
  urldate = {2023-01-23},
  abstract = {Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-theart retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction based models on average achieve the best zeroshot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/C7THMF77/Thakur et al. - 2021 - BEIR A Heterogenous Benchmark for Zero-shot Evalu.pdf}
}

@incollection{thrunLearningLearnIntroduction1998,
  title = {Learning to {{Learn}}: {{Introduction}} and {{Overview}}},
  shorttitle = {Learning to {{Learn}}},
  booktitle = {Learning to {{Learn}}},
  author = {Thrun, Sebastian and Pratt, Lorien},
  editor = {Thrun, Sebastian and Pratt, Lorien},
  year = {1998},
  pages = {3--17},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4615-5529-2_1},
  urldate = {2023-02-02},
  abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications (see e.g., [Langley, 1992; Widrow et al., 1994]).},
  isbn = {978-1-4615-5529-2},
  langid = {english},
  keywords = {Face Recognition,Learning Algorithm,Learning Task,Reinforcement Learning,Target Function}
}

@article{tiddiKnowledgeGraphsTools2022,
  title = {Knowledge Graphs as Tools for Explainable Machine Learning: {{A}} Survey},
  shorttitle = {Knowledge Graphs as Tools for Explainable Machine Learning},
  author = {Tiddi, Ilaria and Schlobach, Stefan},
  year = {2022},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {302},
  pages = {103627},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103627},
  urldate = {2022-05-11},
  abstract = {This paper provides an extensive overview of the use of knowledge graphs in the context of Explainable Machine Learning. As of late, explainable AI has become a very active field of research by addressing the limitations of the latest machine learning solutions that often provide highly accurate, but hardly scrutable and interpretable decisions. An increasing interest has also been shown in the integration of Knowledge Representation techniques in Machine Learning applications, mostly motivated by the complementary strengths and weaknesses that could lead to a new generation of hybrid intelligent systems. Following this idea, we hypothesise that knowledge graphs, which naturally provide domain background knowledge in a machine-readable format, could be integrated in Explainable Machine Learning approaches to help them provide more meaningful, insightful and trustworthy explanations. Using a systematic literature review methodology we designed an analytical framework to explore the current landscape of Explainable Machine Learning. We focus particularly on the integration with structured knowledge at large scale, and use our framework to analyse a variety of Machine Learning domains, identifying the main characteristics of such knowledge-based, explainable systems from different perspectives. We then summarise the strengths of such hybrid systems, such as improved understandability, reactivity, and accuracy, as well as their limitations, e.g. in handling noise or extracting knowledge efficiently. We conclude by discussing a list of open challenges left for future research.},
  langid = {english},
  keywords = {Explainable AI,Explainable systems,Explanations,Knowledge graphs,Neuro-symbolic integration,Subsymbolic AI,Symbolic AI},
  annotation = {QID: Q109302191},
  file = {/home/xav/Zotero/storage/2EB9LQVJ/Tiddi et Schlobach - 2022 - Knowledge graphs as tools for explainable machine .pdf;/home/xav/Zotero/storage/FJ8PX5BS/S0004370221001788.html}
}

@misc{townsendDoc2DictInformationExtraction2021,
  title = {{{Doc2Dict}}: {{Information Extraction}} as {{Text Generation}}},
  shorttitle = {{{Doc2Dict}}},
  author = {Townsend, Benjamin and {Ito-Fisher}, Eamon and Zhang, Lily and May, Madison},
  year = {2021},
  month = oct,
  number = {arXiv:2105.07510},
  eprint = {arXiv:2105.07510},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2105.07510},
  urldate = {2022-08-25},
  abstract = {Typically, information extraction (IE) requires a pipeline approach: first, a sequence labeling model is trained on manually annotated documents to extract relevant spans; then, when a new document arrives, a model predicts spans which are then post-processed and standardized to convert the information into a database entry. We replace this labor-intensive workflow with a transformer language model trained on existing database records to directly generate structured JSON. Our solution removes the workload associated with producing token-level annotations and takes advantage of a data source which is generally quite plentiful (e.g. database records). As long documents are common in information extraction tasks, we use gradient checkpointing and chunked encoding to apply our method to sequences of up to 32,000 tokens on a single GPU. Our Doc2Dict approach is competitive with more complex, hand-engineered pipelines and offers a simple but effective baseline for document-level information extraction. We release our Doc2Dict model and code to reproduce our experiments and facilitate future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/JX5DAJ9F/Townsend et al. - 2021 - Doc2Dict Information Extraction as Text Generatio.pdf}
}

@misc{trivediMuSiQueMultihopQuestions2022,
  title = {{{MuSiQue}}: {{Multihop Questions}} via {{Single-hop Question Composition}}},
  shorttitle = {{{MuSiQue}}},
  author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  year = {2022},
  month = may,
  number = {arXiv:2108.00573},
  eprint = {arXiv:2108.00573},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.00573},
  urldate = {2022-12-14},
  abstract = {Multihop reasoning remains an elusive goal as existing multihop benchmarks are known to be largely solvable via shortcuts. Can we create a question answering (QA) dataset that, by construction, \textbackslash emph\{requires\} proper multihop reasoning? To this end, we introduce a bottom-up approach that systematically selects composable pairs of single-hop questions that are connected, i.e., where one reasoning step critically relies on information from another. This bottom-up methodology lets us explore a vast space of questions and add stringent filters as well as other mechanisms targeting connected reasoning. It provides fine-grained control over the construction process and the properties of the resulting \$k\$-hop questions. We use this methodology to create MuSiQue-Ans, a new multihop QA dataset with 25K 2-4 hop questions. Relative to existing datasets, MuSiQue-Ans is more difficult overall (3x increase in human-machine gap), and harder to cheat via disconnected reasoning (e.g., a single-hop model has a 30 point drop in F1). We further add unanswerable contrast questions to produce a more stringent dataset, MuSiQue-Full. We hope our datasets will help the NLP community develop models that perform genuine multihop reasoning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/XPE6CEQI/Trivedi et al. - 2022 - MuSiQue Multihop Questions via Single-hop Questio.pdf;/home/xav/Zotero/storage/Z9DWK8CM/2108.html}
}

@misc{trivediTeachingBroadReasoning2022,
  title = {Teaching {{Broad Reasoning Skills}} for {{Multi-Step QA}} by {{Generating Hard Contexts}}},
  author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  year = {2022},
  month = nov,
  number = {arXiv:2205.12496},
  eprint = {arXiv:2205.12496},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.12496},
  urldate = {2022-12-18},
  abstract = {Question-answering datasets require a broad set of reasoning skills. We show how to use question decompositions to teach language models these broad reasoning skills in a robust fashion. Specifically, we use widely available QDMR representations to programmatically create hard-to-cheat synthetic contexts for real questions in six multi-step reasoning datasets. These contexts are carefully designed to avoid reasoning shortcuts prevalent in real contexts that prevent models from learning the right skills. This results in a pretraining dataset, named TeaBReaC, containing 525K multi-step questions (with associated formal programs) covering about 900 reasoning patterns. We show that pretraining standard language models (LMs) on TeaBReaC before fine-tuning them on target datasets improves their performance by up to 13 F1 points across 4 multi-step QA datasets, with up to 21 point gain on more complex questions. The resulting models also demonstrate higher robustness, with a 5-8 F1 point improvement on two contrast sets. Furthermore, TeaBReaC pretraining substantially improves model performance and robustness even when starting with numerate LMs pretrained using recent methods (e.g., PReasM, POET). Our work thus shows how to effectively use decomposition-guided contexts to robustly teach multi-step reasoning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/NR7M3I5V/Trivedi et al. - 2022 - Teaching Broad Reasoning Skills for Multi-Step QA .pdf;/home/xav/Zotero/storage/9Q33AELM/2205.html}
}

@article{TRIZMethodologieResolution2022,
  title = {{TRIZ Methodologie de r\'esolution innovente de probl\`eme}},
  year = {2022},
  month = apr,
  journal = {Wikip\'edia},
  urldate = {2022-05-12},
  abstract = {TRIZ (acronyme russe de la Th\'eorie de R\'esolution des Probl\`emes Inventifs, Teorija Reshenija Izobretateliskih Zadatch (\cyrchar\CYRT\cyrchar\cyre\cyrchar\cyro\cyrchar\cyrr\cyrchar\cyri\cyrchar\cyrya{} \cyrchar\CYRR\cyrchar\cyre\cyrchar\cyrsh\cyrchar\cyre\cyrchar\cyrn\cyrchar\cyri\cyrchar\cyrya{} \cyrchar\CYRI\cyrchar\cyrz\cyrchar\cyro\cyrchar\cyrb\cyrchar\cyrr\cyrchar\cyre\cyrchar\cyrt\cyrchar\cyra\cyrchar\cyrt\cyrchar\cyre\cyrchar\cyrl\cyrchar\cyrsftsn\cyrchar\cyrs\cyrchar\cyrk\cyrchar\cyri\cyrchar\cyrh{} \cyrchar\CYRZ\cyrchar\cyra\cyrchar\cyrd\cyrchar\cyra\cyrchar\cyrch{} - \cyrchar\CYRT\cyrchar\CYRR\cyrchar\CYRI\cyrchar\CYRZ )) est une approche heuristique destin\'ee \`a r\'esoudre des probl\`emes d'innovation, principalement techniques. Elle est \'elabor\'ee \`a partir de 1946 par l'ing\'enieur sovi\'etique Genrich Altshuller, lorsqu'il constata que le progr\`es technologique suit de fa\c{c}on g\'en\'erale un cours descriptible par des lois. Ces lois sugg\`erent une proc\'edure \`a suivre pour innover en mati\`ere de technologies, notamment en explorant des solutions g\'en\'eriques, emprunt\'ees \`a d'autres domaines, qui n'ont pas encore \'et\'e appliqu\'ees au probl\`eme particulier \`a l'\'etude.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 192806978},
  file = {/home/xav/Zotero/storage/52MXVNC3/TRIZ.html}
}

@misc{TRIZOrientedArtificial,
  title = {{{TRIZ}} Oriented Artificial Intelligence},
  urldate = {2022-05-15},
  howpublished = {https://research.iaun.ac.ir/pd/mehdiakbari/pdfs/PaperC\_9788.pdf},
  file = {/home/xav/Zotero/storage/M68LT45U/PaperC_9788.pdf}
}

@misc{TRIZRDFOntology,
  title = {{{TRIZ RDF Ontology RDFData}}/{{BusinessStandards}}.Ttl at Master {$\cdot$} Wumm-Project/{{RDFData}}},
  journal = {GitHub},
  urldate = {2022-05-11},
  abstract = {The RDFData collected within the Open Discovery Project - RDFData/BusinessStandards.ttl at master {$\cdot$} wumm-project/RDFData},
  howpublished = {https://github.com/wumm-project/RDFData},
  langid = {english},
  file = {/home/xav/Zotero/storage/3PXU323V/RDFData.html}
}

@article{truongSensitiveDataDetection2020,
  title = {Sensitive {{Data Detection}} with {{High-Throughput Neural Network Models}} for {{Financial Institutions}}},
  author = {Truong, Anh and Walters, Austin and Goodsitt, Jeremy},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.09597 [cs]},
  eprint = {2012.09597},
  primaryclass = {cs},
  urldate = {2022-05-17},
  abstract = {Named Entity Recognition has been extensively investigated in many fields. However, the application of sensitive entity detection for production systems in financial institutions has not been well explored due to the lack of publicly available, labeled datasets. In this paper, we use internal and synthetic datasets to evaluate various methods of detecting NPI (Nonpublic Personally Identifiable) information commonly found within financial institutions, in both unstructured and structured data formats. Character-level neural network models including CNN, LSTM, BiLSTM-CRF, and CNN-CRF are investigated on two prediction tasks: (i) entity detection on multiple data formats, and (ii) column-wise entity prediction on tabular datasets. We compare these models with other standard approaches on both real and synthetic data, with respect to F1-score, precision, recall, and throughput. The real datasets include internal structured data and public email data with manually tagged labels. Our experimental results show that the CNN model is simple yet effective with respect to accuracy and throughput and thus, is the most suitable candidate model to be deployed in the production environment(s). Finally, we provide several lessons learned on data limitations, data labelling and the intrinsic overlap of data entities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/VJEUIYA8/Truong et al. - 2020 - Sensitive Data Detection with High-Throughput Neur.pdf;/home/xav/Zotero/storage/SXIX854Y/2012.html}
}

@article{tsamardinosJustAddData2022,
  title = {Just {{Add Data}}: Automated Predictive Modeling for Knowledge Discovery and Feature Selection},
  shorttitle = {Just {{Add Data}}},
  author = {Tsamardinos, Ioannis and Charonyktakis, Paulos and Papoutsoglou, Georgios and Borboudakis, Giorgos and Lakiotaki, Kleanthi and Zenklusen, Jean Claude and Juhl, Hartmut and Chatzaki, Ekaterini and Lagani, Vincenzo},
  year = {2022},
  month = jun,
  journal = {npj Precision Oncology},
  volume = {6},
  number = {1},
  pages = {1--17},
  publisher = {{Nature Publishing Group}},
  issn = {2397-768X},
  doi = {10.1038/s41698-022-00274-8},
  urldate = {2022-11-22},
  abstract = {Fully automated machine learning (AutoML) for predictive modeling is becoming a reality, giving rise to a whole new field. We present the basic ideas and principles of Just Add Data Bio (JADBio), an AutoML platform applicable to the low-sample, high-dimensional omics data that arise in translational medicine and bioinformatics applications. In addition to predictive and diagnostic models ready for clinical use, JADBio focuses on knowledge discovery by performing feature selection and identifying the corresponding biosignatures, i.e., minimal-size subsets of biomarkers that are jointly predictive of the outcome or phenotype of interest. It also returns a palette of useful information for interpretation, clinical use of the models, and decision making. JADBio is qualitatively and quantitatively compared against Hyper-Parameter Optimization Machine Learning libraries. Results show that in typical omics dataset analysis, JADBio manages to identify signatures comprising of just a handful of features while maintaining competitive predictive performance and accurate out-of-sample performance estimation.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {High-throughput screening,Predictive markers},
  file = {/home/xav/Zotero/storage/KDCNZG6W/Tsamardinos et al. - 2022 - Just Add Data automated predictive modeling for k.pdf;/home/xav/Zotero/storage/HLBU62WW/s41698-022-00274-8.html}
}

@misc{tyagiTextSummarizationClustering2020,
  title = {Text {{Summarization}} for {{Clustering}} Documents},
  author = {Tyagi, Gaurika},
  year = {2020},
  month = aug,
  journal = {Medium},
  urldate = {2022-05-13},
  abstract = {This is a part 2 of the series analyzing healthcare chart notes using Natural Language Processing (NLP)},
  howpublished = {https://towardsdatascience.com/text-summarization-for-clustering-documents-2e074da6437a},
  langid = {english},
  file = {/home/xav/Zotero/storage/PICCTUQX/text-summarization-for-clustering-documents-2e074da6437a.html}
}

@article{uc-cetinaSurveyReinforcementLearning2023,
  title = {Survey on Reinforcement Learning for Language Processing},
  author = {{Uc-Cetina}, V{\'i}ctor and {Navarro-Guerrero}, Nicol{\'a}s and {Martin-Gonzalez}, Anabel and Weber, Cornelius and Wermter, Stefan},
  year = {2023},
  month = feb,
  journal = {Artificial Intelligence Review},
  volume = {56},
  number = {2},
  pages = {1543--1575},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10205-5},
  urldate = {2023-02-28},
  abstract = {In recent years some researchers have explored the use of reinforcement learning (RL) algorithms as key components in the solution of various natural language processing (NLP) tasks. For instance, some of these algorithms leveraging deep neural learning have found their way into conversational systems. This paper reviews the state of the art of RL methods for their possible use for different problems of NLP, focusing primarily on conversational systems, mainly due to their growing relevance. We provide detailed descriptions of the problems as well as discussions of why RL is well-suited to solve them. Also, we analyze the advantages and limitations of these methods. Finally, we elaborate on promising research directions in NLP that might benefit from RL.},
  langid = {english},
  keywords = {Conversational systems,Natural language processing,Parsing,Reinforcement learning,Text generation,Translation},
  file = {/home/xav/Zotero/storage/IJ44IXSA/Uc-Cetina et al. - 2023 - Survey on reinforcement learning for language proc.pdf}
}

@incollection{ulhassanHumanintheLoopTasksData2019,
  title = {Human-in-the-{{Loop Tasks}} for {{Data Management}}, {{Citizen Sensing}}, and {{Actuation}} in {{Smart Environments}}},
  author = {{ul Hassan}, Umair and Curry, Edward},
  year = {2019},
  month = nov,
  pages = {139--158},
  doi = {10.1007/978-3-030-29665-0_9},
  abstract = {Humans are playing critical roles in the management of data at large scales, through activities including schema building, matching data elements, resolving conflicts, and ranking results. The application of human-in-the-loop within intelligent systems in smart environments presents challenges in the areas of programming paradigms, execution methods, and task design. This chapter examines current human-in-the-loop approaches for data management tasks, including data integration, data collection (e.g. citizen sensing), and query refinement. A comparison of approaches (Augmented Algorithms, Declarative Programming, and Stand-alone Platforms) that can enable human tasks within data management is presented. The chapter also covers spatial tasks where users within the smart environment are requested to take physical actions in the environment in the form of citizen actuation.},
  isbn = {978-3-030-29664-3},
  file = {/home/xav/Zotero/storage/EDNQE6GP/ul Hassan et Curry - 2019 - Human-in-the-Loop Tasks for Data Management, Citiz.pdf}
}

@inproceedings{ullrichUsingBloomTaxonomy2021,
  title = {Using {{Bloom}}'s {{Taxonomy}} to {{Classify Question Complexity}}},
  booktitle = {Proceedings of the {{Fourth International Conference}} on {{Natural Language}} and {{Speech Processing}} ({{ICNLSP}} 2021)},
  author = {Ullrich, Sabine and Geierhos, Michaela},
  year = {2021},
  pages = {285--289},
  publisher = {{Association for Computational Linguistics}},
  address = {{Trento, Italy}},
  urldate = {2023-01-22},
  file = {/home/xav/Zotero/storage/RF7FCQTR/Ullrich et Geierhos - 2021 - Using Bloom's Taxonomy to Classify Question Comple.pdf}
}

@misc{upadhyaySharingLearnLearning2023,
  title = {Sharing to Learn and Learning to Share -- {{Fitting}} Together {{Meta-Learning}}, {{Multi-Task Learning}}, and {{Transfer Learning}}: {{A}} Meta Review},
  shorttitle = {Sharing to Learn and Learning to Share -- {{Fitting}} Together {{Meta-Learning}}, {{Multi-Task Learning}}, and {{Transfer Learning}}},
  author = {Upadhyay, Richa and Phlypo, Ronald and Saini, Rajkumar and Liwicki, Marcus},
  year = {2023},
  month = jan,
  number = {arXiv:2111.12146},
  eprint = {arXiv:2111.12146},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Integrating knowledge across different domains is an essential feature of human learning. Learning paradigms such as transfer learning, meta learning, and multi-task learning reflect the human learning process by exploiting the prior knowledge for new tasks, encouraging faster learning and good generalization for new tasks. This article gives a detailed view of these learning paradigms and their comparative analysis. The weakness of one learning algorithm turns out to be a strength of another, and thus merging them is a prevalent trait in the literature. There are numerous research papers that focus on each of these learning paradigms separately and provide a comprehensive overview of them. However, this article provides a review of research studies that combine (two of) these learning algorithms. This survey describes how these techniques are combined to solve problems in many different fields of study, including computer vision, natural language processing, hyper-spectral imaging, and many more, in supervised setting only. As a result, the global generic learning network \textendash{} an amalgamation of meta learning, transfer learning, and multi-task learning \textendash{} is introduced here, along with some open research questions and future research directions in the multi-task setting.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/6VKVVF2M/Upadhyay et al. - 2023 - Sharing to learn and learning to share -- Fitting .pdf}
}

@article{UPMLFrameworkKnowledge,
  title = {{{UPML}}: {{A Framework}} for {{Knowledge System Reuse}}}
}

@phdthesis{valverdeMethodologieAideInnovation2015,
  type = {These de Doctorat},
  title = {M\'ethodologie d'aide \`a l'innovation Par l'exploitation Des Brevets et Des Ph\'enom\`enes Physiques Impliqu\'es},
  author = {Valverde, Ulises},
  year = {2015},
  month = dec,
  urldate = {2022-05-11},
  abstract = {L'objectif de ce travail de th\`ese est de d\'evelopper une m\'ethodologie d'extraction de connaissances \`a partir de brevets pour aider les concepteurs dans la phase de r\'esolution de probl\`emes industriels. La m\'ethodologie est fond\'ee sur trois piliers : la d\'efinition, la recherche / analyse et l'innovation. La d\'efinition exhaustive de la fonction principale du syst\`eme industriel cible le champ de recherche et permet la r\'ecup\'eration de mots cl\'es initiaux gr\^ace \`a une analyse approfondie de l'existant. La recherche it\'erative des brevets se base sur la d\'ecomposition fonctionnelle et sur l'analyse physique. L'analyse int\`egre la d\'ecomposition fonctionnelle \'energ\'etique pour d\'eceler les \'energies, les flux fonctionnels transmis et les ph\'enom\`enes physiques impliqu\'es dans le processus de conversion \'energ\'etique afin de s\'electionner des effets physiques potentiellement pertinents. Pour d\'elimiter le champ d'exploration nous formulons des requ\^etes de recherche \`a partir d'une base de donn\'ees de mots cl\'es constitu\'ee par des mots cl\'es initiaux, des mots cl\'es physiques et des mots cl\'es technologiques. Une matrice des d\'ecouvertes bas\'ee sur les croisements entre ces mots cl\'es permet le classement des brevets pertinents. La recherche des opportunit\'es d'innovation exploite la matrice des d\'ecouvertes pour d\'eceler les tendances \'evolutives suivies par les inventions. Les opportunit\'es sont d\'eduites \`a partir de l'analyse des cellules non pourvues de la matrice des d\'ecouvertes, de l'analyse par tendances d'\'evolution et du changement de concept par la substitution du convertisseur \'energ\'etique. Nous proposons des tendances d'\'evolution construites \`a partir de lois d'\'evolution de la th\'eorie TRIZ, d'heuristiques de conception et de r\`egles de l'art de l'ing\'enieur. Un cas d'application concernant l'\'etude d'\'evolution et la proposition de nouveaux syst\`emes de s\'eparation de m\'elanges bi-phasiques en offshore profond met en valeur la m\'ethode.},
  collaborator = {Nadeau, Jean-Pierre and Nadeau, Jean-Pierre},
  copyright = {Licence Etalab},
  school = {Paris, ENSAM},
  keywords = {Aide √† l'innovation,Conception inventive,Cr√©ativit√©,Cr√©ativit√© en technologie,Creativity,D√©composition fonctionnelle,Design,Evolution trends,Exploitation des brevets,Functional decomposition,Innovation aid,Innovations technologiques,Inventive design,Patent exploitation,Tendances d'√©volution},
  file = {/home/xav/Zotero/storage/W5W6YN27/Valverde - 2015 - M√©thodologie d‚Äôaide √† l‚Äôinnovation par l‚Äôexploitat.pdf}
}

@misc{vanbekkumModularDesignPatterns2021,
  title = {Modular {{Design Patterns}} for {{Hybrid Learning}} and {{Reasoning Systems}}: A Taxonomy, Patterns and Use Cases},
  shorttitle = {Modular {{Design Patterns}} for {{Hybrid Learning}} and {{Reasoning Systems}}},
  author = {{van Bekkum}, Michael and {de Boer}, Maaike and {van Harmelen}, Frank and {Meyer-Vitali}, Andr{\'e} and ten Teije, Annette},
  year = {2021},
  month = mar,
  number = {arXiv:2102.11965},
  eprint = {arXiv:2102.11965},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.11965},
  urldate = {2022-10-24},
  abstract = {The unification of statistical (data-driven) and symbolic (knowledge-driven) methods is widely recognised as one of the key challenges of modern AI. Recent years have seen large number of publications on such hybrid neuro-symbolic AI systems. That rapidly growing literature is highly diverse and mostly empirical, and is lacking a unifying view of the large variety of these hybrid systems. In this paper we analyse a large body of recent literature and we propose a set of modular design patterns for such hybrid, neuro-symbolic systems. We are able to describe the architecture of a very large number of hybrid systems by composing only a small set of elementary patterns as building blocks. The main contributions of this paper are: 1) a taxonomically organised vocabulary to describe both processes and data structures used in hybrid systems; 2) a set of 15+ design patterns for hybrid AI systems, organised in a set of elementary patterns and a set of compositional patterns; 3) an application of these design patterns in two realistic use-cases for hybrid AI systems. Our patterns reveal similarities between systems that were not recognised until now. Finally, our design patterns extend and refine Kautz' earlier attempt at categorising neuro-symbolic architectures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/URRPDSRL/van Bekkum et al. - 2021 - Modular Design Patterns for Hybrid Learning and Re.pdf}
}

@misc{varmaSocraticLearningAugmenting2017,
  title = {Socratic {{Learning}}: {{Augmenting Generative Models}} to {{Incorporate Latent Subsets}} in {{Training Data}}},
  shorttitle = {Socratic {{Learning}}},
  author = {Varma, Paroma and He, Bryan and Iter, Dan and Xu, Peng and Yu, Rose and De Sa, Christopher and R{\'e}, Christopher},
  year = {2017},
  month = sep,
  number = {arXiv:1610.08123},
  eprint = {arXiv:1610.08123},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1610.08123},
  urldate = {2023-02-07},
  abstract = {A challenge in training discriminative models like neural networks is obtaining enough labeled training data. Recent approaches use generative models to combine weak supervision sources, like user-defined heuristics or knowledge bases, to label training data. Prior work has explored learning accuracies for these sources even without ground truth labels, but they assume that a single accuracy parameter is sufficient to model the behavior of these sources over the entire training set. In particular, they fail to model latent subsets in the training data in which the supervision sources perform differently than on average. We present Socratic learning, a paradigm that uses feedback from a corresponding discriminative model to automatically identify these subsets and augments the structure of the generative model accordingly. Experimentally, we show that without any ground truth labels, the augmented generative model reduces error by up to 56.06\% for a relation extraction task compared to a state-of-the-art weak supervision technique that utilizes generative models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/9VFILAQG/Varma et al. - 2017 - Socratic Learning Augmenting Generative Models to.pdf;/home/xav/Zotero/storage/YS45QQYV/1610.html}
}

@misc{varshneyCanOpenDomainQA2022,
  title = {Can {{Open-Domain QA Reader Utilize External Knowledge Efficiently}} like {{Humans}}?},
  author = {Varshney, Neeraj and Luo, Man and Baral, Chitta},
  year = {2022},
  month = nov,
  number = {arXiv:2211.12707},
  eprint = {arXiv:2211.12707},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.12707},
  urldate = {2023-03-21},
  abstract = {Recent state-of-the-art open-domain QA models are typically based on a two stage retriever-reader approach in which the retriever first finds the relevant knowledge/passages and the reader then leverages that to predict the answer. Prior work has shown that the performance of the reader usually tends to improve with the increase in the number of these passages. Thus, state-of-the-art models use a large number of passages (e.g. 100) for inference. While the reader in this approach achieves high prediction performance, its inference is computationally very expensive. We humans, on the other hand, use a more efficient strategy while answering: firstly, if we can confidently answer the question using our already acquired knowledge then we do not even use the external knowledge, and in the case when we do require external knowledge, we don't read the entire knowledge at once, instead, we only read that much knowledge that is sufficient to find the answer. Motivated by this procedure, we ask a research question "Can the open-domain QA reader utilize external knowledge efficiently like humans without sacrificing the prediction performance?" Driven by this question, we explore an approach that utilizes both 'closed-book' (leveraging knowledge already present in the model parameters) and 'open-book' inference (leveraging external knowledge). Furthermore, instead of using a large fixed number of passages for open-book inference, we dynamically read the external knowledge in multiple 'knowledge iterations'. Through comprehensive experiments on NQ and TriviaQA datasets, we demonstrate that this dynamic reading approach improves both the 'inference efficiency' and the 'prediction accuracy' of the reader. Comparing with the FiD reader, this approach matches its accuracy by utilizing just 18.32\% of its reader inference cost and also outperforms it by achieving up to 55.10\% accuracy on NQ Open.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/6P5DMYLF/Varshney et al. - 2022 - Can Open-Domain QA Reader Utilize External Knowled.pdf;/home/xav/Zotero/storage/H46YHWZP/2211.html}
}

@misc{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  number = {arXiv:1706.03762},
  eprint = {arXiv:1706.03762},
  publisher = {{arXiv}},
  urldate = {2022-07-19},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/NSCDNWIS/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@misc{voorheesTRECCOVIDConstructingPandemic2020,
  title = {{{TREC-COVID}}: {{Constructing}} a {{Pandemic Information Retrieval Test Collection}}},
  shorttitle = {{{TREC-COVID}}},
  author = {Voorhees, Ellen and Alam, Tasmeer and Bedrick, Steven and {Demner-Fushman}, Dina and Hersh, William R. and Lo, Kyle and Roberts, Kirk and Soboroff, Ian and Wang, Lucy Lu},
  year = {2020},
  month = may,
  number = {arXiv:2005.04474},
  eprint = {arXiv:2005.04474},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.04474},
  urldate = {2023-01-23},
  abstract = {TREC-COVID is a community evaluation designed to build a test collection that captures the information needs of biomedical researchers using the scientific literature during a pandemic. One of the key characteristics of pandemic search is the accelerated rate of change: the topics of interest evolve as the pandemic progresses and the scientific literature in the area explodes. The COVID-19 pandemic provides an opportunity to capture this progression as it happens. TREC-COVID, in creating a test collection around COVID-19 literature, is building infrastructure to support new research and technologies in pandemic search.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval,H.3.0},
  file = {/home/xav/Zotero/storage/XWV4KVM6/Voorhees et al. - 2020 - TREC-COVID Constructing a Pandemic Information Re.pdf;/home/xav/Zotero/storage/4LUIZF4L/2005.html}
}

@misc{wahleCohesiveDistillationArchitecture2023,
  title = {A {{Cohesive Distillation Architecture}} for {{Neural Language Models}}},
  author = {Wahle, Jan Philip},
  year = {2023},
  month = jan,
  number = {arXiv:2301.08130},
  eprint = {arXiv:2301.08130},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.08130},
  urldate = {2023-01-23},
  abstract = {A recent trend in Natural Language Processing is the exponential growth in Language Model (LM) size, which prevents research groups without a necessary hardware infrastructure from participating in the development process. This study investigates methods for Knowledge Distillation (KD) to provide efficient alternatives to large-scale models. In this context, KD means extracting information about language encoded in a Neural Network and Lexical Knowledge Databases. We developed two methods to test our hypothesis that efficient architectures can gain knowledge from LMs and extract valuable information from lexical sources. First, we present a technique to learn confident probability distribution for Masked Language Modeling by prediction weighting of multiple teacher networks. Second, we propose a method for Word Sense Disambiguation (WSD) and lexical KD that is general enough to be adapted to many LMs. Our results show that KD with multiple teachers leads to improved training convergence. When using our lexical pre-training method, LM characteristics are not lost, leading to increased performance in Natural Language Understanding (NLU) tasks over the state-of-the-art while adding no parameters. Moreover, the improved semantic understanding of our model increased the task performance beyond WSD and NLU in a real-problem scenario (Plagiarism Detection). This study suggests that sophisticated training methods and network architectures can be superior over scaling trainable parameters. On this basis, we suggest the research area should encourage the development and use of efficient models and rate impacts resulting from growing LM size equally against task performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/H3AZMG9W/Wahle - 2023 - A Cohesive Distillation Architecture for Neural La.pdf;/home/xav/Zotero/storage/UABSDKD7/2301.html}
}

@misc{wangArchivalQALargescaleBenchmark2022,
  title = {{{ArchivalQA}}: {{A Large-scale Benchmark Dataset}} for {{Open Domain Question Answering}} over {{Historical News Collections}}},
  shorttitle = {{{ArchivalQA}}},
  author = {Wang, Jiexin and Jatowt, Adam and Yoshikawa, Masatoshi},
  year = {2022},
  month = feb,
  number = {arXiv:2109.03438},
  eprint = {arXiv:2109.03438},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.03438},
  urldate = {2023-02-01},
  abstract = {In the last few years, open-domain question answering (ODQA) has advanced rapidly due to the development of deep learning techniques and the availability of large-scale QA datasets. However, the current datasets are essentially designed for synchronic document collections (e.g., Wikipedia). Temporal news collections such as long-term news archives spanning several decades, are rarely used in training the models despite they are quite valuable for our society. To foster the research in the field of ODQA on such historical collections, we present ArchivalQA, a large question answering dataset consisting of 532,444 question-answer pairs which is designed for temporal news QA. We divide our dataset into four subparts based on the question difficulty levels and the containment of temporal expressions, which we believe are useful for training and testing ODQA systems characterized by different strengths and abilities. The novel QA dataset-constructing framework that we introduce can be also applied to generate non-ambiguous questions of good quality over other types of temporal document collections.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/6NR3PXX4/Wang et al. - 2022 - ArchivalQA A Large-scale Benchmark Dataset for Op.pdf;/home/xav/Zotero/storage/LUHKJEIL/2109.html}
}

@misc{wangAskingAnsweringQuestions2020,
  title = {Asking and {{Answering Questions}} to {{Evaluate}} the {{Factual Consistency}} of {{Summaries}}},
  author = {Wang, Alex and Cho, Kyunghyun and Lewis, Mike},
  year = {2020},
  month = apr,
  number = {arXiv:2004.04228},
  eprint = {arXiv:2004.04228},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.04228},
  urldate = {2023-01-24},
  abstract = {Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose an automatic evaluation protocol called QAGS (pronounced "kags") that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/NZ5JCIK4/Wang et al. - 2020 - Asking and Answering Questions to Evaluate the Fac.pdf;/home/xav/Zotero/storage/MMSXTSCH/2004.html}
}

@article{wangBenchmarkingGeneralizationInContext2022,
  title = {Benchmarking {{Generalization}} via {{In-Context Instructions}} on 1,600+ {{Language Tasks}}},
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.07705 [cs]},
  eprint = {2204.07705},
  primaryclass = {cs},
  urldate = {2022-05-06},
  abstract = {How can we measure the generalization of models to a variety of unseen tasks when provided with their language instructions? To facilitate progress in this goal, we introduce Natural-Instructions v2, a benchmark of 1,600+ diverse language tasks and their expert-written instructions. It covers 70+ distinct task types, such as tagging, in-filling, and rewriting. These tasks are collected with contributions of NLP practitioners in the community and through an iterative peer review process to ensure their quality. With this large and diverse collection of tasks, we are able to rigorously benchmark cross-task generalization of models -- training on a subset of tasks and evaluating on the remaining unseen ones. For instance, we quantify generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances, and model sizes. Based on these insights, we introduce Tk-Instruct, an encoder-decoder Transformer that is trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) which outperforms existing larger models on our benchmark. We hope this benchmark facilitates future progress toward more general-purpose language understanding models.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/K8RNIF85/Wang et al. - 2022 - Benchmarking Generalization via In-Context Instruc.pdf}
}

@misc{wangComprehensiveSurveyContinual2023,
  title = {A {{Comprehensive Survey}} of {{Continual Learning}}: {{Theory}}, {{Method}} and {{Application}}},
  shorttitle = {A {{Comprehensive Survey}} of {{Continual Learning}}},
  author = {Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  year = {2023},
  month = jan,
  number = {arXiv:2302.00487},
  eprint = {arXiv:2302.00487},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.00487},
  urldate = {2023-03-24},
  abstract = {To cope with real-world dynamics, an intelligent agent needs to incrementally acquire, update, accumulate, and exploit knowledge throughout its lifetime. This ability, known as continual learning, provides a foundation for AI systems to develop themselves adaptively. In a general sense, continual learning is explicitly limited by catastrophic forgetting, where learning a new task usually results in a dramatic performance drop of the old tasks. Beyond this, increasingly numerous advances have emerged in recent years that largely extend the understanding and application of continual learning. The growing and widespread interest in this direction demonstrates its realistic significance as well as complexity. In this work, we present a comprehensive survey of continual learning, seeking to bridge the basic settings, theoretical foundations, representative methods, and practical applications. Based on existing theoretical and empirical results, we summarize the general objectives of continual learning as ensuring a proper stability-plasticity trade-off and an adequate intra/inter-task generalizability in the context of resource efficiency. Then we provide a state-of-the-art and elaborated taxonomy, extensively analyzing how representative strategies address continual learning, and how they are adapted to particular challenges in various applications. Through an in-depth discussion of continual learning in terms of the current trends, cross-directional prospects and interdisciplinary connections with neuroscience, we believe that such a holistic perspective can greatly facilitate subsequent exploration in this field and beyond.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ARUAI2Z5/Wang et al. - 2023 - A Comprehensive Survey of Continual Learning Theo.pdf;/home/xav/Zotero/storage/SU2JEWGT/2302.html}
}

@inproceedings{wangFormalModelSemantic2008,
  title = {A {{Formal Model}} of {{Semantic Web Service Ontology}} ({{WSMO}}) {{Execution}}},
  booktitle = {13th {{IEEE International Conference}} on {{Engineering}} of {{Complex Computer Systems}} (Iceccs 2008)},
  author = {Wang, Hai H. and Gibbins, Nick and Payne, Terry and Saleh, Ahmed and Sun, Jun},
  year = {2008},
  month = mar,
  pages = {111--120},
  doi = {10.1109/ICECCS.2008.25},
  abstract = {Semantic Web services have been one of the most significant research areas within the semantic Web vision, and have been recognized as a promising technology that exhibits huge commercial potential. Current semantic Web service research focuses on defining models and languages for the semantic markup of all relevant aspects of services, which are accessible through a Web service interface. The Web service modelling ontology (WSMO) is one of the most significant semantic Web service framework proposed to date. To support the standardization and tool support of WSMO, a formal semantics of the language is highly desirable. As there are a few variants of WSMO and it is still under development, the semantics of WSMO needs to be formally defined to facilitate easy reuse and future development. In this paper, we present a formal object-Z semantics of WSMO. Different aspects of the language have been precisely defined within one unified framework. This model provides a formal unambiguous specification, which can be used to develop tools and facilitate future development.},
  keywords = {Application software,Automation,Computer vision,Mediation,Natural languages,Ontologies,OWL,Semantic Web,Sun,Web services},
  file = {/home/xav/Zotero/storage/GBD9UDSP/A formal model of semantic Web Service Ontology (WSMO) execution.pdf}
}

@article{wangFormalModelSemantic2012,
  title = {A Formal Model of the {{Semantic Web Service Ontology}} ({{WSMO}})},
  author = {Wang, Hai H. and Gibbins, Nick and Payne, Terry R. and Redavid, Domenico},
  year = {2012},
  month = mar,
  journal = {Information Systems},
  volume = {37},
  number = {1},
  pages = {33--60},
  issn = {0306-4379},
  doi = {10.1016/j.is.2011.07.003},
  urldate = {2022-11-07},
  abstract = {Semantic Web Service, one of the most significant research areas within the Semantic Web vision, has attracted increasing attention from both the research community and industry. The Web Service Modelling Ontology (WSMO) has been proposed as an enabling framework for the total/partial automation of the tasks (e.g., discovery, selection, composition, mediation, execution, monitoring, etc.) involved in both intra- and inter-enterprise integration of Web services. To support the standardisation and tool support of WSMO, a formal model of the language is highly desirable. As several variants of WSMO have been proposed by the WSMO community, which are still under development, the syntax and semantics of WSMO should be formally defined to facilitate easy reuse and future development. In this paper, we present a formal Object-Z formal model of WSMO, where different aspects of the language have been precisely defined within one unified framework. This model not only provides a formal unambiguous model which can be used to develop tools and facilitate future development, but as demonstrated in this paper, can be used to identify and eliminate errors present in existing documentation.},
  keywords = {Object-Z,Semantics Web Service,WSMO},
  file = {/home/xav/Zotero/storage/LIPKS33H/Wang et al. - 2012 - A formal model of the Semantic Web Service Ontolog.pdf}
}

@misc{wangGPLGenerativePseudo2022,
  title = {{{GPL}}: {{Generative Pseudo Labeling}} for {{Unsupervised Domain Adaptation}} of {{Dense Retrieval}}},
  shorttitle = {{{GPL}}},
  author = {Wang, Kexin and Thakur, Nandan and Reimers, Nils and Gurevych, Iryna},
  year = {2022},
  month = apr,
  number = {arXiv:2112.07577},
  eprint = {arXiv:2112.07577},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.07577},
  urldate = {2023-01-23},
  abstract = {Dense retrieval approaches can overcome the lexical gap and lead to significantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method Generative Pseudo Labeling (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we find the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. The code and the models are available at https://github.com/UKPLab/gpl.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/R8ZXTPRQ/Wang et al. - 2022 - GPL Generative Pseudo Labeling for Unsupervised D.pdf;/home/xav/Zotero/storage/2WUPF6H4/2112.html}
}

@article{wangHumanintheloopMachineLearning,
  title = {Human-in-the-Loop {{Machine Learning}}: {{A Macro-Micro Perspective}}},
  author = {Wang, Jiangtao and Guo, Bin and Chen, Luke},
  pages = {11},
  abstract = {Though technical advance of artificial intelligence and machine learning has enabled many promising intelligent systems, many computing tasks are still not able to be fully accomplished by machine intelligence. Motivated by the complementary nature of human and machine intelligence, an emerging trend is to involve humans in the loop of machine learning and decision-making. In this paper, we provide a macro-micro review of human-inthe-loop machine learning. We first describe major machine learning challenges which can be addressed by human intervention in the loop. Then we examine closely the latest research and findings of introducing humans into each step of the lifecycle of machine learning. Finally, we analyze current research gaps and point out future research directions.},
  langid = {english},
  file = {/home/xav/Zotero/storage/N5Q3CI2Y/Wang et al. - Human-in-the-loop Machine Learning A Macro-Micro .pdf}
}

@misc{wangIterativelyPromptPretrained2022,
  title = {Iteratively {{Prompt Pre-trained Language Models}} for {{Chain}} of {{Thought}}},
  author = {Wang, Boshi and Deng, Xiang and Sun, Huan},
  year = {2022},
  month = oct,
  number = {arXiv:2203.08383},
  eprint = {arXiv:2203.08383},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.08383},
  urldate = {2023-02-07},
  abstract = {While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex \& multi-step reasoning. Similar to how humans develop a "chain of thought" for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step's contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/FYUIPDHQ/Wang et al. - 2022 - Iteratively Prompt Pre-trained Language Models for.pdf;/home/xav/Zotero/storage/SBIPWSC7/2203.html}
}

@article{wangKnowledgeGraphQuality2021,
  title = {Knowledge Graph Quality Control: {{A}} Survey},
  shorttitle = {Knowledge Graph Quality Control},
  author = {Wang, Xiangyu and Chen, Lyuzhou and Ban, Taiyu and Usman, Muhammad and Guan, Yifeng and Liu, Shikang and Wu, Tianhao and Chen, Huanhuan},
  year = {2021},
  month = sep,
  journal = {Fundamental Research},
  volume = {1},
  number = {5},
  pages = {607--626},
  issn = {2667-3258},
  doi = {10.1016/j.fmre.2021.09.003},
  urldate = {2022-05-12},
  abstract = {A knowledge graph (KG), a special form of semantic network, integrates fragmentary data into a graph to support knowledge processing and reasoning. KG quality control is important to the utility of KGs. It is essential to investigate KG quality and the parameters influencing KG quality to better understand its quality control. Although many works have been conducted to evaluate the dimensions of KG quality, quality control of the construction process, and enhancement methods for quality, a comprehensive literature review has not been presented on this topic. This paper intends to fill this research gap by presenting a comprehensive survey on the quality control of KGs. First, this paper defines six main evaluation dimensions of KG quality and investigates their correlations and differences. Second, quality control treatments during KG construction are introduced from the perspective of these dimensions of KG quality. Third, the quality enhancement of a constructed KG is described from various dimensions. This paper ultimately aims to promote the research and applications of KGs.},
  langid = {english},
  keywords = {Knowledge graph,Quality control,Quality enhancement,Quality evaluation},
  file = {/home/xav/Zotero/storage/9BP2A2R6/Wang et al. - 2021 - Knowledge graph quality control A survey.pdf;/home/xav/Zotero/storage/CMCQ7L8C/S2667325821001655.html}
}

@misc{wangLargeLanguageModels2023,
  title = {Large {{Language Models Are Implicitly Topic Models}}: {{Explaining}} and {{Finding Good Demonstrations}} for {{In-Context Learning}}},
  shorttitle = {Large {{Language Models Are Implicitly Topic Models}}},
  author = {Wang, Xinyi and Zhu, Wanrong and Wang, William Yang},
  year = {2023},
  month = jan,
  number = {arXiv:2301.11916},
  eprint = {arXiv:2301.11916},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.11916},
  urldate = {2023-01-30},
  abstract = {In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5\% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that large language models implicitly infer a latent concept variable.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/LY5XZSND/Wang et al. - 2023 - Large Language Models Are Implicitly Topic Models.pdf}
}

@article{wangLiLTSimpleEffective2022,
  title = {{{LiLT}}: {{A Simple}} yet {{Effective Language-Independent Layout Transformer}} for {{Structured Document Understanding}}},
  shorttitle = {{{LiLT}}},
  author = {Wang, Jiapeng and Jin, Lianwen and Ding, Kai},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.13669 [cs]},
  eprint = {2202.13669},
  primaryclass = {cs},
  urldate = {2022-05-17},
  abstract = {Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/JT9A4JIK/Wang et al. - 2022 - LiLT A Simple yet Effective Language-Independent .pdf}
}

@article{wangNeuralNetworksAre2020,
  title = {Neural {{Networks Are More Productive Teachers Than Human Raters}}: {{Active Mixup}} for {{Data-Efficient Knowledge Distillation}} from a {{Blackbox Model}}},
  shorttitle = {Neural {{Networks Are More Productive Teachers Than Human Raters}}},
  author = {Wang, Dongdong and Li, Yandong and Wang, Liqiang and Gong, Boqing},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.13960 [cs]},
  eprint = {2003.13960},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {We study how to train a student deep neural network for visual recognition by distilling knowledge from a blackbox teacher model in a data-efficient manner. Progress on this problem can significantly reduce the dependence on large-scale datasets for learning high-performing visual recognition models. There are two major challenges. One is that the number of queries into the teacher model should be minimized to save computational and/or financial costs. The other is that the number of images used for the knowledge distillation should be small; otherwise, it violates our expectation of reducing the dependence on large-scale datasets. To tackle these challenges, we propose an approach that blends mixup and active learning. The former effectively augments the few unlabeled images by a big pool of synthetic images sampled from the convex hull of the original images, and the latter actively chooses from the pool hard examples for the student neural network and query their labels from the teacher model. We validate our approach with extensive experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/W63YJSWP/Wang et al. - 2020 - Neural Networks Are More Productive Teachers Than .pdf;/home/xav/Zotero/storage/Q69VLCXA/2003.html}
}

@misc{wangPINTOFaithfulLanguage2022,
  title = {{{PINTO}}: {{Faithful Language Reasoning Using Prompt-Generated Rationales}}},
  shorttitle = {{{PINTO}}},
  author = {Wang, Peifeng and Chan, Aaron and Ilievski, Filip and Chen, Muhao and Ren, Xiang},
  year = {2022},
  month = nov,
  number = {arXiv:2211.01562},
  eprint = {arXiv:2211.01562},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.01562},
  urldate = {2023-02-07},
  abstract = {Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM's internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO's reasoning LM is fine-tuned to solve the task using the generated rationale as context, while regularized to output less confident predictions when the rationale is perturbed. Across four datasets, we show that PINTO significantly improves the generalization ability of the reasoning LM, yielding higher performance on both in-distribution and out-of-distribution test sets. Also, we find that PINTO's rationales are more faithful to its task predictions than those generated by competitive baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/324BIEH8/Wang et al. - 2022 - PINTO Faithful Language Reasoning Using Prompt-Ge.pdf;/home/xav/Zotero/storage/G38FFWUY/2211.html}
}

@article{wangPuttingHumansNatural2021,
  title = {Putting {{Humans}} in the {{Natural Language Processing Loop}}: {{A Survey}}},
  shorttitle = {Putting {{Humans}} in the {{Natural Language Processing Loop}}},
  author = {Wang, Zijie J. and Choi, Dongjin and Xu, Shenyu and Yang, Diyi},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.04044 [cs]},
  eprint = {2103.04044},
  primaryclass = {cs},
  urldate = {2022-03-21},
  abstract = {How can we design Natural Language Processing (NLP) systems that learn from human feedback? There is a growing research body of Human-in-the-loop (HITL) NLP frameworks that continuously integrate human feedback to improve the model itself. HITL NLP research is nascent but multifarious -- solving various NLP problems, collecting diverse feedback from different people, and applying different methods to learn from collected feedback. We present a survey of HITL NLP work from both Machine Learning (ML) and Human-Computer Interaction (HCI) communities that highlights its short yet inspiring history, and thoroughly summarize recent frameworks focusing on their tasks, goals, human interactions, and feedback learning methods. Finally, we discuss future directions for integrating human feedback in the NLP development loop.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/8NAKK8ID/Wang et al. - 2021 - Putting Humans in the Natural Language Processing .pdf;/home/xav/Zotero/storage/SHS7AEVC/2103.html}
}

@misc{wangSelfConsistencyImprovesChain2022,
  title = {Self-{{Consistency Improves Chain}} of {{Thought Reasoning}} in {{Language Models}}},
  author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  year = {2022},
  month = apr,
  number = {arXiv:2203.11171},
  eprint = {2203.11171},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-25},
  abstract = {We explore a simple ensemble strategy, self-consistency, that significantly improves the reasoning accuracy of large language models. The idea is to sample a diverse set of reasoning paths from a language model via chain of thought prompting then return the most consistent final answer in the set. We evaluate self-consistency on a range of arithmetic and commonsense reasoning benchmarks, and find that it robustly improves accuracy across a variety of language models and model scales without the need for additional training or auxiliary models. When combined with a recent large language model, PaLM-540B, self-consistency increases performance to state-of-the-art levels across several benchmark reasoning tasks, including GSM8K (56.5\% -{$>$} 74.4\%), SVAMP (79.0\% -{$>$} 86.6\%), AQuA (35.8\% -{$>$} 48.3\%), StrategyQA (75.3\% -{$>$} 81.6\%) and ARC-challenge (85.2\% -{$>$} 88.7\%).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/2KY3CDF3/Wang et al. - 2022 - Self-Consistency Improves Chain of Thought Reasoni.pdf}
}

@misc{wangSelfInstructAligningLanguage2022,
  title = {Self-{{Instruct}}: {{Aligning Language Model}} with {{Self Generated Instructions}}},
  shorttitle = {Self-{{Instruct}}},
  author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A. and Khashabi, Daniel and Hajishirzi, Hannaneh},
  year = {2022},
  month = dec,
  number = {arXiv:2212.10560},
  eprint = {arXiv:2212.10560},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.10560},
  urldate = {2023-01-03},
  abstract = {Large "instruction-tuned" language models (finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off its own generations. Our pipeline generates instruction, input, and output samples from a language model, then prunes them before using them to finetune the original model. Applying our method to vanilla GPT3, we demonstrate a 33\% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT\_001, which is trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5\% absolute gap behind InstructGPT\_001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/ASS7WBCU/Wang et al. - 2022 - Self-Instruct Aligning Language Model with Self G.pdf;/home/xav/Zotero/storage/NXWYEW2M/2212.html}
}

@misc{wangSuperNaturalInstructionsGeneralizationDeclarative2022,
  title = {Super-{{NaturalInstructions}}: {{Generalization}} via {{Declarative Instructions}} on 1600+ {{NLP Tasks}}},
  shorttitle = {Super-{{NaturalInstructions}}},
  author = {Wang, Yizhong and Mishra, Swaroop and Alipoormolabashi, Pegah and Kordi, Yeganeh and Mirzaei, Amirreza and Arunkumar, Anjana and Ashok, Arjun and Dhanasekaran, Arut Selvan and Naik, Atharva and Stap, David and Pathak, Eshaan and Karamanolakis, Giannis and Lai, Haizhi Gary and Purohit, Ishan and Mondal, Ishani and Anderson, Jacob and Kuznia, Kirby and Doshi, Krima and Patel, Maitreya and Pal, Kuntal Kumar and Moradshahi, Mehrad and Parmar, Mihir and Purohit, Mirali and Varshney, Neeraj and Kaza, Phani Rohitha and Verma, Pulkit and Puri, Ravsehaj Singh and Karia, Rushang and Sampat, Shailaja Keyur and Doshi, Savan and Mishra, Siddhartha and Reddy, Sujan and Patro, Sumanta and Dixit, Tanay and Shen, Xudong and Baral, Chitta and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh and Khashabi, Daniel},
  year = {2022},
  month = oct,
  number = {arXiv:2204.07705},
  eprint = {arXiv:2204.07705},
  publisher = {{arXiv}},
  urldate = {2022-12-07},
  abstract = {How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce SUPER-NATURALINSTRUCTIONS,1 a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions\textemdash training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/SQG3U3BJ/Wang et al. - 2022 - Super-NaturalInstructions Generalization via Decl.pdf}
}

@misc{wangSurveyTableandTextHybridQA2022,
  title = {A {{Survey}} on {{Table-and-Text HybridQA}}: {{Concepts}}, {{Methods}}, {{Challenges}} and {{Future Directions}}},
  shorttitle = {A {{Survey}} on {{Table-and-Text HybridQA}}},
  author = {Wang, Dingzirui and Dou, Longxu and Che, Wanxiang},
  year = {2022},
  month = dec,
  number = {arXiv:2212.13465},
  eprint = {arXiv:2212.13465},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {Table-and-text hybrid question answering (HybridQA) is a widely used and challenging NLP task commonly applied in the financial and scientific domain. The early research focuses on migrating other QA task methods to HybridQA, while with further research, more and more HybridQAspecific methods have been present. With the rapid development of HybridQA, the systematic survey is still under-explored to summarize the main techniques and advance further research. So we present this work to summarize the current HybridQA benchmarks and methods, then analyze the challenges and future directions of this task. The contributions of this paper can be summarized in three folds: (1) first survey, to our best knowledge, including benchmarks, methods and challenges for HybridQA; (2) systematic investigation with the reasonable comparison of the existing systems to articulate their advantages and shortcomings; (3) detailed analysis of challenges in four important dimensions to shed light on future directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/74R47IKI/Wang et al. - 2022 - A Survey on Table-and-Text HybridQA Concepts, Met.pdf}
}

@misc{wangTextEmbeddingsWeaklySupervised2022,
  title = {Text {{Embeddings}} by {{Weakly-Supervised Contrastive Pre-training}}},
  author = {Wang, Liang and Yang, Nan and Huang, Xiaolong and Jiao, Binxing and Yang, Linjun and Jiang, Daxin and Majumder, Rangan and Wei, Furu},
  year = {2022},
  month = dec,
  number = {arXiv:2212.03533},
  eprint = {arXiv:2212.03533},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.03533},
  urldate = {2023-01-02},
  abstract = {This paper presents E5, a family of state-of-the-art text embeddings that transfer well to a wide range of tasks. The model is trained in a contrastive manner with weak supervision signals from our curated large-scale text pair dataset (called CCPairs). E5 can be readily used as a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification, achieving strong performance in both zero-shot and fine-tuned settings. We conduct extensive evaluations on 56 datasets from the BEIR and MTEB benchmarks. For zero-shot settings, E5 is the first model that outperforms the strong BM25 baseline on the BEIR retrieval benchmark without using any labeled data. When fine-tuned, E5 obtains the best results on the MTEB benchmark, beating existing embedding models with 40x more parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/H69UEM64/Wang et al. - 2022 - Text Embeddings by Weakly-Supervised Contrastive P.pdf;/home/xav/Zotero/storage/JBBZRDSZ/2212.html}
}

@misc{wangTimeBERTExtendingPreTrained2022,
  title = {{{TimeBERT}}: {{Extending Pre-Trained Language Representations}} with {{Temporal Information}}},
  shorttitle = {{{TimeBERT}}},
  author = {Wang, Jiexin and Jatowt, Adam and Yoshikawa, Masatoshi},
  year = {2022},
  month = aug,
  number = {arXiv:2204.13032},
  eprint = {arXiv:2204.13032},
  publisher = {{arXiv}},
  urldate = {2023-02-06},
  abstract = {Time is an important aspect of documents and is used in a range of NLP and IR tasks. In this work, we investigate methods for incorporating temporal information during pre-training to further improve the performance on time-related tasks. Compared with BERT which utilizes synchronic document collections (BooksCorpus and English Wikipedia) as the training corpora, we use long-span temporal news article collection for building word representations. We introduce TimeBERT, a novel language representation model trained on a temporal collection of news articles via two new pre-training tasks, which harness two distinct temporal signals to construct time-aware language representations. The experimental results show that TimeBERT consistently outperforms BERT and other existing pre-trained models, with substantial gains on different downstream NLP tasks or applications for which time is of high importance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/REC6YVLJ/Wang et al. - 2022 - TimeBERT Extending Pre-Trained Language Represent.pdf}
}

@misc{wangWhatLanguageModel2022,
  title = {What {{Language Model Architecture}} and {{Pretraining Objective Work Best}} for {{Zero-Shot Generalization}}?},
  author = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05832},
  eprint = {arXiv:2204.05832},
  publisher = {{arXiv}},
  urldate = {2022-09-02},
  abstract = {Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/TUJTG4Q7/Wang et al. - 2022 - What Language Model Architecture and Pretraining O.pdf}
}

@article{wangZeroLabelLanguageLearning2021,
  title = {Towards {{Zero-Label Language Learning}}},
  author = {Wang, Zirui and Yu, Adams Wei and Firat, Orhan and Cao, Yuan},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.09193 [cs]},
  eprint = {2109.09193},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {This paper explores zero-label learning in Natural Language Processing (NLP), whereby no human-annotated data is used anywhere during training and models are trained purely on synthetic data. At the core of our framework is a novel approach for better leveraging the powerful pretrained language models. Specifically, inspired by the recent success of few-shot inference on GPT-3, we present a training data creation procedure named Unsupervised Data Generation (UDG), which leverages few-shot prompts to synthesize high-quality training data without real human annotations. Our method enables zero-label learning as we train task-specific models solely on the synthetic data, yet we achieve better or comparable results from strong baseline models trained on human-labeled data. Furthermore, when mixed with labeled data, our approach serves as a highly effective data augmentation procedure, achieving new state-of-the-art results on the SuperGLUE benchmark.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/WZYWXFKZ/Wang et al. - 2021 - Towards Zero-Label Language Learning.pdf;/home/xav/Zotero/storage/JQ9XH8HQ/2109.html}
}

@misc{wardArgumentativeRewardLearning2022,
  title = {Argumentative {{Reward Learning}}: {{Reasoning About Human Preferences}}},
  shorttitle = {Argumentative {{Reward Learning}}},
  author = {Ward, Francis Rhys and Belardinelli, Francesco and Toni, Francesca},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14010},
  eprint = {arXiv:2209.14010},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14010},
  urldate = {2022-12-21},
  abstract = {We define a novel neuro-symbolic framework, argumentative reward learning, which combines preference-based argumentation with existing approaches to reinforcement learning from human feedback. Our method improves prior work by generalising human preferences, reducing the burden on the user and increasing the robustness of the reward model. We demonstrate this with a number of experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/L2GS8MX2/Ward et al. - 2022 - Argumentative Reward Learning Reasoning About Hum.pdf}
}

@misc{weiChainThoughtPrompting2022,
  title = {Chain of {{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2022},
  month = jun,
  number = {arXiv:2201.11903},
  eprint = {2201.11903},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-25},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/UNZPZ5Q5/Wei et al. - 2022 - Chain of Thought Prompting Elicits Reasoning in La.pdf}
}

@misc{weiEmergentAbilitiesLarge2022,
  title = {Emergent {{Abilities}} of {{Large Language Models}}},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07682},
  eprint = {arXiv:2206.07682},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.07682},
  urldate = {2022-11-28},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BH8IL2ZL/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf;/home/xav/Zotero/storage/LSYKAFFH/2206.html}
}

@misc{weiFinetunedLanguageModels2022,
  title = {Finetuned {{Language Models Are Zero-Shot Learners}}},
  author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y. and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M. and Le, Quoc V.},
  year = {2022},
  month = feb,
  number = {arXiv:2109.01652},
  eprint = {arXiv:2109.01652},
  publisher = {{arXiv}},
  urldate = {2023-02-01},
  abstract = {This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning\textemdash finetuning language models on a collection of datasets described via instructions\textemdash substantially improves zeroshot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/CZ3CJGII/Wei et al. - 2022 - Finetuned Language Models Are Zero-Shot Learners.pdf}
}

@article{weizenbaumELIZAComputerProgram1966,
  title = {{{ELIZA}}\textemdash a Computer Program for the Study of Natural Language Communication between Man and Machine},
  author = {Weizenbaum, Joseph},
  year = {1966},
  langid = {english},
  annotation = {QID: Q55869048},
  file = {/home/xav/Zotero/storage/IDSNCJNI/Weizenbaum - ELIZA‚Äîa computer program for the study of natural .pdf}
}

@misc{wengLargeLanguageModels2022,
  title = {Large {{Language Models}} Are Reasoners with {{Self-Verification}}},
  author = {Weng, Yixuan and Zhu, Minjun and He, Shizhu and Liu, Kang and Zhao, Jun},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09561},
  eprint = {arXiv:2212.09561},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.09561},
  urldate = {2023-01-29},
  abstract = {When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from incorrect CoT. Code is available at \textbackslash url\{https://github.com/WENGSYX/Self-Verification\}},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/4GGZ5B2H/Weng et al. - 2022 - Large Language Models are reasoners with Self-Veri.pdf;/home/xav/Zotero/storage/BXZ2FTFX/2212.html}
}

@misc{westonAICompleteQuestionAnswering2015,
  title = {Towards {{AI-Complete Question Answering}}: {{A Set}} of {{Prerequisite Toy Tasks}}},
  shorttitle = {Towards {{AI-Complete Question Answering}}},
  author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and {van Merri{\"e}nboer}, Bart and Joulin, Armand and Mikolov, Tomas},
  year = {2015},
  month = dec,
  number = {arXiv:1502.05698},
  eprint = {arXiv:1502.05698},
  publisher = {{arXiv}},
  urldate = {2023-02-01},
  abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/5AX74XDW/Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of P.pdf}
}

@article{westSymbolicKnowledgeDistillation2021,
  title = {Symbolic {{Knowledge Distillation}}: From {{General Language Models}} to {{Commonsense Models}}},
  shorttitle = {Symbolic {{Knowledge Distillation}}},
  author = {West, Peter and Bhagavatula, Chandra and Hessel, Jack and Hwang, Jena D. and Jiang, Liwei and Bras, Ronan Le and Lu, Ximing and Welleck, Sean and Choi, Yejin},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.07178 [cs]},
  eprint = {2110.07178},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {The common practice for training commonsense models has gone from-human-to-corpus-to-machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from-machine-to-corpus-to-machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically-as text-in addition to the neural model. We also distill only one aspect-the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/LKGQHZWQ/West et al. - 2021 - Symbolic Knowledge Distillation from General Lang.pdf;/home/xav/Zotero/storage/SFKYD2RZ/2110.html}
}

@misc{whangDataCollectionQuality2022,
  title = {Data {{Collection}} and {{Quality Challenges}} in {{Deep Learning}}: {{A Data-Centric AI Perspective}}},
  shorttitle = {Data {{Collection}} and {{Quality Challenges}} in {{Deep Learning}}},
  author = {Whang, Steven Euijong and Roh, Yuji and Song, Hwanjun and Lee, Jae-Gil},
  year = {2022},
  month = dec,
  number = {arXiv:2112.06409},
  eprint = {arXiv:2112.06409},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Data-centric AI is at the center of a fundamental shift in software engineering where machine learning becomes the new software, powered by big data and computing infrastructure. Here software engineering needs to be re-thought where data becomes a first-class citizen on par with code. One striking observation is that a significant portion of the machine learning process is spent on data preparation. Without good data, even the best machine learning algorithms cannot perform well. As a result, data-centric AI practices are now becoming mainstream. Unfortunately, many datasets in the real world are small, dirty, biased, and even poisoned. In this survey, we study the research landscape for data collection and data quality primarily for deep learning applications. Data collection is important because there is lesser need for feature engineering for recent deep learning approaches, but instead more need for large amounts of data. For data quality, we study data validation, cleaning, and integration techniques. Even if the data cannot be fully cleaned, we can still cope with imperfect data during model training using robust model training techniques. In addition, while bias and fairness have been less studied in traditional data management research, these issues become essential topics in modern machine learning applications. We thus study fairness measures and unfairness mitigation techniques that can be applied before, during, or after model training. We believe that the data management community is well poised to solve these problems.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/QL64MZAM/Whang et al. - 2022 - Data Collection and Quality Challenges in Deep Lea.pdf}
}

@misc{WhatMyAI,
  title = {What's in My {{AI}}?},
  journal = {Dr Alan D. Thompson \textendash{} Life Architect},
  urldate = {2022-05-19},
  abstract = {A Comprehensive Analysis of Datasets Used to Train GPT-1, GPT-2, GPT-3, GPT-NeoX-20B, Megatron-11B, MT-NLG, and Gopher Alan D. Thompson LifeArchitect.ai March 2022 26 pages incl title page, references, appendix. Download PDF (2.5MB). Abstract Pre-trained transformer language models have become a stepping stone towards artificial general intelligence (AGI), with some researchers reporting that AGI may evolve [\ldots ]},
  file = {/home/xav/Zotero/storage/FKLMABSG/What‚Äôs in my AI.pdf;/home/xav/Zotero/storage/4TZP5CTH/whats-in-my-ai.html}
}

@misc{WhyMetaLatest,
  title = {Why {{Meta}}'s Latest Large Language Model Survived Only Three Days Online},
  journal = {MIT Technology Review},
  urldate = {2023-02-16},
  abstract = {Galactica was supposed to help scientists. Instead, it mindlessly spat out biased and incorrect nonsense.},
  howpublished = {https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/},
  langid = {english},
  file = {/home/xav/Zotero/storage/ECBDVQZ4/meta-large-language-model-ai-only-survived-three-days-gpt-3-science.html}
}

@article{wiegreffeReframingHumanAICollaboration2021,
  title = {Reframing {{Human-AI Collaboration}} for {{Generating Free-Text Explanations}}},
  author = {Wiegreffe, Sarah and Hessel, Jack and Swayamdipta, Swabha and Riedl, Mark O. and Choi, Yejin},
  year = {2021},
  journal = {ArXiv},
  abstract = {A pipeline that combines GPT-3 with a supervised filter that incorporates humans-in-the-loop via binary acceptability judgments is created, and despite significant subjectivity intrinsic to judging acceptability, this approach is able to consistently filter GPT3 generated explanations deemed acceptable by humans. Large language models are increasingly capable of generating fluent-appearing text with relatively little task-specific supervision. But can these models accurately explain classification decisions? We consider the task of generating free-text explanations using a small number of human-written examples (i.e., in a few-shot manner). We find that (1) authoring higher-quality examples for prompting results in higher quality generations; and (2) surprisingly, in a head-to-head comparison, crowdworkers often prefer explanations generated by GPT-3 to crowdsourced humanwritten explanations contained within existing datasets. Crowdworker ratings also show, however, that while models produce factual, grammatical, and sufficient explanations, they have room to improve, e.g., along axes such as providing novel information and supporting the label. We create a pipeline that combines GPT-3 with a supervised filter that incorporates humans-in-the-loop via binary acceptability judgments. Despite significant subjectivity intrinsic to judging acceptability, our approach is able to consistently filter GPT-3 generated explanations deemed acceptable by humans.},
  file = {/home/xav/Zotero/storage/3XU25XMF/Wiegreffe et al. - 2021 - Reframing Human-AI Collaboration for Generating Fr.pdf}
}

@article{wiegreffeTeachMeExplain2021,
  title = {Teach {{Me}} to {{Explain}}: {{A Review}} of {{Datasets}} for {{Explainable NLP}}},
  shorttitle = {Teach {{Me}} to {{Explain}}},
  author = {Wiegreffe, Sarah and Marasovi{\'c}, Ana},
  year = {2021},
  journal = {undefined},
  urldate = {2022-05-04},
  abstract = {This review identifies three predominant classes of explanations (highlights, free-text, and structured), organize the literature on annotating each type, point to what has been learned to date, and give recommendations for collecting EXNLP datasets in the future. Explainable NLP (EXNLP) has increasingly focused on collecting humanannotated explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as a loss signal to train models to produce explanations for their predictions, and as a means to evaluate the quality of model-generated explanations. In this review, we identify three predominant classes of explanations (highlights, free-text, and structured), organize the literature on annotating each type, point to what has been learned to date, and give recommendations for collecting EXNLP datasets in the future.},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/CYACMXZQ/Wiegreffe et Marasoviƒá - 2021 - Teach Me to Explain A Review of Datasets for Expl.pdf;/home/xav/Zotero/storage/8D2DFWNF/1a3a0047d74639784e7a7450854c28c9e7bdaf0a.html}
}

@inproceedings{williamsDeducingPropertiesRequired2021,
  title = {Deducing the {{Properties Required}} by {{General Collective Intelligence Platforms}}},
  author = {Williams, Andy E.},
  year = {2021},
  doi = {10.31730/osf.io/243wx},
  abstract = {GCI is proposed here to be the most important innovation in the history and immediate future of human civilization. A functional modeling approach is used to derive the properties that must be possessed by a platform with the capacity to significantly increase the general collective intelligence or c factor of groups. Such platforms have been termed ``General Collective Intelligence'' or GCI platforms. Having general problem-solving ability, a GCI potentially enables groups to execute any collective reasoning process, including abstracting (generalizing) a reasoning process so it might be reused in any other domain where it applies. A GCI can be shown to have the potential to exponentially increase the capacity of a group to create generalizations and other relationships, and capacity to store and exchange those relationships. Since relationships are concepts, and since the number of relationships between concepts better specify the location of any concept in conceptual space and therefore increases the density of conceptual space as a whole, GCI represents a phase change in collective cognition at which the collective conceptual space can expand exponentially in size and density. Each reasoning process connecting this far larger space of concepts has outcomes, making it potentially possible through these additional concepts to accumulate far greater impact on any outcome in the world. Because this phase change is not believed to have been possible at any point before in history, and is believed cannot occur again until the advent of another system with general problem-solving ability, such as a second order GCI or an Artificial General Intelligence (AGI), and because both AGI and second order GCI are believed to require GCI, GCI is proposed here to be the most important innovation in the history and immediate future of human civilization.},
  file = {/home/xav/Zotero/storage/XBQYEFQM/Williams - 2021 - Deducing the Properties Required by General Collec.pdf}
}

@article{wilsonSituationalRelevance1973,
  title = {Situational Relevance},
  author = {Wilson, Patrick},
  year = {1973},
  month = aug,
  journal = {Information Storage and Retrieval},
  volume = {9},
  number = {8},
  pages = {457--471},
  issn = {0020-0271},
  doi = {10.1016/0020-0271(73)90096-X},
  urldate = {2023-04-07},
  abstract = {The concept of situational relevance is introduced, based on W. S. Cooper's definition of logical relevance, on the notion of evidential relevance drawn from inductive logic, on the notions of a personal stock of knowledge and a set of personal concerns, the latter explained in terms of preferences over ranges of alternatives. Situationally relevant items of information are those that answer, or logically help to answer, questions of concern. Significant situationally relevant information is explained in terms of changes of view in relation to questions of concern. It is claimed that situational relevance is an explication of the ordinary notion of practical relevance, and that it is the appropriate relevance concept to use in evaluation of systems supplying practically relevant information.},
  langid = {english},
  file = {/home/xav/Zotero/storage/2LEJY458/002002717390096X.html}
}

@article{witteveenParaphrasingLargeLanguage2019,
  title = {Paraphrasing with {{Large Language Models}}},
  author = {Witteveen, Sam and Andrews, Martin},
  year = {2019},
  journal = {Proceedings of the 3rd Workshop on Neural Generation and Translation},
  eprint = {1911.09661},
  pages = {215--220},
  doi = {10.18653/v1/D19-5623},
  urldate = {2022-05-14},
  abstract = {Recently, large language models such as GPT-2 have shown themselves to be extremely adept at text generation and have also been able to achieve high-quality results in many downstream NLP tasks such as text classification, sentiment analysis and question answering with the aid of fine-tuning. We present a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects. Our approach is demonstrated to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/PQP84PHF/Witteveen et Andrews - 2019 - Paraphrasing with Large Language Models.pdf;/home/xav/Zotero/storage/7TWNE2PN/1911.html}
}

@misc{WSDMWebSearch,
  title = {{{WSDM}} | {{Web Search}} and {{Data Mining}}},
  urldate = {2023-02-15},
  howpublished = {https://www.wsdm-conference.org/},
  file = {/home/xav/Zotero/storage/FKULUDMC/www.wsdm-conference.org.html}
}

@inproceedings{wuAIChainsTransparent2022,
  title = {{{AI Chains}}: {{Transparent}} and {{Controllable Human-AI Interaction}} by {{Chaining Large Language Model Prompts}}},
  shorttitle = {{{AI Chains}}},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wu, Tongshuang and Terry, Michael and Cai, Carrie Jun},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--22},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3517582},
  urldate = {2022-06-02},
  abstract = {Although large language models (LLMs) have demonstrated impressive potential on simple tasks, their breadth of scope, lack of transparency, and insufficient controllability can make them less effective when assisting humans on more complex tasks. In response, we introduce the concept of Chaining LLM steps together, where the output of one step becomes the input for the next, thus aggregating the gains per step. We first define a set of LLM primitive operations useful for Chain construction, then present an interactive system where users can modify these Chains, along with their intermediate results, in a modular way. In a 20-person user study, we found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration. Additionally, we saw that users developed new ways of interacting with LLMs through Chains: they leveraged sub-tasks to calibrate model expectations, compared and contrasted alternative strategies by observing parallel downstream effects, and debugged unexpected model outputs by ``unit-testing'' sub-components of a Chain. In two case studies, we further explore how LLM Chains may be used in future applications.},
  isbn = {978-1-4503-9157-3},
  keywords = {Human-AI Interaction,Large Language Models,Natural Language Processing},
  file = {/home/xav/Zotero/storage/KJV5KYK2/Wu et al. - 2022 - AI Chains Transparent and Controllable Human-AI I.pdf}
}

@article{wuGraphfreeMultihopReading2021,
  title = {Graph-Free {{Multi-hop Reading Comprehension}}: {{A Select-to-Guide Strategy}}},
  shorttitle = {Graph-Free {{Multi-hop Reading Comprehension}}},
  author = {Wu, Bohong and Zhang, Zhuosheng and Zhao, Hai},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.11823 [cs]},
  eprint = {2107.11823},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Multi-hop reading comprehension (MHRC) requires not only to predict the correct answer span in the given passage, but also to provide a chain of supporting evidences for reasoning interpretability. It is natural to model such a process into graph structure by understanding multi-hop reasoning as jumping over entity nodes, which has made graph modelling dominant on this task. Recently, there have been dissenting voices about whether graph modelling is indispensable due to the inconvenience of the graph building, however existing state-of-the-art graph-free attempts suffer from huge performance gap compared to graph-based ones. This work presents a novel graph-free alternative which firstly outperform all graph models on MHRC. In detail, we exploit a select-to-guide (S2G) strategy to accurately retrieve evidence paragraphs in a coarse-to-fine manner, incorporated with two novel attention mechanisms, which surprisingly shows conforming to the nature of multi-hop reasoning. Our graph-free model achieves significant and consistent performance gain over strong baselines and the current new state-of-the-art on the MHRC benchmark, HotpotQA, among all the published works.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/6RRUGRXQ/Wu et al. - 2021 - Graph-free Multi-hop Reading Comprehension A Sele.pdf}
}

@misc{wuHumanInTheLoopDocumentLayout2021,
  title = {Human-{{In-The-Loop Document Layout Analysis}}},
  author = {Wu, Xingjiao and Ma, Tianlong and Li, Xin and Chen, Qin and He, Liang},
  year = {2021},
  month = aug,
  number = {arXiv:2108.02095},
  eprint = {arXiv:2108.02095},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.02095},
  urldate = {2022-11-18},
  abstract = {Document layout analysis (DLA) aims to divide a document image into different types of regions. DLA plays an important role in the document content understanding and information extraction systems. Exploring a method that can use less data for effective training contributes to the development of DLA. We consider a Human-in-the-loop (HITL) collaborative intelligence in the DLA. Our approach was inspired by the fact that the HITL push the model to learn from the unknown problems by adding a small amount of data based on knowledge. The HITL select key samples by using confidence. However, using confidence to find key samples is not suitable for DLA tasks. We propose the Key Samples Selection (KSS) method to find key samples in high-level tasks (semantic segmentation) more accurately through agent collaboration, effectively reducing costs. Once selected, these key samples are passed to human beings for active labeling, then the model will be updated with the labeled samples. Hence, we revisited the learning system from reinforcement learning and designed a sample-based agent update strategy, which effectively improves the agent's ability to accept new samples. It achieves significant improvement results in two benchmarks (DSSE-200 (from 77.1\% to 86.3\%) and CS-150 (from 88.0\% to 95.6\%)) by using 10\% of labeled data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/VMCURHVT/Wu et al. - 2021 - Human-In-The-Loop Document Layout Analysis.pdf}
}

@misc{wuOneTeacherEnough2021,
  title = {One {{Teacher}} Is {{Enough}}? {{Pre-trained Language Model Distillation}} from {{Multiple Teachers}}},
  shorttitle = {One {{Teacher}} Is {{Enough}}?},
  author = {Wu, Chuhan and Wu, Fangzhao and Huang, Yongfeng},
  year = {2021},
  month = jun,
  number = {arXiv:2106.01023},
  eprint = {arXiv:2106.01023},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.01023},
  urldate = {2023-02-10},
  abstract = {Pre-trained language models (PLMs) achieve great success in NLP. However, their huge model sizes hinder their applications in many practical systems. Knowledge distillation is a popular technique to compress PLMs, which learns a small student model from a large teacher PLM. However, the knowledge learned from a single teacher may be limited and even biased, resulting in low-quality student model. In this paper, we propose a multi-teacher knowledge distillation framework named MT-BERT for pre-trained language model compression, which can train high-quality student model from multiple teacher PLMs. In MT-BERT we design a multi-teacher co-finetuning method to jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MT-BERT in compressing PLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/69QA2KE4/Wu et al. - 2021 - One Teacher is Enough Pre-trained Language Model .pdf;/home/xav/Zotero/storage/P5BVPDP2/2106.html}
}

@book{wuPromptChainerChainingLarge2022,
  title = {{{PromptChainer}}: {{Chaining Large Language Model Prompts}} through {{Visual Programming}}},
  shorttitle = {{{PromptChainer}}},
  author = {Wu, Tongshuang and Jiang, Ellen and Donsbach, Aaron and Gray, Jeff and Molina, Alejandra and Terry, Michael and Cai, Carrie},
  year = {2022},
  month = mar,
  abstract = {While LLMs can effectively help prototype single ML functionalities, many real-world applications involve complex tasks that cannot be easily handled via a single run of an LLM. Recent work has found that chaining multiple LLM runs together (with the output of one step being the input to the next) can help users accomplish these more complex tasks, and in a way that is perceived to be more transparent and controllable. However, it remains unknown what users need when authoring their own LLM chains -- a key step for lowering the barriers for non-AI-experts to prototype AI-infused applications. In this work, we explore the LLM chain authoring process. We conclude from pilot studies find that chaining requires careful scaffolding for transforming intermediate node outputs, as well as debugging the chain at multiple granularities; to help with these needs, we designed PromptChainer, an interactive interface for visually programming chains. Through case studies with four people, we show that PromptChainer supports building prototypes for a range of applications, and conclude with open questions on scaling chains to complex tasks, and supporting low-fi chain prototyping.},
  file = {/home/xav/Zotero/storage/HRKEY9BS/Wu et al. - 2022 - PromptChainer Chaining Large Language Model Promp.pdf}
}

@misc{wuRecursivelySummarizingBooks2021,
  title = {Recursively {{Summarizing Books}} with {{Human Feedback}}},
  author = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M. and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  year = {2021},
  month = sep,
  number = {arXiv:2109.10862},
  eprint = {arXiv:2109.10862},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.10862},
  urldate = {2023-01-16},
  abstract = {A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases (\$\textbackslash sim5\textbackslash\%\$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/JJK846TI/Wu et al. - 2021 - Recursively Summarizing Books with Human Feedback.pdf;/home/xav/Zotero/storage/CJK3IENI/2109.html}
}

@article{wuSurveyHumanintheloopMachine2021,
  title = {A {{Survey}} of {{Human-in-the-loop}} for {{Machine Learning}}},
  author = {Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  year = {2021},
  month = nov,
  journal = {arXiv:2108.00941 [cs]},
  eprint = {2108.00941},
  primaryclass = {cs},
  urldate = {2022-03-21},
  abstract = {Human-in-the-loop aims to train an accurate prediction model with minimum cost by integrating human knowledge and experience. Humans can provide training data for machine learning applications and directly accomplish tasks that are hard for computers in the pipeline with the help of machine-based approaches. In this paper, we survey existing works on human-in-the-loop from a data perspective and classify them into three categories with a progressive relationship: (1) the work of improving model performance from data processing, (2) the work of improving model performance through interventional model training, and (3) the design of the system independent human-in-the-loop. Using the above categorization, we summarize major approaches in the field; along with their technical strengths/ weaknesses, we have simple classification and discussion in natural language processing, computer vision, and others. Besides, we provide some open challenges and opportunities. This survey intends to provide a high-level summarization for human-in-the-loop and motivates interested readers to consider approaches for designing effective human-in-the-loop solutions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {QID: Q114174713},
  file = {/home/xav/Zotero/storage/5PNDV2J7/Wu et al. - 2021 - A Survey of Human-in-the-loop for Machine Learning.pdf;/home/xav/Zotero/storage/H5AVHVBD/Wu et al. - 2022 - A Survey of Human-in-the-loop for Machine Learning.pdf;/home/xav/Zotero/storage/M3RLM2VP/2108.html}
}

@misc{xiaoOffsiteTuningTransferLearning2023,
  title = {Offsite-{{Tuning}}: {{Transfer Learning}} without {{Full Model}}},
  shorttitle = {Offsite-{{Tuning}}},
  author = {Xiao, Guangxuan and Lin, Ji and Han, Song},
  year = {2023},
  month = feb,
  number = {arXiv:2302.04870},
  eprint = {arXiv:2302.04870},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.04870},
  urldate = {2023-02-13},
  abstract = {Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available at https://github.com/mit-han-lab/offsite-tuning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ISUWIPUX/Xiao et al. - 2023 - Offsite-Tuning Transfer Learning without Full Mod.pdf;/home/xav/Zotero/storage/A54ME5RV/2302.html}
}

@misc{xieUnifiedSKGUnifyingMultiTasking2022,
  title = {{{UnifiedSKG}}: {{Unifying}} and {{Multi-Tasking Structured Knowledge Grounding}} with {{Text-to-Text Language Models}}},
  shorttitle = {{{UnifiedSKG}}},
  author = {Xie, Tianbao and Wu, Chen Henry and Shi, Peng and Zhong, Ruiqi and Scholak, Torsten and Yasunaga, Michihiro and Wu, Chien-Sheng and Zhong, Ming and Yin, Pengcheng and Wang, Sida I. and Zhong, Victor and Wang, Bailin and Li, Chengzu and Boyle, Connor and Ni, Ansong and Yao, Ziyu and Radev, Dragomir and Xiong, Caiming and Kong, Lingpeng and Zhang, Rui and Smith, Noah A. and Zettlemoyer, Luke and Yu, Tao},
  year = {2022},
  month = oct,
  number = {arXiv:2201.05966},
  eprint = {arXiv:2201.05966},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.05966},
  urldate = {2023-01-24},
  abstract = {Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UnifiedSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UnifiedSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves state-of-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UnifiedSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UnifiedSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UnifiedSKG is easily extensible to more tasks, and it is open-sourced at https://github.com/hkunlp/unifiedskg.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/KRHVSCBH/Xie et al. - 2022 - UnifiedSKG Unifying and Multi-Tasking Structured .pdf;/home/xav/Zotero/storage/2H295GFF/2201.html}
}

@article{xiongAdaptingPretrainedTexttoText2022,
  title = {Adapting {{Pretrained Text-to-Text Models}} for {{Long Text Sequences}}},
  author = {Xiong, Wenhan and Gupta, Anchit and Toshniwal, Shubham and Mehdad, Yashar and Yih, Wen-tau},
  year = {2022},
  journal = {undefined},
  urldate = {2022-10-03},
  abstract = {A long-context model is built that achieves competitive performance on long-text QA tasks and establishes the new state of the art onLong-text summarization datasets, often outperforming previous methods with larger model sizes. We present an empirical study of adapting an existing pretrained text-to-text model for long-sequence inputs. Through a comprehensive study along three axes of the pretraining pipeline \textendash{} model architecture, optimization objective, and pretraining corpus, we propose an effective recipe to build long-context models from existing short-context models. Specifically, we replace the full attention in transformers with pooling-augmented blockwise attention , and pretrain the model with a masked-span prediction task with spans of varying length. In terms of the pretraining corpus, we find that using randomly concatenated short-documents from a large open-domain corpus results in better performance than using existing long document corpora which are typically limited in their domain coverage. With these findings, we build a long-context model that achieves competitive performance on long-text QA tasks and establishes the new state of the art on five long-text summarization datasets , often outperforming previous methods with larger model sizes.},
  langid = {english},
  file = {/home/xav/Zotero/storage/JDU6ME3B/Xiong et al. - 2022 - Adapting Pretrained Text-to-Text Models for Long T.pdf}
}

@article{xiongAnsweringComplexOpenDomain2021,
  title = {Answering {{Complex Open-Domain Questions}} with {{Multi-Hop Dense Retrieval}}},
  author = {Xiong, Wenhan and Li, Xiang Lorraine and Iyer, Srini and Du, Jingfei and Lewis, Patrick and Wang, William Yang and Mehdad, Yashar and Yih, Wen-tau and Riedel, Sebastian and Kiela, Douwe and O{\u g}uz, Barlas},
  year = {2021},
  month = feb,
  journal = {arXiv:2009.12756 [cs]},
  eprint = {2009.12756},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {We propose a simple and efficient multi-hop dense retrieval approach for answering complex open-domain questions, which achieves state-of-the-art performance on two multi-hop datasets, HotpotQA and multi-evidence FEVER. Contrary to previous work, our method does not require access to any corpus-specific information, such as inter-document hyperlinks or human-annotated entity markers, and can be applied to any unstructured text corpus. Our system also yields a much better efficiency-accuracy trade-off, matching the best published accuracy on HotpotQA while being 10 times faster at inference time.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/FTFAIRCH/Xiong et al. - 2021 - Answering Complex Open-Domain Questions with Multi.pdf}
}

@article{xiongFinetuningMultihopQuestion2020,
  title = {Fine-Tuning {{Multi-hop Question Answering}} with {{Hierarchical Graph Network}}},
  author = {Xiong, Guanming},
  year = {2020},
  month = jun,
  journal = {arXiv:2004.13821 [cs]},
  eprint = {2004.13821},
  primaryclass = {cs},
  urldate = {2022-03-28},
  abstract = {In this paper, we present a two stage model for multi-hop question answering. The first stage is a hierarchical graph network, which is used to reason over multi-hop question and is capable to capture different levels of granularity using the nature structure(i.e., paragraphs, questions, sentences and entities) of documents. The reasoning process is convert to node classify task(i.e., paragraph nodes and sentences nodes). The second stage is a language model fine-tuning task. In a word, stage one use graph neural network to select and concatenate support sentences as one paragraph, and stage two find the answer span in language model fine-tuning paradigm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/WE99D3EB/Xiong - 2020 - Fine-tuning Multi-hop Question Answering with Hier.pdf;/home/xav/Zotero/storage/P65D9IUV/2004.html}
}

@misc{xuBigTextQAQuestionAnswering2022,
  title = {{{BigText-QA}}: {{Question Answering}} over a {{Large-Scale Hybrid Knowledge Graph}}},
  shorttitle = {{{BigText-QA}}},
  author = {Xu, Jingjing and Biryukov, M. and Theobald, M. and Venugopal, V. E.},
  year = {2022},
  urldate = {2022-12-19},
  abstract = {BigText-QA is developed, which is able to combine the best of both worlds\textemdash a canonical set of named entities, mapped to a structured background KB (such as YAGO or Wikidata), as well as an open set of textual clauses providing highly diversified relational paraphrases with rich context information. Answering complex questions over textual resources remains a challenging problem\textemdash especially when interpreting the fine-grained relationships among multiple entities that occur within a naturallanguage question or clue. Curated knowledge bases (KBs), such as YAGO, DBpedia, Freebase and Wikidata, have been widely used in this context and gained great acceptance for question-answering (QA) applications in the past decade. While current KBs offer a concise representation of structured knowledge, they lack the variety of formulations and semantic nuances as well as the context of information provided by the natural-language sources. With BigText-QA, we aim to develop an integrated QA system which is able to answer questions based on a more redundant form of a knowledge graph (KG) that organizes both structured and unstructured (i.e., ``hybrid'') knowledge in a unified graphical representation. BigText-QA thereby is able to combine the best of both worlds\textemdash a canonical set of named entities, mapped to a structured background KB (such as YAGO or Wikidata), as well as an open set of textual clauses providing highly diversified relational paraphrases with rich context information.},
  howpublished = {https://www.semanticscholar.org/paper/BigText-QA\%3A-Question-Answering-over-a-Large-Scale-Xu-Biryukov/1122fab167a885d1d2f2a3be2826d2649baf8575},
  langid = {english},
  file = {/home/xav/Zotero/storage/9PJGJE4F/Xu et al. - 2022 - BigText-QA Question Answering over a Large-Scale .pdf}
}

@article{xuDynamicSemanticGraph2021,
  title = {Dynamic {{Semantic Graph Construction}} and {{Reasoning}} for {{Explainable Multi-hop Science Question Answering}}},
  author = {Xu, Weiwen and Zhang, Huihui and Cai, Deng and Lam, Wai},
  year = {2021},
  month = may,
  journal = {arXiv:2105.11776 [cs]},
  eprint = {2105.11776},
  primaryclass = {cs},
  urldate = {2022-04-06},
  abstract = {Knowledge retrieval and reasoning are two key stages in multi-hop question answering (QA) at web scale. Existing approaches suffer from low confidence when retrieving evidence facts to fill the knowledge gap and lack transparent reasoning process. In this paper, we propose a new framework to exploit more valid facts while obtaining explainability for multi-hop QA by dynamically constructing a semantic graph and reasoning over it. We employ Abstract Meaning Representation (AMR) as semantic graph representation. Our framework contains three new ideas: (a) \{\textbackslash tt AMR-SG\}, an AMR-based Semantic Graph, constructed by candidate fact AMRs to uncover any hop relations among question, answer and multiple facts. (b) A novel path-based fact analytics approach exploiting \{\textbackslash tt AMR-SG\} to extract active facts from a large fact pool to answer questions. (c) A fact-level relation modeling leveraging graph convolution network (GCN) to guide the reasoning process. Results on two scientific multi-hop QA datasets show that we can surpass recent approaches including those using additional knowledge graphs while maintaining high explainability on OpenBookQA and achieve a new state-of-the-art result on ARC-Challenge in a computationally practicable setting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/JUPIGZZ3/Xu et al. - 2021 - Dynamic Semantic Graph Construction and Reasoning .pdf;/home/xav/Zotero/storage/CEDZWHZZ/2105.html}
}

@misc{xuGoldfishMemoryLongTerm2021,
  title = {Beyond {{Goldfish Memory}}: {{Long-Term Open-Domain Conversation}}},
  shorttitle = {Beyond {{Goldfish Memory}}},
  author = {Xu, Jing and Szlam, Arthur and Weston, Jason},
  year = {2021},
  month = jul,
  number = {arXiv:2107.07567},
  eprint = {arXiv:2107.07567},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.07567},
  urldate = {2023-02-10},
  abstract = {Despite recent improvements in open-domain dialogue models, state of the art models are trained and evaluated on short conversations with little context. In contrast, the long-term conversation setting has hardly been studied. In this work we collect and release a human-human dataset consisting of multiple chat sessions whereby the speaking partners learn about each other's interests and discuss the things they have learnt from past sessions. We show how existing models trained on existing datasets perform poorly in this long-term conversation setting in both automatic and human evaluations, and we study long-context models that can perform much better. In particular, we find retrieval-augmented methods and methods with an ability to summarize and recall previous conversations outperform the standard encoder-decoder architectures currently considered state of the art.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/N44NTTHJ/Xu et al. - 2021 - Beyond Goldfish Memory Long-Term Open-Domain Conv.pdf;/home/xav/Zotero/storage/PHCB8BRL/2107.html}
}

@article{xuHumanParityCommonsenseQA2021,
  title = {Human {{Parity}} on {{CommonsenseQA}}: {{Augmenting Self-Attention}} with {{External Attention}}},
  shorttitle = {Human {{Parity}} on {{CommonsenseQA}}},
  author = {Xu, Yichong and Zhu, Chenguang and Wang, Shuohang and Sun, Siqi and Cheng, Hao and Liu, Xiaodong and Gao, Jianfeng and He, Pengcheng and Zeng, Michael and Huang, Xuedong},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.03254 [cs]},
  eprint = {2112.03254},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\% in comparison to the human accuracy of 88.9\%.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/2P9GMBS6/Xu et al. - 2021 - Human Parity on CommonsenseQA Augmenting Self-Att.pdf}
}

@article{xuLayoutLMv2MultimodalPretraining2022,
  title = {{{LayoutLMv2}}: {{Multi-modal Pre-training}} for {{Visually-Rich Document Understanding}}},
  shorttitle = {{{LayoutLMv2}}},
  author = {Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
  year = {2022},
  month = jan,
  journal = {arXiv:2012.14740 [cs]},
  eprint = {2012.14740},
  primaryclass = {cs},
  urldate = {2022-05-17},
  abstract = {Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-ofthe-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 \textrightarrow{} 0.8420), CORD (0.9493 \textrightarrow{} 0.9601), SROIE (0.9524 \textrightarrow{} 0.9781), Kleister-NDA (0.8340 \textrightarrow{} 0.8520), RVL-CDIP (0.9443 \textrightarrow{} 0.9564), and DocVQA (0.7295 \textrightarrow{} 0.8672). We made our model and code publicly available at https://aka.ms /layoutlmv2.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/FFNGQ5NB/Xu et al. - 2022 - LayoutLMv2 Multi-modal Pre-training for Visually-.pdf}
}

@misc{xuPrivacyPreservingMachineLearning2021,
  title = {Privacy-{{Preserving Machine Learning}}: {{Methods}}, {{Challenges}} and {{Directions}}},
  shorttitle = {Privacy-{{Preserving Machine Learning}}},
  author = {Xu, Runhua and Baracaldo, Nathalie and Joshi, James},
  year = {2021},
  month = sep,
  number = {arXiv:2108.04417},
  eprint = {arXiv:2108.04417},
  publisher = {{arXiv}},
  urldate = {2023-02-09},
  abstract = {Machine learning (ML) is increasingly being adopted in a wide variety of application domains. Usually, a well-performing ML model relies on a large volume of training data and high-powered computational resources. Such a need for and the use of huge volumes of data raise serious privacy concerns because of the potential risks of leakage of highly privacy-sensitive information; further, the evolving regulatory environments that increasingly restrict access to and use of privacy-sensitive data add significant challenges to fully benefiting from the power of ML for data-driven applications. A trained ML model may also be vulnerable to adversarial attacks such as membership, attribute, or property inference attacks and model inversion attacks. Hence, well-designed privacy-preserving ML (PPML) solutions are critically needed for many emerging applications. Increasingly, significant research efforts from both academia and industry can be seen in PPML areas that aim toward integrating privacy-preserving techniques into ML pipeline or specific algorithms, or designing various PPML architectures. In particular, existing PPML research cross-cut ML, systems and applications design, as well as security and privacy areas; hence, there is a critical need to understand state-of-the-art research, related challenges and a research roadmap for future research in PPML area. In this paper, we systematically review and summarize existing privacy-preserving approaches and propose a Phase, Guarantee, and Utility (PGU) triad based model to understand and guide the evaluation of various PPML solutions by decomposing their privacy-preserving functionalities. We discuss the unique characteristics and challenges of PPML and outline possible research directions that leverage as well as benefit multiple research communities such as ML, distributed systems, security and privacy.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/549HAE25/Xu et al. - 2021 - Privacy-Preserving Machine Learning Methods, Chal.pdf}
}

@misc{xuRaiseChildLarge2021,
  title = {Raise a {{Child}} in {{Large Language Model}}: {{Towards Effective}} and {{Generalizable Fine-tuning}}},
  shorttitle = {Raise a {{Child}} in {{Large Language Model}}},
  author = {Xu, Runxin and Luo, Fuli and Zhang, Zhiyuan and Tan, Chuanqi and Chang, Baobao and Huang, Songfang and Huang, Fei},
  year = {2021},
  month = sep,
  number = {arXiv:2109.05687},
  eprint = {arXiv:2109.05687},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, CHILD-TUNING, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that CHILDTUNING consistently outperforms the vanilla fine-tuning by 1.5 {$\sim$} 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 {$\sim$} 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that CHILD-TUNING can obtain better generalization performance by large margins.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/GXCT7WX6/Xu et al. - 2021 - Raise a Child in Large Language Model Towards Eff.pdf}
}

@misc{xuSurveyDynamicNeural2022,
  title = {A {{Survey}} on {{Dynamic Neural Networks}} for {{Natural Language Processing}}},
  author = {Xu, Canwen and McAuley, Julian},
  year = {2022},
  month = feb,
  number = {arXiv:2202.07101},
  eprint = {arXiv:2202.07101},
  publisher = {{arXiv}},
  urldate = {2023-02-09},
  abstract = {Effectively scaling large Transformer models is a main driver of recent advances in natural language processing. Dynamic neural networks, as an emerging research direction, are capable of scaling up neural networks with sub-linear increases in computation and time by dynamically adjusting their computational path based on the input. Dynamic neural networks could be a promising solution to the growing parameter numbers of pretrained language models, allowing both model pretraining with trillions of parameters and faster inference on mobile devices. In this survey, we summarize progress of three types of dynamic neural networks in NLP: skimming, mixture of experts, and early exit. We also highlight current challenges in dynamic neural networks and directions for future research.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/QUMRJALM/Xu et McAuley - 2022 - A Survey on Dynamic Neural Networks for Natural La.pdf}
}

@misc{xuUniversalDiscriminatorZeroShot2022,
  title = {A {{Universal Discriminator}} for {{Zero-Shot Generalization}}},
  author = {Xu, Haike and Lin, Zongyu and Zhou, Jing and Zheng, Yanan and Yang, Zhilin},
  year = {2022},
  month = nov,
  number = {arXiv:2211.08099},
  eprint = {arXiv:2211.08099},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.08099},
  urldate = {2022-12-21},
  abstract = {Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the option with the highest probability. This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0\textbackslash\%, 7.8\textbackslash\%, and 11.5\textbackslash\% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile, our approach requires minimal prompting efforts, which largely improves robustness and is essential for real-world applications. Furthermore, we also jointly train a generalized UD in combination with generative tasks, which maintains its advantage on discriminative tasks and simultaneously works on generative tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/84IFD36F/Xu et al. - 2022 - A Universal Discriminator for Zero-Shot Generaliza.pdf}
}

@inproceedings{yahosseiniExperimentalStudyModeling2020,
  title = {Experimental {{Study}} and {{Modeling}} of {{Three Classes}} of {{Collective Problem-Solving Methods}}},
  author = {Yahosseini, Kyanoush Seyed},
  year = {2020},
  doi = {10.17169/REFUBIUM-27367},
  abstract = {This work introduces a novel approach for aggregating judgments \textendash{} the transmission chain \textendash{} which has not yet been consistently evaluated in the context of collective intelligence and addresses the question of whether such a transmission chain can foster collective intelligence for binary-choice problems. In many social systems, groups of individuals can find remarkably efficient solutions to complex cognitive problems, sometimes even outperforming a single expert. The success of the group, however, crucially depends on how the judgments of the group members are aggregated to produce the collective answer. A large variety of such aggregation methods have been described in the literature, such as averaging the independent judgments, relying on the majority or setting up a group discussion. In the present work, we introduce a novel approach for aggregating judgments \textendash{} the transmission chain \textendash{} which has not yet been consistently evaluated in the context of collective intelligence. In a transmission chain, all group members have access to a unique collective solution and can improve it sequentially. Over repeated improvements, the collective solution that emerges reflects the judgments of every group member. We address the question of whether such a transmission chain can foster collective intelligence for binary-choice problems. In a series of},
  file = {/home/xav/Zotero/storage/WI9SUJAZ/Yahosseini - 2020 - Experimental Study and Modeling of Three Classes o.pdf}
}

@misc{yangChainThoughtImitation2022,
  title = {Chain of {{Thought Imitation}} with {{Procedure Cloning}}},
  author = {Yang, Mengjiao and Schuurmans, Dale and Abbeel, Pieter and Nachum, Ofir},
  year = {2022},
  month = may,
  number = {arXiv:2205.10816},
  eprint = {arXiv:2205.10816},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.10816},
  urldate = {2022-09-02},
  abstract = {Imitation learning aims to extract high-performance policies from logged demonstrations of expert behavior. It is common to frame imitation learning as a supervised learning problem in which one fits a function approximator to the input-output mapping exhibited by the logged demonstrations (input observations to output actions). While the framing of imitation learning as a supervised input-output learning problem allows for applicability in a wide variety of settings, it is also an overly simplistic view of the problem in situations where the expert demonstrations provide much richer insight into expert behavior. For example, applications such as path navigation, robot manipulation, and strategy games acquire expert demonstrations via planning, search, or some other multi-step algorithm, revealing not just the output action to be imitated but also the procedure for how to determine this action. While these intermediate computations may use tools not available to the agent during inference (e.g., environment simulators), they are nevertheless informative as a way to explain an expert's mapping of state to actions. To properly leverage expert procedure information without relying on the privileged tools the expert may have used to perform the procedure, we propose procedure cloning, which applies supervised sequence prediction to imitate the series of expert computations. This way, procedure cloning learns not only what to do (i.e., the output action), but how and why to do it (i.e., the procedure). Through empirical analysis on navigation, simulated robotic manipulation, and game-playing environments, we show that imitating the intermediate computations of an expert's behavior enables procedure cloning to learn policies exhibiting significant generalization to unseen environment configurations, including those configurations for which running the expert's procedure directly is infeasible.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/HTKXAVT3/Yang et al. - 2022 - Chain of Thought Imitation with Procedure Cloning.pdf}
}

@misc{yangEnhancingMultimodalMultihop2022,
  title = {Enhancing {{Multi-modal}} and {{Multi-hop Question Answering}} via {{Structured Knowledge}} and {{Unified Retrieval-Generation}}},
  author = {Yang, Qian and Chen, Qian and Wang, Wen and Hu, Baotian and Zhang, Min},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08632},
  eprint = {arXiv:2212.08632},
  publisher = {{arXiv}},
  urldate = {2023-01-24},
  abstract = {Multi-modal and multi-hop question answering aims to answer a question based on multiple input sources from different modalities. Previous methods retrieve the evidence separately and feed the retrieved evidence to a language model to generate the corresponding answer. However, these methods fail to build connections between candidates and thus cannot model the inter-dependent relation during retrieval. Moreover, the reasoning process over multi-modality candidates can be unbalanced without building alignments between different modalities. To address this limitation, we propose a Structured Knowledge and Unified Retrieval Generation based method (SKURG). We align the sources from different modalities via the shared entities and map them into a shared semantic space via structured knowledge. Then, we utilize a unified retrieval-generation decoder to integrate intermediate retrieval results for answer generation and adaptively determine the number of retrieval steps. We perform experiments on two multi-modal and multi-hop datasets: WebQA and MultimodalQA. The results demonstrate that SKURG achieves state-of-the-art performance on both retrieval and answer generation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/6XAFIGQ5/Yang et al. - 2022 - Enhancing Multi-modal and Multi-hop Question Answe.pdf}
}

@article{yangLearningDecomposeCompound2018,
  title = {Learning to {{Decompose Compound Questions}} with {{Reinforcement Learning}}},
  author = {Yang, Haihong and Wang, Han and Guo, Shuang and Zhang, Wei and Chen, Huajun},
  year = {2018},
  month = sep,
  urldate = {2022-05-04},
  abstract = {We propose a learning-to-decompose agent that helps simple-question answerers to answer compound question over knowledge graph.},
  langid = {english},
  file = {/home/xav/Zotero/storage/A5GV7XUU/Yang et al. - 2018 - Learning to Decompose Compound Questions with Rein.pdf;/home/xav/Zotero/storage/S66QQUEC/LEARNING TO DECOMPOSE COMPOUND QUESTIONS WITH REIN.pdf;/home/xav/Zotero/storage/2ILMMT9P/forum.html}
}

@misc{yangLogicSolverInterpretableMath2022,
  title = {{{LogicSolver}}: {{Towards Interpretable Math Word Problem Solving}} with {{Logical Prompt-enhanced Learning}}},
  shorttitle = {{{LogicSolver}}},
  author = {Yang, Zhicheng and Qin, Jinghui and Chen, Jiaqi and Lin, Liang and Liang, Xiaodan},
  year = {2022},
  month = oct,
  number = {arXiv:2205.08232},
  eprint = {arXiv:2205.08232},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.08232},
  urldate = {2023-02-07},
  abstract = {Recently, deep learning models have made great progress in MWP solving on answer accuracy. However, they are uninterpretable since they mainly rely on shallow heuristics to achieve high performance without understanding and reasoning the grounded math logic. To address this issue and make a step towards interpretable MWP solving, we first construct a high-quality MWP dataset named InterMWP which consists of 11,495 MWPs and annotates interpretable logical formulas based on algebraic knowledge as the grounded linguistic logic of each solution equation. Different from existing MWP datasets, our InterMWP benchmark asks for a solver to not only output the solution expressions but also predict the corresponding logical formulas. We further propose a novel approach with logical prompt and interpretation generation, called LogicSolver. For each MWP, our LogicSolver first retrieves some highly-correlated algebraic knowledge and then passes them to the backbone model as prompts to improve the semantic representations of MWPs. With these improved semantic representations, our LogicSolver generates corresponding solution expressions and interpretable knowledge formulas in accord with the generated solution expressions, simultaneously. Experimental results show that our LogicSolver has stronger logical formula-based interpretability than baselines while achieving higher answer accuracy with the help of logical prompts, simultaneously. The source code and dataset is available at https://github.com/yangzhch6/InterMWP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/NS3YN2EZ/Yang et al. - 2022 - LogicSolver Towards Interpretable Math Word Probl.pdf;/home/xav/Zotero/storage/GRP5KLL5/2205.html}
}

@misc{yangRe3GeneratingLonger2022,
  title = {Re3: {{Generating Longer Stories With Recursive Reprompting}} and {{Revision}}},
  shorttitle = {Re3},
  author = {Yang, Kevin and Tian, Yuandong and Peng, Nanyun and Klein, Dan},
  year = {2022},
  month = oct,
  number = {arXiv:2210.06774},
  eprint = {arXiv:2210.06774},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.06774},
  urldate = {2023-02-17},
  abstract = {We consider the problem of automatically generating longer stories of over two thousand words. Compared to prior work on shorter stories, long-range plot coherence and relevance are more central challenges here. We propose the Recursive Reprompting and Revision framework (Re3) to address these challenges by (a) prompting a general-purpose language model to construct a structured overarching plan, and (b) generating story passages by repeatedly injecting contextual information from both the plan and current story state into a language model prompt. We then revise by (c) reranking different continuations for plot coherence and premise relevance, and finally (d) editing the best continuation for factual consistency. Compared to similar-length stories generated directly from the same base model, human evaluators judged substantially more of Re3's stories as having a coherent overarching plot (by 14\% absolute increase), and relevant to the given initial premise (by 20\%).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/8FSXLE3V/Yang et al. - 2022 - Re3 Generating Longer Stories With Recursive Repr.pdf;/home/xav/Zotero/storage/J4YAQPJN/2210.html}
}

@misc{yangSurveyKnowledgeEnhanced2022,
  title = {A {{Survey}} of {{Knowledge Enhanced Pre-trained Models}}},
  author = {Yang, Jian and Xiao, Gang and Shen, Yulong and Jiang, Wei and Hu, Xinyu and Zhang, Ying and Peng, Jinghui},
  year = {2022},
  month = may,
  number = {arXiv:2110.00269},
  eprint = {arXiv:2110.00269},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2110.00269},
  urldate = {2022-12-31},
  abstract = {Pre-trained models learn informative representations on large-scale training data through a self-supervised or supervised learning method, which has achieved promising performance in natural language processing (NLP), computer vision (CV), and cross-modal fields after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. Pre-trained models with knowledge injection, which we call knowledge enhanced pre-trained models (KEPTMs), possess deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPTMs in NLP and CV. We first introduce the progress of pre-trained models and knowledge representation learning. Then we systematically categorize existing KEPTMs from three different perspectives. Finally, we outline some potential directions of KEPTMs for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/6UE8UQH2/Yang et al. - 2022 - A Survey of Knowledge Enhanced Pre-trained Models.pdf}
}

@book{yanHeuristicTRIZProblem2012,
  title = {A {{Heuristic TRIZ Problem Solving Approach}} Based on {{Semantic Relatedness}} and {{Ontology Reasoning}}},
  author = {Yan, Wei and {Zanni-Merk}, Cecilia and Rousselot, Francois and Cavallucci, Denis and Collet, Pierre},
  year = {2012},
  month = sep,
  volume = {243},
  pages = {1572},
  doi = {10.3233/978-1-61499-105-2-1563},
  abstract = {The theory of inventive problem solving (TRIZ) was developed to solve inventive problems in different industrial fields. In recent decades, modern innovation theories and methods proposed several different knowledge sources, whose use requires extensive knowledge about different engineering domains. In order to facilitate the use of the TRIZ knowledge sources, this paper explores a heuristic TRIZ problem solving approach. Firstly, TRIZ users start solving inventive problem with the TRIZ knowledge source of their choice. Then other similar knowledge sources are used according to a calculation of semantic relatedness. Finally, heuristic solutions are returned by ontology reasoning on the knowledge sources. The case of a "Diving Fin" is used to show the heuristic TRIZ problem solving process in detail.},
  file = {/home/xav/Zotero/storage/Z7USGUXM/Yan et al. - 2012 - A Heuristic TRIZ Problem Solving Approach based on.pdf}
}

@article{yanIngeniousTRIZAutomaticOntologybased2015,
  title = {{{IngeniousTRIZ}}: {{An}} Automatic Ontology-Based System for Solving Inventive Problems},
  shorttitle = {{{IngeniousTRIZ}}},
  author = {Yan, W. and Liu, H. and {Zanni-Merk}, C. and Cavallucci, D.},
  year = {2015},
  journal = {Knowledge-Based Systems},
  volume = {75},
  pages = {52--65},
  issn = {0950-7051},
  urldate = {2022-05-15},
  abstract = {IngeniousTRIZ: An automatic ontology-based system for solving inventive problems},
  langid = {english},
  file = {/home/xav/Zotero/storage/JN4B4N7M/Yan et al. - 2015 - IngeniousTRIZ An automatic ontology-based system .pdf;/home/xav/Zotero/storage/NXF4Z7NZ/IngeniousTRIZ_An_automatic_ontology_based_system_for_solving_inventive_problems.html}
}

@article{yanMetaKnowledgeModeling,
  title = {({{Meta}}){{Knowledge Modeling}} for {{Inventive Design}}},
  author = {Yan, Wei},
  pages = {263},
  abstract = {n nombre croissant d'industries ressentent le besoin de formaliser leurs processus d'innovation. Dans ce contexte, les outils du domaine de la qualit\'e et les approches d'aide \`a la cr\'eativit\'e provenant du "brain storming" ont d\'ej\`a montr\'e leurs limites. Afin de r\'epondre \`a ces besoins, la TRIZ (Acronyme russe pour Th\'eorie de R\'esolution des Probl\`emes Inventifs), d\'evelopp\'ee par l'ing\'enieur russe G. S. Altshuller au milieu du 20\`eme si\`ecle, propose une m\'ethode syst\'ematique de r\'esolution de probl\`emes inventifs multidomaines. Selon TRIZ, la r\'esolution de probl\`emes inventifs consiste en la construction du mod\`ele et l'utilisation des sources de connaissance de la TRIZ. Plusieurs mod\`eles et sources de connaissances permettent la r\'esolution de probl\`emes inventifs de types diff\'erents, comme les quarante Principes Inventifs pour l'\'elimination des contradictions techniques. Toutes ces sources se situent \`a des niveaux d'abstractions relativement \'elev\'es et sont, donc, ind\'ependantes d'un domaine particulier, qui n\'ecessitent des connaissances approfondies des domaines d'ing\'enierie diff\'erents. Afin de faciliter le processus de r\'esolution de probl\`emes inventifs, un "Syst\`eme Intelligent de Gestion de Connaissances" est d\'evelopp\'e dans cette th\`ese. D'une part, en int\'egrant les ontologies des bases de connaissance de la TRIZ, le gestionnaire propose aux utilisateurs de sources de connaissance pertinentes pour le mod\`ele qu'ils construisent, et d'autre part, le gestionnaire a la capacit\'e de remplir "automatiquement" les mod\`eles associ\'es aux autres bases de connaissance. Ces travaux de recherche visent \`a faciliter et automatiser le processus de r\'esolution de probl\`emes inventifs. Ils sont bas\'es sur le calcul de similarit\'e s\'emantique et font usage de diff\'erentes technologies provenantes de domaine de l'Ing\'enierie de Connaissances (mod\'elisation et raisonnement bas\'es sur les ontologies, notamment). Tout d'abord, des m\'ethodes de calcul de similarit\'e s\'emantique sont propos\'ees pour rechercher et d\'efinir les liens manquants entre les bases de connaissance de la TRIZ. Ensuite, les sources de connaissance de la TRIZ sont formalis\'ees comme des ontologies afin de pouvoir utiliser des m\'ecanismes d'inf\'erence heuristique pour la recherche de solutions sp\'ecifiques. Pour r\'esoudre des probl\`emes inventifs, les utilisateurs de la TRIZ choisissent dans un premier temps une base de connaissance et obtiennent une solution abstraite. Ensuite, les \'el\'ements des autres bases de connaissance similaires aux \'el\'ements s\'electionn\'es dans la premi\`ere base sont propos\'es sur la base de la similarit\'e s\'emantique pr\'ealablement calcul\'ee. A l'aide de ces \'el\'ements et des effets physiques heuristiques, d'autres solutions conceptuelles sont obtenues par inf\'erence sur les ontologies. Enfin, un prototype logiciel est d\'evelopp\'e. Il est bas\'e sur cette similarit\'e s\'emantique et les ontologies interviennent en support du processus de g\'en\'eration automatique de solutions conceptuelles.},
  langid = {english},
  file = {/home/xav/Zotero/storage/7GUBBPBS/Yan - (Meta)Knowledge Modeling for Inventive Design.pdf}
}

@article{yanOntologybasedApproachInventive2014,
  title = {An Ontology-Based Approach for Inventive Problem Solving},
  author = {Yan, W. and {Zanni-Merk}, C. and Cavallucci, D. and Collet, P.},
  year = {2014},
  month = jan,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {27},
  pages = {175--190},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2013.07.005},
  urldate = {2022-05-04},
  abstract = {With the development of the theory of inventive problem solving (TRIZ), different knowledge sources were established in order to solve different types of inventive problems, such as 40 inventive principles for eliminating the technical contradictions. These knowledge sources with different levels of abstraction are all built independent of the specific application field, and require extensive knowledge about different engineering domains. In order to facilitate the use of the TRIZ knowledge sources, this paper explores a new inventive problem solving approach based on ontologies. In this approach, the TRIZ users start solving an inventive problem with the TRIZ knowledge source of their choice to obtain an abstract solution. According to the selected items of that first knowledge source, the similar items of other knowledge sources are obtained based on the semantic similarity calculated in advance. Considering that all the TRIZ knowledge sources are described as short-texts, the missing links among the TRIZ knowledge sources are defined based on short-text semantic similarity. At the same time, the ontology reasoning mechanism, deployed on Prot\'eg\'e and JESS, is used to provide heuristic solutions dynamically for TRIZ users. The case of a ``Space Boiler'' is used to show this ontology-based inventive problem solving process in detail.},
  langid = {english},
  keywords = {Knowledge source,Ontology,Ontology reasoning,Semantic similarity,The theory of inventive problem solving (TRIZ),WordNet},
  file = {/home/xav/Zotero/storage/4H9EWJT6/Yan et al. - 2014 - An ontology-based approach for inventive problem s.pdf;/home/xav/Zotero/storage/PHA28JRJ/S0952197613001334.html}
}

@article{yanSOLUTIONPROBLEMRuleBased2022,
  title = {{{SOLUTION}} \& {{PROBLEM}} ? {{A Rule-Based Heuristic Methodology}} for {{Su-Field Analysis}} in {{Industrial Engineering Design}}},
  author = {Yan, Wei and {Zanni-Merk}, Cecilia and Cavallucci, Denis and Cao, Qiushi and Zhang, Liang and Ji, Zengyan},
  year = {2022},
  month = mar,
  journal = {Information},
  volume = {13},
  number = {3},
  pages = {143},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2078-2489},
  doi = {10.3390/info13030143},
  urldate = {2022-05-15},
  abstract = {Industrial engineering design is a crucial issue in manufacturing. To meet the competitive global market, manufacturers are continuously seeking solutions to design industrial products and systems inventively. Su-Field analysis, which is one of the TRIZ analysis tools for inventive design problems, has been used to effectively improve the performance of industrial systems. However, the inventive standards used for engineering design are summarized and classified according to a large number of patents in different fields. They are built on a highly abstract basis and are independent of specific application fields, making their use require much more technical knowledge than other TRIZ tools. To facilitate the use of invention standards, in particular to capture the uncertainty or imprecision described in the standards, this paper proposes a rule-based heuristic approach. First, Su-Field analysis ontology and fuzzy analysis ontology are constructed to represent precise and fuzzy knowledge in the process of solving inventive problems respectively. Then, SWRL (Semantic Web Rule Language) reasoning and fuzzy reasoning are executed to generate heuristic conceptual solutions. Finally, we develop a software prototype and elaborate the resolution of ``Auguste Piccard's Stratostat '' in the prototype.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {fuzzy reasoning,heuristic Su-Field analysis,industrial engineering design,inventive standards,ontology reasoning,SWRL (Semantic Web Rule Language) reasoning},
  file = {/home/xav/Zotero/storage/XN6L4B94/Yan et al. - 2022 - A Rule-Based Heuristic Methodology for Su-Field An.pdf;/home/xav/Zotero/storage/5DIX6SUZ/143.html}
}

@article{yaoKGBERTBERTKnowledge2019,
  title = {{{KG-BERT}}: {{BERT}} for {{Knowledge Graph Completion}}},
  shorttitle = {{{KG-BERT}}},
  author = {Yao, Liang and Mao, Chengsheng and Luo, Yuan},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.03193 [cs]},
  eprint = {1909.03193},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Knowledge graphs are important resources for many artificial intelligence tasks but often suffer from incompleteness. In this work, we propose to use pre-trained language models for knowledge graph completion. We treat triples in knowledge graphs as textual sequences and propose a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples. Our method takes entity and relation descriptions of a triple as input and computes scoring function of the triple with the KG-BERT language model. Experimental results on multiple benchmark knowledge graphs show that our method can achieve state-of-the-art performance in triple classification, link prediction and relation prediction tasks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/CLUTK38I/Yao et al. - 2019 - KG-BERT BERT for Knowledge Graph Completion.pdf}
}

@misc{yaoReActSynergizingReasoning2022,
  title = {{{ReAct}}: {{Synergizing Reasoning}} and {{Acting}} in {{Language Models}}},
  shorttitle = {{{ReAct}}},
  author = {Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03629},
  eprint = {arXiv:2210.03629},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03629},
  urldate = {2022-10-12},
  abstract = {While large language models (LLMs) have demonstrated impressive capabilities across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with external sources, such as knowledge bases or environments, to gather additional information. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines, as well as improved human interpretability and trustworthiness over methods without reasoning or acting components. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes issues of hallucination and error propagation prevalent in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generates human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. On two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34\% and 10\% respectively, while being prompted with only one or two in-context examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/MMXU9WM6/Yao et al. - 2022 - ReAct Synergizing Reasoning and Acting in Languag.pdf}
}

@misc{yeLargeLanguageModels2023,
  title = {Large {{Language Models}} Are {{Versatile Decomposers}}: {{Decompose Evidence}} and {{Questions}} for {{Table-based Reasoning}}},
  shorttitle = {Large {{Language Models}} Are {{Versatile Decomposers}}},
  author = {Ye, Yunhu and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin},
  year = {2023},
  month = jan,
  number = {arXiv:2301.13808},
  eprint = {arXiv:2301.13808},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2301.13808},
  urldate = {2023-02-07},
  abstract = {Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and structured tabular data. However, previous table-based reasoning solutions usually suffer from significant performance degradation on huge evidence (tables). In addition, most existing methods struggle to reason over complex questions since the required information is scattered in different places. To alleviate the above challenges, we exploit large language models (LLMs) as decomposers for effective table-based reasoning, which (i) decompose huge evidence (a huge table) into sub-evidence (a small table) to mitigate the interference of useless information for table reasoning; and (ii) decompose complex questions into simpler sub-questions for text reasoning. Specifically, we first use the LLMs to break down the evidence (tables) involved in the current question, retaining the relevant evidence and excluding the remaining irrelevant evidence from the huge table. In addition, we propose a "parsing-execution-filling" strategy to alleviate the hallucination dilemma of the chain of thought by decoupling logic and numerical computation in each step. Extensive experiments show that our method can effectively leverage decomposed evidence and questions and outperforms the strong baselines on TabFact, WikiTableQuestion, and FetaQA datasets. Notably, our model outperforms human performance for the first time on the TabFact dataset.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/7JKXKWKP/Ye et al. - 2023 - Large Language Models are Versatile Decomposers D.pdf;/home/xav/Zotero/storage/WUQJHIRL/2301.html}
}

@article{yeSpendingThinkingTime2021,
  title = {Spending {{Thinking Time Wisely}}: {{Accelerating MCTS}} with {{Virtual Expansions}}},
  shorttitle = {Spending {{Thinking Time Wisely}}},
  author = {Ye, Weirui and Abbeel, Pieter and Gao, Yang},
  year = {2021},
  month = nov,
  urldate = {2022-11-27},
  abstract = {One of the most important AI research questions is to trade off computation versus performance, since "perfect rational" exists in theory but it is impossible to achieve in practice. Recently, Monte-Carlo tree search (MCTS) has attracted considerable attention due to the significant improvement of performance in varieties of challenging domains. However, the expensive time cost during search severely restricts its scope for applications. This paper proposes the Virtual MCTS (V-MCTS), a variant of MCTS that mimics the human behavior that spends adequate amounts of time to think about different questions. Inspired by this, we propose a strategy that converges to the ground truth MCTS search results with much less computation. We give theoretical bounds of the V-MCTS and evaluate the performance in \$9 \textbackslash times 9\$ Go board games and Atari games. Experiments show that our method can achieve similar performances as the original search algorithm while requiring less than \$50\textbackslash\%\$ number of search times on average. We believe that this approach is a viable alternative for tasks with limited time and resources.},
  langid = {english},
  file = {/home/xav/Zotero/storage/GF42E5C8/Ye et al. - 2021 - Spending Thinking Time Wisely Accelerating MCTS w.pdf}
}

@misc{yeUnreliabilityExplanationsFewshot2022,
  title = {The {{Unreliability}} of {{Explanations}} in {{Few-shot Prompting}} for {{Textual Reasoning}}},
  author = {Ye, Xi and Durrett, Greg},
  year = {2022},
  month = oct,
  number = {arXiv:2205.03401},
  eprint = {arXiv:2205.03401},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.03401},
  urldate = {2023-02-07},
  abstract = {Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/XDQQZMLQ/Ye et Durrett - 2022 - The Unreliability of Explanations in Few-shot Prom.pdf;/home/xav/Zotero/storage/C4EZ2Q5L/2205.html}
}

@misc{yongBLOOMAddingLanguage2022,
  title = {{{BLOOM}}+1: {{Adding Language Support}} to {{BLOOM}} for {{Zero-Shot Prompting}}},
  shorttitle = {{{BLOOM}}+1},
  author = {Yong, Zheng-Xin and Schoelkopf, Hailey and Muennighoff, Niklas and Aji, Alham Fikri and Adelani, David Ifeoluwa and Almubarak, Khalid and Bari, M. Saiful and Sutawika, Lintang and Kasai, Jungo and Baruwa, Ahmed and Winata, Genta Indra and Biderman, Stella and Radev, Dragomir and Nikoulina, Vassilina},
  year = {2022},
  month = dec,
  number = {arXiv:2212.09535},
  eprint = {arXiv:2212.09535},
  publisher = {{arXiv}},
  urldate = {2023-03-24},
  abstract = {The BLOOM model is a large open-source multilingual language model capable of zero-shot learning, but its pretraining was limited to 46 languages. To improve its zero-shot performance on unseen languages, it is desirable to adapt BLOOM, but previous works have only explored adapting small language models. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https: //github.com/bigscience-workshop/ multilingual-modeling/.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/GW3KFYEX/Yong et al. - 2022 - BLOOM+1 Adding Language Support to BLOOM for Zero.pdf}
}

@misc{yuAcTuneUncertaintyawareActive2022,
  title = {{{AcTune}}: {{Uncertainty-aware Active Self-Training}} for {{Semi-Supervised Active Learning}} with {{Pretrained Language Models}}},
  shorttitle = {{{AcTune}}},
  author = {Yu, Yue and Kong, Lingkai and Zhang, Jieyu and Zhang, Rongzhi and Zhang, Chao},
  year = {2022},
  month = may,
  number = {arXiv:2112.08787},
  eprint = {arXiv:2112.08787},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.08787},
  urldate = {2023-01-04},
  abstract = {While pre-trained language model (PLM) fine-tuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data. Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data. We propose \{\textbackslash ours\}, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning. AcTune switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and low-uncertainty ones for model self-training. Under this framework, we design (1) a region-aware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2\textbackslash\% on average. Our implementation will be available at \textbackslash url\{https://github.com/yueyu1030/actune\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/LTD58IZM/Yu et al. - 2022 - AcTune Uncertainty-aware Active Self-Training for.pdf}
}

@misc{yuALERTAdaptingLanguage2022,
  title = {{{ALERT}}: {{Adapting Language Models}} to {{Reasoning Tasks}}},
  shorttitle = {{{ALERT}}},
  author = {Yu, Ping and Wang, Tianlu and Golovneva, Olga and Alkhamissy, Badr and Ghosh, Gargi and Diab, Mona and Celikyilmaz, Asli},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08286},
  eprint = {arXiv:2212.08286},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.08286},
  urldate = {2022-12-25},
  abstract = {Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/FFB66TJF/Yu et al. - 2022 - ALERT Adapting Language Models to Reasoning Tasks.pdf}
}

@misc{yuanBARTScoreEvaluatingGenerated2021,
  title = {{{BARTScore}}: {{Evaluating Generated Text}} as {{Text Generation}}},
  shorttitle = {{{BARTScore}}},
  author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  year = {2021},
  month = oct,
  number = {arXiv:2106.11520},
  eprint = {arXiv:2106.11520},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.11520},
  urldate = {2023-02-16},
  abstract = {A wide variety of NLP applications, such as machine translation, summarization, and dialog, involve text generation. One major challenge for these applications is how to evaluate whether such generated texts are actually fluent, accurate, or effective. In this work, we conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models. The general idea is that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. We operationalize this idea using BART, an encoder-decoder based pre-trained model, and propose a metric BARTScore with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency, or factuality). BARTScore is conceptually simple and empirically effective. It can outperform existing top-scoring metrics in 16 of 22 test settings, covering evaluation of 16 datasets (e.g., machine translation, text summarization) and 7 different perspectives (e.g., informativeness, factuality). Code to calculate BARTScore is available at https://github.com/neulab/BARTScore, and we have released an interactive leaderboard for meta-evaluation at http://explainaboard.nlpedia.ai/leaderboard/task-meval/ on the ExplainaBoard platform, which allows us to interactively understand the strengths, weaknesses, and complementarity of each metric.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/WVPSYUKQ/Yuan et al. - 2021 - BARTScore Evaluating Generated Text as Text Gener.pdf;/home/xav/Zotero/storage/Q4LYA4RC/2106.html}
}

@misc{yuanCanWeAutomate2021,
  title = {Can {{We Automate Scientific Reviewing}}?},
  author = {Yuan, Weizhe and Liu, Pengfei and Neubig, Graham},
  year = {2021},
  month = jan,
  number = {arXiv:2102.00176},
  eprint = {arXiv:2102.00176},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.00176},
  urldate = {2022-10-05},
  abstract = {The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question "can we automate scientific reviewing?", discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate first-pass peer reviews for scientific papers. Arguably the most difficult part of this is defining what a "good" review is in the first place, so we first discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We finally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github.com/neulab/ReviewAdvisor, as well as a ReviewAdvisor system: http://review.nlpedia.ai/.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/HITX83ED/Yuan et al. - 2021 - Can We Automate Scientific Reviewing.pdf}
}

@misc{yuanFewshotQueryFocusedSummarization2022,
  title = {Few-Shot {{Query-Focused Summarization}} with {{Prefix-Merging}}},
  author = {Yuan, Ruifeng and Wang, Zili and Cao, Ziqiang and Li, Wenjie},
  year = {2022},
  month = nov,
  number = {arXiv:2211.16164},
  eprint = {arXiv:2211.16164},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.16164},
  urldate = {2023-02-02},
  abstract = {Query-focused summarization has been considered as an important extension for text summarization. It aims to generate a concise highlight for a given query. Different from text summarization, query-focused summarization has long been plagued by the problem of lacking high-quality large-scale datasets. In this paper, we investigate the idea that whether we can integrate and transfer the knowledge of text summarization and question answering to assist the few-shot learning in query-focused summarization. Here, we propose prefix-merging, a prefix-based pretraining strategy for few-shot learning in query-focused summarization. Drawn inspiration from prefix-tuning, we are allowed to integrate the task knowledge from text summarization and question answering into a properly designed prefix and apply the merged prefix to query-focused summarization. With only a small amount of trainable parameters, prefix-merging outperforms fine-tuning on query-focused summarization. We further discuss the influence of different prefix designs and propose a visualized explanation for how prefix-merging works.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BIBXVUQQ/Yuan et al. - 2022 - Few-shot Query-Focused Summarization with Prefix-M.pdf;/home/xav/Zotero/storage/3QBNWZH3/2211.html}
}

@article{yuanImprovingNeuralQuestion2021,
  title = {Improving {{Neural Question Generation}} Using {{Deep Linguistic Representation}}},
  author = {Yuan, Wei and He, Tieke and Dai, Xinyu},
  year = {2021},
  journal = {WWW},
  doi = {10.1145/3442381.3449975},
  abstract = {The experimental results demonstrate that the proposed approach outperforms the state-of-the-art QG systems, and significantly improves the baseline by 17.2\% and 6. Question Generation (QG) is a challenging Natural Language Processing (NLP) task which aims at generating questions with given answers and context. There are many works incorporating linguistic features to improve the performance of QG. However, similar to traditional word embedding, these works normally embed such features with a set of trainable parameters, which results in the linguistic features not fully exploited. In this work, inspired by the recent achievements of text representation, we propose to utilize linguistic information via large pre-trained neural models. First, these models are trained in several specific NLP tasks in order to better represent linguistic features. Then, such feature representation is fused into a seq2seq based QG model to guide question generation. Extensive experiments were conducted on two benchmark Question Generation datasets to evaluate the effectiveness of our approach. The experimental results demonstrate that our approach outperforms the state-of-the-art QG systems, as a result, it significantly improves the baseline by 17.2\% and 6.2\% under the BLEU-4 metric on these two datasets, respectively.}
}

@article{yuanReStructuredPretraining2022,
  title = {{{reStructured Pre-training}}},
  author = {Yuan, Weizhe and Liu, Pengfei},
  year = {2022},
  pages = {111},
  abstract = {In this work, we try to decipher the internal connection of NLP technology development in the past decades, searching for essence, which rewards us with a (potential) new learning paradigm for NLP tasks, dubbed as reStructured Pre-training (RST - Signals :)) ). In such a paradigm, the role of data will be re-emphasized, and model pre-training and fine-tuning of downstream tasks are viewed as a process of data storing and accessing. Based on that, we operationalize the simple principle that a good storage mechanism should not only have the ability to cache a large amount of data but also consider the ease of access. We achieve this by pre-training models over restructured data that consist of a variety of valuable information instead of raw data after overcoming several engineering challenges. Experimentally, RST models not only surpass strong competitors (e.g., T0) on 52/55 popular datasets from a variety of NLP tasks (e.g., classification, information extraction, fact retrieval, text generation, etc.) without fine-tuning on downstream tasks, but also achieve superior performance in National College Entrance Examination - English (Gaokao-English), the most authoritative examination in China, which millions of students will attend every year. Specifically, the proposed system Qin ( ) achieves 40 points higher than the average scores made by students and 15 points higher than GPT3 with 1/16 parameters. In particular, Qin gets a high score of 138.5 (the full mark is 150) in the 2018 English exam (national paper III). We have released the Gaokao Benchmark with an online submission platform that contains ten annotated English papers from 2018-2021 so far (and will be expanded annually), which allows more AI models to attend Gaokao, establishing a relatively fair test bed for human and AI competition and helping us better understand where we are.},
  langid = {english},
  file = {/home/xav/Zotero/storage/CHQASUS4/Yuan et Liu - reStructured Pre-training.pdf}
}

@misc{yuanRoadmapBigModel2022,
  title = {A {{Roadmap}} for {{Big Model}}},
  author = {Yuan, Sha and Zhao, Hanyu and Zhao, Shuai and Leng, Jiahong and Liang, Yangxiao and Wang, Xiaozhi and Yu, Jifan and Lv, Xin and Shao, Zhou and He, Jiaao and Lin, Yankai and Han, Xu and Liu, Zhenghao and Ding, Ning and Rao, Yongming and Gao, Yizhao and Zhang, Liang and Ding, Ming and Fang, Cong and Wang, Yisen and Long, Mingsheng and Zhang, Jing and Dong, Yinpeng and Pang, Tianyu and Cui, Peng and Huang, Lingxiao and Liang, Zheng and Shen, Huawei and Zhang, Hui and Zhang, Quanshi and Dong, Qingxiu and Tan, Zhixing and Wang, Mingxuan and Wang, Shuo and Zhou, Long and Li, Haoran and Bao, Junwei and Pan, Yingwei and Zhang, Weinan and Yu, Zhou and Yan, Rui and Shi, Chence and Xu, Minghao and Zhang, Zuobai and Wang, Guoqiang and Pan, Xiang and Li, Mengjie and Chu, Xiaoyu and Yao, Zijun and Zhu, Fangwei and Cao, Shulin and Xue, Weicheng and Ma, Zixuan and Zhang, Zhengyan and Hu, Shengding and Qin, Yujia and Xiao, Chaojun and Zeng, Zheni and Cui, Ganqu and Chen, Weize and Zhao, Weilin and Yao, Yuan and Li, Peng and Zheng, Wenzhao and Zhao, Wenliang and Wang, Ziyi and Zhang, Borui and Fei, Nanyi and Hu, Anwen and Ling, Zenan and Li, Haoyang and Cao, Boxi and Han, Xianpei and Zhan, Weidong and Chang, Baobao and Sun, Hao and Deng, Jiawen and Zheng, Chujie and Li, Juanzi and Hou, Lei and Cao, Xigang and Zhai, Jidong and Liu, Zhiyuan and Sun, Maosong and Lu, Jiwen and Lu, Zhiwu and Jin, Qin and Song, Ruihua and Wen, Ji-Rong and Lin, Zhouchen and Wang, Liwei and Su, Hang and Zhu, Jun and Sui, Zhifang and Zhang, Jiajun and Liu, Yang and He, Xiaodong and Huang, Minlie and Tang, Jian and Tang, Jie},
  year = {2022},
  month = apr,
  number = {arXiv:2203.14101},
  eprint = {arXiv:2203.14101},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.14101},
  urldate = {2023-02-02},
  abstract = {With the rapid development of deep learning, training Big Models (BMs) for multiple downstream tasks becomes a popular paradigm. Researchers have achieved various outcomes in the construction of BMs and the BM application in many fields. At present, there is a lack of research work that sorts out the overall progress of BMs and guides the follow-up research. In this paper, we cover not only the BM technologies themselves but also the prerequisites for BM training and applications with BMs, dividing the BM review into four parts: Resource, Models, Key Technologies and Application. We introduce 16 specific BM-related topics in those four parts, they are Data, Knowledge, Computing System, Parallel Training System, Language Model, Vision Model, Multi-modal Model, Theory\&Interpretability, Commonsense Reasoning, Reliability\&Security, Governance, Evaluation, Machine Translation, Text Generation, Dialogue and Protein Research. In each topic, we summarize clearly the current studies and propose some future research directions. At the end of this paper, we conclude the further development of BMs in a more general view.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/URKX7AGP/Yuan et al. - 2022 - A Roadmap for Big Model.pdf;/home/xav/Zotero/storage/2KMPTCT9/2203.html}
}

@article{yuanUnifiedQuestionGeneration2022,
  title = {Unified {{Question Generation}} with {{Continual Lifelong Learning}}},
  author = {Yuan, Wei and Yin, Hongzhi and He, Tieke and Chen, Tong and Wang, Qiufeng and Cui, Lizhen},
  year = {2022},
  month = apr,
  journal = {Proceedings of the ACM Web Conference 2022},
  eprint = {2201.09696},
  pages = {871--881},
  doi = {10.1145/3485447.3511930},
  urldate = {2022-05-03},
  abstract = {Question Generation (QG), as a challenging Natural Language Processing task, aims at generating questions based on given answers and context. Existing QG methods mainly focus on building or training models for specific QG datasets. These works are subject to two major limitations: (1) They are dedicated to specific QG formats (e.g., answer-extraction or multi-choice QG), therefore, if we want to address a new format of QG, a re-design of the QG model is required. (2) Optimal performance is only achieved on the dataset they were just trained on. As a result, we have to train and keep various QG models for different QG datasets, which is resource-intensive and ungeneralizable. To solve the problems, we propose a model named Unified-QG based on lifelong learning techniques, which can continually learn QG tasks across different datasets and formats. Specifically, we first build a format-convert encoding to transform different kinds of QG formats into a unified representation. Then, a method named \textbackslash emph\{STRIDER\} (\textbackslash emph\{S\}imilari\textbackslash emph\{T\}y \textbackslash emph\{R\}egular\textbackslash emph\{I\}zed \textbackslash emph\{D\}ifficult \textbackslash emph\{E\}xample \textbackslash emph\{R\}eplay) is built to alleviate catastrophic forgetting in continual QG learning. Extensive experiments were conducted on \$8\$ QG datasets across \$4\$ QG formats (answer-extraction, answer-abstraction, multi-choice, and boolean QG) to demonstrate the effectiveness of our approach. Experimental results demonstrate that our Unified-QG can effectively and continually adapt to QG tasks when datasets and formats vary. In addition, we verify the ability of a single trained Unified-QG model in improving \$8\$ Question Answering (QA) systems' performance through generating synthetic QA data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/VANEVYP2/Yuan et al. - 2022 - Unified Question Generation with Continual Lifelon.pdf;/home/xav/Zotero/storage/NW9LB3V8/2201.html}
}

@article{yuCOCODRCombatingDistribution2022,
  title = {{{COCO-DR}}: {{Combating Distribution Shifts}} in {{Zero-Shot Dense Retrieval}} with {{Contrastive}} and {{Distributionally Robust Learning}}},
  author = {Yu, Yue and Xiong, Chenyan and Sun, Si and Zhang, Chao and Overwijk, Arnold},
  year = {2022},
  abstract = {We present a new zero-shot dense retrieval (ZeroDR) method, COCO-DR, to improve the generalization ability of dense retrieval by combating the distribution shifts between source training tasks and target scenarios. To mitigate the impact of document differences, COCODR continues pretraining the language model on the target corpora to adapt the model to target distributions via COtinuous COtrastive learning. To prepare for unseen target queries, COCO-DR leverages implicit Distributionally Robust Optimization (iDRO) to reweight samples from different source query clusters for improving model robustness over rare queries during fine-tuning. COCO-DR achieves superior average performance on BEIR, the zero-shot retrieval benchmark. At BERTBase scale, COCODRBase outperforms other ZeroDR models with 60\texttimes{} larger size. At BERTLarge scale, COCODRLarge outperforms the giant GPT-3 embedding model which has 500\texttimes{} more parameters. Our analysis show the correlation of COCO-DR's effectiveness in combating distribution shifts and improving zero-shot accuracy. Our code and model can be found at https://github.com/OpenMatch/COCO-DR.},
  langid = {english},
  file = {/home/xav/Zotero/storage/59WXX896/Yu et al. - COCO-DR Combating Distribution Shifts in Zero-Sho.pdf}
}

@article{yuKnowledgeInfusedPretrainedModels2020,
  title = {Knowledge-{{Infused Pre-trained Models}} for {{KG Completion}}},
  author = {Yu, Han and Jiang, Rong and Zhou, Bin and Li, Aiping},
  year = {2020},
  journal = {undefined},
  urldate = {2022-05-04},
  abstract = {This paper introduces a novel method for KG completion task by knowledge-infused pre-trained language models which can capture both linguistic information and factual knowledge to compute the plausible of the triples. Knowledge graphs (KG) are the basis for many artificial intelligence applications but still suffer from incompleteness. In this paper, we introduce a novel method for KG completion task by knowledge-infused pre-trained language models. We represent each triple in the KG as textual sequences and transform the KG completion task into a sentence classification task that fits the input of the language model. Our KG completion framework based on the knowledge-infused pre-trained language model which can capture both linguistic information and factual knowledge to compute the plausible of the triples. Experiments show that our method achieves better results than previous state-of-the-art on multiple benchmark datasets.},
  langid = {english},
  file = {/home/xav/Zotero/storage/YWG4NDTE/f684e44c7c8daeb0b2311635d710a399433980ad.html}
}

@misc{yuRetrievalAugmentationCommonsense2022,
  title = {Retrieval {{Augmentation}} for {{Commonsense Reasoning}}: {{A Unified Approach}}},
  shorttitle = {Retrieval {{Augmentation}} for {{Commonsense Reasoning}}},
  author = {Yu, W. and Zhu, Chenguang and Zhang, Zhihan and Wang, Shuohang and Zhang, Zhuosheng and Fang, Yuwei and Jiang, Meng},
  year = {2022},
  urldate = {2022-10-26},
  abstract = {A common thread of retrieval-augmented methods in the existing literature focuses on retrieving encyclopedic knowledge, such as Wikipedia, which facilitates well-defined entity and relation spaces that can be modeled. However, applying such methods to commonsense reasoning tasks faces two unique challenges, i.e., the lack of a general large-scale corpus for retrieval and a corresponding effective commonsense retriever. In this pa-per, we systematically investigate how to leverage commonsense knowledge retrieval to improve commonsense reasoning tasks. We proposed a unified framework of R etrieval-A ugmented Co mmonsense reasoning (called RAC O ), including a newly constructed commonsense corpus with over 20 million documents and novel strategies for training a commonsense retriever. We conducted experiments on four different commonsense reasoning tasks. Extensive evaluation results showed that our proposed RAC O can signif-icantly outperform other knowledge-enhanced method counterparts, achieving new SoTA performance on the CommonGen 1 and CREAK 2 leaderboards. Our code is available at https: //github.com/wyu97/RACo .},
  howpublished = {https://www.semanticscholar.org/paper/Retrieval-Augmentation-for-Commonsense-Reasoning\%3A-A-Yu-Zhu/d4f77cdb04d7ae02860415877cc4c463e93595a1},
  langid = {english},
  file = {/home/xav/Zotero/storage/GKJSCED3/Yu et al. - 2022 - Retrieval Augmentation for Commonsense Reasoning .pdf}
}

@misc{zadehPRUFMeaningRepresentation1977,
  title = {{{PRUF}} a Meaning Representation Language for Natural Languages},
  author = {ZADEH},
  year = {1977},
  month = oct,
  file = {/home/xav/Zotero/storage/4A66U87F/ark__67375_6H6-GL4Z9V4W-Q.pdf}
}

@misc{zaheerBigBirdTransformers2021,
  title = {Big {{Bird}}: {{Transformers}} for {{Longer Sequences}}},
  shorttitle = {Big {{Bird}}},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  year = {2021},
  month = jan,
  number = {arXiv:2007.14062},
  eprint = {arXiv:2007.14062},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2007.14062},
  urldate = {2023-02-10},
  abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/xav/Zotero/storage/LIR3VQHV/Zaheer et al. - 2021 - Big Bird Transformers for Longer Sequences.pdf;/home/xav/Zotero/storage/7558KG6Y/2007.html}
}

@misc{zaheerTeacherGuidedTraining2022,
  title = {Teacher {{Guided Training}}: {{An Efficient Framework}} for {{Knowledge Transfer}}},
  shorttitle = {Teacher {{Guided Training}}},
  author = {Zaheer, Manzil and Rawat, Ankit Singh and Kim, Seungyeon and You, Chong and Jain, Himanshu and Veit, Andreas and Fergus, Rob and Kumar, Sanjiv},
  year = {2022},
  month = aug,
  number = {arXiv:2208.06825},
  eprint = {arXiv:2208.06825},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.06825},
  urldate = {2023-02-09},
  abstract = {The remarkable performance gains realized by large pretrained models, e.g., GPT-3, hinge on the massive amounts of data they are exposed to during training. Analogously, distilling such large models to compact models for efficient deployment also necessitates a large amount of (labeled or unlabeled) training data. In this paper, we propose the teacher-guided training (TGT) framework for training a high-quality compact model that leverages the knowledge acquired by pretrained generative models, while obviating the need to go through a large volume of data. TGT exploits the fact that the teacher has acquired a good representation of the underlying data domain, which typically corresponds to a much lower dimensional manifold than the input space. Furthermore, we can use the teacher to explore input space more efficiently through sampling or gradient-based methods; thus, making TGT especially attractive for limited data or long-tail settings. We formally capture this benefit of proposed data-domain exploration in our generalization bounds. We find that TGT can improve accuracy on several image classification benchmarks as well as a range of text classification and retrieval tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/TFA688WR/Zaheer et al. - 2022 - Teacher Guided Training An Efficient Framework fo.pdf;/home/xav/Zotero/storage/FTGCTXX7/2208.html}
}

@article{zaibConversationalQuestionAnswering2022,
  title = {Conversational Question Answering: A Survey},
  shorttitle = {Conversational Question Answering},
  author = {Zaib, Munazza and Zhang, Wei Emma and Sheng, Quan Z. and Mahmood, Adnan and Zhang, Yang},
  year = {2022},
  month = dec,
  journal = {Knowledge and Information Systems},
  volume = {64},
  number = {12},
  pages = {3151--3195},
  issn = {0219-3116},
  doi = {10.1007/s10115-022-01744-y},
  urldate = {2023-01-13},
  abstract = {Question answering (QA) systems provide a way of querying the information available in various formats including, but not limited to, unstructured and structured data in natural languages. It constitutes a considerable part of conversational artificial intelligence (AI) which has led to the introduction of a special research topic on conversational question answering (CQA), wherein a system is required to understand the given context and then engages in multi-turn QA to satisfy a user's information needs. While the focus of most of the existing research work is subjected to single-turn QA, the field of multi-turn QA has recently grasped attention and prominence owing to the availability of large-scale, multi-turn QA datasets and the development of pre-trained language models. With a good amount of models and research papers adding to the literature every year recently, there is a dire need of arranging and presenting the related work in a unified manner to streamline future research. This survey is an effort to present a comprehensive review of the state-of-the-art research trends of CQA primarily based on reviewed papers over the recent years. Our findings show that there has been a trend shift from single-turn to multi-turn QA which empowers the field of Conversational AI from different perspectives. This survey is intended to provide an epitome for the research community with the hope of laying a strong foundation for the field of CQA.},
  langid = {english},
  keywords = {Conversational agents,Conversational AI,Conversational machine reading comprehension,Knowledge base,Question answering},
  file = {/home/xav/Zotero/storage/YV53K6VS/Zaib et al. - 2022 - Conversational question answering a survey.pdf}
}

@misc{zakariVQAVisualReasoning2022,
  title = {{{VQA}} and {{Visual Reasoning}}: {{An Overview}} of {{Recent Datasets}}, {{Methods}} and {{Challenges}}},
  shorttitle = {{{VQA}} and {{Visual Reasoning}}},
  author = {Zakari, Rufai Yusuf and Owusu, Jim Wilson and Wang, Hailin and Qin, Ke and Lawal, Zaharaddeen Karami and Dong, Yuezhou},
  year = {2022},
  month = dec,
  number = {arXiv:2212.13296},
  eprint = {arXiv:2212.13296},
  publisher = {{arXiv}},
  urldate = {2023-01-24},
  abstract = {Artificial Intelligence (AI) and its applications have sparked extraordinary interest in recent years. This achievement can be ascribed in part to advances in AI subfields including Machine Learning (ML), Computer Vision (CV), and Natural Language Processing (NLP). Deep learning, a sub-field of machine learning that employs artificial neural network concepts, has enabled the most rapid growth in these domains. The integration of vision and language has sparked a lot of attention as a result of this. The tasks have been created in such a way that they properly exemplify the concepts of deep learning. In this review paper, we provide a thorough and an extensive review of the state of the arts approaches, key models design principles and discuss existing datasets, methods, their problem formulation and evaluation measures for VQA and Visual reasoning tasks to understand vision and language representation learning. We also present some potential future paths in this field of research, with the hope that our study may generate new ideas and novel approaches to handle existing difficulties and develop new applications.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/NQHI7VUK/Zakari et al. - 2022 - VQA and Visual Reasoning An Overview of Recent Da.pdf}
}

@article{zanni-merkFormalOntologyGeneralized2013,
  title = {A Formal Ontology for a Generalized Inventive Design Methodology},
  author = {{Zanni-Merk}, Cecilia and {de Bertrand de Beuvron}, Fran{\c c}ois and Rousselot, Fran{\c c}ois and Yan, Wei},
  year = {2013},
  month = jan,
  journal = {Applied Ontology},
  volume = {8},
  number = {4},
  pages = {231--273},
  publisher = {{IOS Press}},
  issn = {1570-5838},
  doi = {10.3233/AO-140128},
  urldate = {2022-05-11},
  abstract = {TRIZ (the Russian acronym for Theory of Resolution of Inventive Problems) is a methodology to guide the search for inventive solutions to one, or a few, difficult problems. Classic TRIZ is not well suited to the examination of complex situations composed of many problems, sub-problems and partial solutions, strongly interconnected. It has therefore been completed to give birth, among others, to the Inventive Design Methodology (IDM) framework. TRIZ and IDM share many similarities with Artificial Intelligence methods: they both propose to solve a problem by reformulation in an abstract model. Generic solving patterns are applied to this abstract model to produce abstract solutions. The domain-specific knowledge is then used to get the final solution concept. However, neither TRIZ nor IDM's descriptions are formal enough to permit a reliable software implementation and rely mainly on the experts' manual work. Therefore this paper proposes an ontological formalization of TRIZ and IDM to overcome these difficulties and allow the development of software tools to assist TRIZ/IDM experts in their w},
  langid = {english},
  file = {/home/xav/Zotero/storage/KGMBWKV9/ao128.html}
}

@misc{zelikmanSTaRBootstrappingReasoning2022,
  title = {{{STaR}}: {{Bootstrapping Reasoning With Reasoning}}},
  shorttitle = {{{STaR}}},
  author = {Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah D.},
  year = {2022},
  month = may,
  number = {arXiv:2203.14465},
  eprint = {2203.14465},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-30},
  abstract = {Generating step-by-step "chain-of-thought" rationales improves language model performance on complex reasoning tasks like mathematics or commonsense question-answering. However, inducing language model rationale generation currently requires either constructing massive rationale datasets or sacrificing accuracy by using only few-shot inference. We propose a technique to iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform successively more complex reasoning. This technique, the "Self-Taught Reasoner" (STaR), relies on a simple loop: generate rationales to answer many questions, prompted with a few rationale examples; if the generated answers are wrong, try again to generate a rationale given the correct answer; fine-tune on all the rationales that ultimately yielded correct answers; repeat. We show that STaR significantly improves performance on multiple datasets compared to a model fine-tuned to directly predict final answers, and performs comparably to fine-tuning a 30\$\textbackslash times\$ larger state-of-the-art language model on CommensenseQA. Thus, STaR lets a model improve itself by learning from its own generated reasoning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/3IVR2364/Zelikman et al. - 2022 - STaR Bootstrapping Reasoning With Reasoning.pdf}
}

@misc{zengGLM130BOpenBilingual2022,
  title = {{{GLM-130B}}: {{An Open Bilingual Pre-trained Model}}},
  shorttitle = {{{GLM-130B}}},
  author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02414},
  eprint = {arXiv:2210.02414},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.02414},
  urldate = {2023-02-16},
  abstract = {We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4\$\textbackslash times\$RTX 3090 (24G) or 8\$\textbackslash times\$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B .},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/XELSWLW2/Zeng et al. - 2022 - GLM-130B An Open Bilingual Pre-trained Model.pdf;/home/xav/Zotero/storage/E66C4C56/2210.html}
}

@article{zengSocraticModelsComposing2022,
  title = {Socratic {{Models}}: {{Composing Zero-Shot Multimodal Reasoning}} with {{Language}}},
  shorttitle = {Socratic {{Models}}},
  author = {Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.00598 [cs]},
  eprint = {2204.00598},
  primaryclass = {cs},
  urldate = {2022-04-23},
  abstract = {Large foundation models can exhibit unique capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g. from spreadsheets, to SAT questions). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this model diversity is symbiotic, and can be leveraged to build AI systems with structured Socratic dialogue -- in which new multimodal tasks are formulated as a guided language-based exchange between different pre-existing foundation models, without additional finetuning. In the context of egocentric perception, we present a case study of Socratic Models (SMs) that can provide meaningful results for complex tasks such as generating free-form answers to contextual questions about egocentric video, by formulating video Q\&A as short story Q\&A, i.e. summarizing the video into a short story, then answering questions about it. Additionally, SMs can generate captions for Internet images, and are competitive with state-of-the-art on zero-shot video-to-text retrieval with 42.8 R@1 on MSR-VTT 1k-A. SMs demonstrate how to compose foundation models zero-shot to capture new multimodal functionalities, without domain-specific data collection. Prototypes are available at socraticmodels.github.io.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/ZDWZBP4R/Zeng et al. - 2022 - Socratic Models Composing Zero-Shot Multimodal Re.pdf;/home/xav/Zotero/storage/BLNN4H74/2204.html}
}

@misc{zengSocraticModelsComposing2022a,
  title = {Socratic {{Models}}: {{Composing Zero-Shot Multimodal Reasoning}} with {{Language}}},
  shorttitle = {Socratic {{Models}}},
  author = {Zeng, Andy and Attarian, Maria and Ichter, Brian and Choromanski, Krzysztof and Wong, Adrian and Welker, Stefan and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and Florence, Pete},
  year = {2022},
  month = may,
  number = {arXiv:2204.00598},
  eprint = {arXiv:2204.00598},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.00598},
  urldate = {2023-02-07},
  abstract = {Large pretrained (e.g., "foundation") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/TNWZB642/Zeng et al. - 2022 - Socratic Models Composing Zero-Shot Multimodal Re.pdf;/home/xav/Zotero/storage/AQK7GV9P/2204.html}
}

@article{zhangARLAdaptiveReinforcement2022,
  title = {{{ARL}}: {{An}} Adaptive Reinforcement Learning Framework for Complex Question Answering over Knowledge Base},
  shorttitle = {{{ARL}}},
  author = {Zhang, Qixuan and Weng, Xinyi and Zhou, Guangyou and Zhang, Yi and Huang, Jimmy Xiangji},
  year = {2022},
  month = may,
  journal = {Information Processing \& Management},
  volume = {59},
  number = {3},
  pages = {102933},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2022.102933},
  urldate = {2023-02-28},
  abstract = {Recently, reinforcement learning (RL)-based methods have achieved remarkable progress in both effectiveness and interpretability for complex question answering over knowledge base (KBQA). However, existing RL-based methods share a common limitation: the agent is usually misled by aimless exploration, as well as sparse and delayed rewards, leading to a large number of spurious relation paths. To address this issue, a new adaptive reinforcement learning (ARL) framework is proposed to learn a better and interpretable model for complex KBQA. First, instead of using a random walk agent, an adaptive path generator is developed with three atomic operations to sequentially generate the relation paths until the agent reaches the target entity. Second, a semantic policy network is presented with both character-level and sentence-level information to better guide the agent. Finally, a new reward function is introduced by considering both the relation paths and the target entity to alleviate sparse and delayed rewards. The empirical results on five benchmark datasets show that our model is more effective than state-of-the-art approaches. Compared with the strong baseline model SRN, the proposed model achieves performance improvements of 23.7\% on MetaQA-3 using the metric Hits@1.},
  langid = {english},
  keywords = {Knowledge base,Question answering,Reinforcement learning,Text mining},
  file = {/home/xav/Zotero/storage/EB4CMW6U/S0306457322000565.html}
}

@misc{zhangAutomaticChainThought2022,
  title = {Automatic {{Chain}} of {{Thought Prompting}} in {{Large Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  year = {2022},
  month = oct,
  number = {arXiv:2210.03493},
  eprint = {arXiv:2210.03493},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.03493},
  urldate = {2023-01-03},
  abstract = {Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like "Let's think step by step" to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the "Let's think step by step" prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/ZMSWQBRM/Zhang et al. - 2022 - Automatic Chain of Thought Prompting in Large Lang.pdf;/home/xav/Zotero/storage/CK9TTQZV/2210.html}
}

@misc{zhangAutomaticEvaluationModeration2021,
  title = {Automatic {{Evaluation}} and {{Moderation}} of {{Open-domain Dialogue Systems}}},
  author = {Zhang, Chen and Sedoc, Jo{\~a}o and D'Haro, Luis Fernando and Banchs, Rafael and Rudnicky, Alexander},
  year = {2021},
  month = dec,
  number = {arXiv:2111.02110},
  eprint = {arXiv:2111.02110},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2111.02110},
  urldate = {2023-02-09},
  abstract = {The development of Open-Domain Dialogue Systems (ODS)is a trending topic due to the large number of research challenges, large societal and business impact, and advances in the underlying technology. However, the development of these kinds of systems requires two important characteristics:1) automatic evaluation mechanisms that show high correlations with human judgements across multiple dialogue evaluation aspects (with explainable features for providing constructive and explicit feedback on the quality of generative models' responses for quick development and deployment)and 2) mechanisms that can help to control chatbot responses,while avoiding toxicity and employing intelligent ways to handle toxic user comments and keeping interaction flow and engagement. This track at the 10th Dialogue System Technology Challenge (DSTC10) is part of the ongoing effort to promote scalable and toxic-free ODS. This paper describes the datasets and baselines provided to participants, as well as submission evaluation results for each of the two proposed subtasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/home/xav/Zotero/storage/DXQ6T32W/Zhang et al. - 2021 - Automatic Evaluation and Moderation of Open-domain.pdf;/home/xav/Zotero/storage/7FL9UL2G/2111.html}
}

@misc{zhangBERTScoreEvaluatingText2020,
  title = {{{BERTScore}}: {{Evaluating Text Generation}} with {{BERT}}},
  shorttitle = {{{BERTScore}}},
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  year = {2020},
  month = feb,
  number = {arXiv:1904.09675},
  eprint = {arXiv:1904.09675},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.09675},
  urldate = {2023-02-16},
  abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/KQGP2RET/Zhang et al. - 2020 - BERTScore Evaluating Text Generation with BERT.pdf;/home/xav/Zotero/storage/WWSMMZ4N/1904.html}
}

@article{zhangDifferentiablePromptMakes2022,
  title = {Differentiable {{Prompt Makes Pre-trained Language Models Better Few-shot Learners}}},
  author = {Zhang, Ningyu and Li, Luoqiu and Chen, Xiang and Deng, Shumin and Bi, Zhen and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
  year = {2022},
  month = feb,
  journal = {arXiv:2108.13161 [cs]},
  eprint = {2108.13161},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Large-scale pre-trained language models have contributed significantly to natural language processing by demonstrating remarkable abilities as few-shot learners. However, their effectiveness depends mainly on scaling the model parameters and prompt design, hindering their implementation in most real-world applications. This study proposes a novel pluggable, extensible, and efficient approach named DifferentiAble pRompT (DART), which can convert small language models into better few-shot learners. The main principle behind this approach involves reformulating potential natural language processing tasks into the task of a pre-trained language model and differentially optimizing the prompt template as well as the target label with backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any pre-trained language models; (ii) Extended to widespread classification tasks. A comprehensive evaluation of standard NLP tasks demonstrates that the proposed approach achieves a better few-shot performance1.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/2INBFKEQ/Zhang et al. - 2022 - Differentiable Prompt Makes Pre-trained Language M.pdf}
}

@inproceedings{zhangFacilitatingEngineersAbilities2018,
  title = {Facilitating {{Engineers Abilities}} to {{Solve Inventive Problems Using CBR}} and {{Semantic Similarity}}},
  booktitle = {{{TFC}}},
  author = {Zhang, Pei and Cavallucci, D. and Bai, Zhonghang and {Zanni-Merk}, C.},
  year = {2018},
  doi = {10.1007/978-3-030-02456-7_17},
  abstract = {It is postulate that a multidisciplinary case base sufficiently populated of multidis disciplinary problem-solution couples is likely to considerably improve the performance of R\&D engineers to solve inventive problems and to enhance the inventiveness of solution. Our industry currently undergoes a period of important changes. The era of computerization implies to companies to change not only through their organization, but also in automating as much as possible their internal processes. Our research focuses on the computerization of the problem-solution couple when facing inventive situations in R\&D. The method used is based on Case-Based Reasoning (CBR) that has already been proven to be useful in routine design. On the other hand, CBR is hardly used in inventive situations because the latter require reasoning outside the circle of knowledge recorded in a database. Our proposal consists in coupling CBR with semantic similarity algorithms. The aim is to resolve a new problem based on its semantic similarity with the old problems. Then the old solution can be adapted to solve the new problem. We postulate that a multidisciplinary case base sufficiently populated of multidisciplinary problem-solution couples is likely to considerably improve the performance of R\&D engineers to solve inventive problems. This being possible by bringing them alternative solutions based on the semantically similar problems, which are distant from their field of origin. In this way, we provide the possibility to enhance the inventiveness of solution. This type of reasoning, largely inspired by the TRIZ theory, is the subject of this paper. The methodology, the experiments and the conclusions that we develop here validate that this type of approach produces the claimed effects on designers although limited to the context where it has been conducted.},
  file = {/home/xav/Zotero/storage/PWI8PXH7/Zhang et al. - 2018 - Facilitating Engineers Abilities to Solve Inventiv.pdf;/home/xav/Zotero/storage/JGNFYSL5/hal-02279761.html}
}

@article{zhangFactTreeReasoningNary2022,
  title = {Fact-{{Tree Reasoning}} for {{N-ary Question Answering}} over {{Knowledge Graphs}}},
  author = {Zhang, Yao and Li, Peiyao and Liang, Hongru and Jatowt, Adam and Yang, Zhenglu},
  year = {2022},
  month = mar,
  journal = {arXiv:2108.08297 [cs]},
  eprint = {2108.08297},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Current Question Answering over Knowledge Graphs (KGQA) task mainly focuses on performing answer reasoning upon KGs with binary facts. However, it neglects the n-ary facts, which contain more than two entities. In this work, we highlight a more challenging but under-explored task: n-ary KGQA, i.e., answering n-ary facts questions upon n-ary KGs. Nevertheless, the multi-hop reasoning framework popular in binary KGQA task is not directly applicable on n-ary KGQA. We propose two feasible improvements: 1) upgrade the basic reasoning unit from entity or relation to fact, and 2) upgrade the reasoning structure from chain to tree. Therefore, we propose a novel fact-tree reasoning framework, FacTree, which integrates the above two upgrades. FacTree transforms the question into a fact tree and performs iterative fact reasoning on the fact tree to infer the correct answer. Experimental results on the n-ary KGQA dataset we constructed and two binary KGQA benchmarks demonstrate the effectiveness of FacTree compared with state-of-the-art methods.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/xav/Zotero/storage/P5HQB9XL/Zhang et al. - 2022 - Fact-Tree Reasoning for N-ary Question Answering o.pdf}
}

@misc{zhangImpactSymbolicRepresentations2022,
  title = {The {{Impact}} of {{Symbolic Representations}} on {{In-context Learning}} for {{Few-shot Reasoning}}},
  author = {Zhang, Hanlin and Zhang, Yi-Fan and Li, Li Erran and Xing, Eric},
  year = {2022},
  month = dec,
  number = {arXiv:2212.08686},
  eprint = {arXiv:2212.08686},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.08686},
  urldate = {2023-02-07},
  abstract = {Pre-trained language models (LMs) have shown remarkable reasoning performance using explanations (or ``chain-of-thought'' (CoT)) for in-context learning. On the other hand, these reasoning tasks are usually presumed to be more approachable for symbolic programming. To make progress towards understanding in-context learning, we curate synthetic datasets containing equivalent (natural, symbolic) data pairs, where symbolic examples contain first-order logic rules and predicates from knowledge bases (KBs). Then we revisit neuro-symbolic approaches and use Language Models as Logic Programmer (LMLP) that learns from demonstrations containing logic rules and corresponding examples to iteratively reason over KBs, recovering Prolog's backward chaining algorithm. Comprehensive experiments are included to systematically compare LMLP with CoT in deductive reasoning settings, showing that LMLP enjoys more than 25\% higher accuracy than CoT on length generalization benchmarks even with fewer parameters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/ULUH6CGN/Zhang et al. - 2022 - The Impact of Symbolic Representations on In-conte.pdf;/home/xav/Zotero/storage/H9778G4I/2212.html}
}

@article{zhangKnowledgeGraphReasoning2022,
  title = {Knowledge {{Graph Reasoning}} with {{Logics}} and {{Embeddings}}: {{Survey}} and {{Perspective}}},
  shorttitle = {Knowledge {{Graph Reasoning}} with {{Logics}} and {{Embeddings}}},
  author = {Zhang, Wen and Chen, Jiaoyan and Li, Juan and Xu, Zezhong and Pan, Jeff Z. and Chen, Huajun},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.07412 [cs]},
  eprint = {2202.07412},
  primaryclass = {cs},
  urldate = {2022-05-09},
  abstract = {Knowledge graph (KG) reasoning is becoming increasingly popular in both academia and industry. Conventional KG reasoning based on symbolic logic is deterministic, with reasoning results being explainable, while modern embedding-based reasoning can deal with uncertainty and predict plausible knowledge, often with high efficiency via vector computation. A promising direction is to integrate both logic-based and embedding-based methods, with the vision to have advantages of both. It has attracted wide research attention with more and more works published in recent years. In this paper, we comprehensively survey these works, focusing on how logics and embeddings are integrated. We first briefly introduce preliminaries, then systematically categorize and discuss works of logic and embedding-aware KG reasoning from different perspectives, and finally conclude and discuss the challenges and further directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/xav/Zotero/storage/G69JGBP4/Zhang et al. - 2022 - Knowledge Graph Reasoning with Logics and Embeddin.pdf}
}

@misc{zhangMultimodalAnalogicalReasoning2023,
  title = {Multimodal {{Analogical Reasoning}} over {{Knowledge Graphs}}},
  author = {Zhang, Ningyu and Li, Lei and Chen, Xiang and Liang, Xiaozhuan and Deng, Shumin and Chen, Huajun},
  year = {2023},
  month = jan,
  number = {arXiv:2210.00312},
  eprint = {arXiv:2210.00312},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.00312},
  urldate = {2023-02-07},
  abstract = {Analogical reasoning is fundamental to human cognition and holds an important place in various fields. However, previous studies mainly focus on single-modal analogical reasoning and ignore taking advantage of structure knowledge. Notably, the research in cognitive psychology has demonstrated that information from multimodal sources always brings more powerful cognitive transfer than single modality sources. To this end, we introduce the new task of multimodal analogical reasoning over knowledge graphs, which requires multimodal reasoning ability with the help of background knowledge. Specifically, we construct a Multimodal Analogical Reasoning dataSet (MARS) and a multimodal knowledge graph MarKG. We evaluate with multimodal knowledge graph embedding and pre-trained Transformer baselines, illustrating the potential challenges of the proposed task. We further propose a novel model-agnostic Multimodal analogical reasoning framework with Transformer (MarT) motivated by the structure mapping theory, which can obtain better performance. Code and datasets are available in https://github.com/zjunlp/MKG\_Analogy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia},
  file = {/home/xav/Zotero/storage/9R95BBYA/Zhang et al. - 2023 - Multimodal Analogical Reasoning over Knowledge Gra.pdf;/home/xav/Zotero/storage/T6UARLIN/2210.html}
}

@misc{zhangMultimodalChainofThoughtReasoning2023,
  title = {Multimodal {{Chain-of-Thought Reasoning}} in {{Language Models}}},
  author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
  year = {2023},
  month = feb,
  number = {arXiv:2302.00923},
  eprint = {arXiv:2302.00923},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2302.00923},
  urldate = {2023-02-07},
  abstract = {Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies are mostly isolated in the language modality with LLMs, where LLMs are hard to deploy. To elicit CoT reasoning in multimodality, a possible solution is to fine-tune small language models by fusing the vision and language features to perform CoT reasoning. The key challenge is that those language models tend to generate hallucinated reasoning chains that mislead the answer inference. To mitigate the effect of such mistakes, we propose Multimodal-CoT that incorporates vision features in a decoupled training framework. The framework separates the rationale generation and answer inference into two stages. By incorporating the vision features in both stages, the model is able to generate effective rationales that contribute to answer inference. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16\% (75.17\%-{$>$}91.68\%) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available at https://github.com/amazon-science/mm-cot.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/9SMRNZG9/Zhang et al. - 2023 - Multimodal Chain-of-Thought Reasoning in Language .pdf;/home/xav/Zotero/storage/EBQ49JIK/2302.html}
}

@article{zhangMUSEMultiScaleTemporal2021,
  title = {{{MUSE}}: {{Multi-Scale Temporal Features Evolution}} for {{Knowledge Tracing}}},
  shorttitle = {{{MUSE}}},
  author = {Zhang, Chengwei and Jiang, Yangzhou and Zhang, Wei and Gu, Chengyu},
  year = {2021},
  month = jan,
  journal = {arXiv:2102.00228 [cs]},
  eprint = {2102.00228},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Transformer based knowledge tracing model is an extensively studied problem in the field of computer-aided education. By integrating temporal features into the encoder-decoder structure, transformers can processes the exercise information and student response information in a natural way. However, current state-of-the-art transformer-based variants still share two limitations. First, extremely long temporal features cannot well handled as the complexity of self-attention mechanism is O(n2). Second, existing approaches track the knowledge drifts under fixed a window size, without considering different temporal-ranges. To conquer these problems, we propose MUSE, which is equipped with multi-scale temporal sensor unit, that takes either local or global temporal features into consideration. The proposed model is capable to capture the dynamic changes in users knowledge states at different temporal-ranges, and provides an efficient and powerful way to combine local and global features to make predictions. Our method won the 5-th place over 3,395 teams in the Riiid AIEd Challenge 2020.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {/home/xav/Zotero/storage/YJEXBFTN/Zhang et al. - 2021 - MUSE Multi-Scale Temporal Features Evolution for .pdf;/home/xav/Zotero/storage/I767XZK4/2102.html}
}

@article{zhangNeuralSymbolicNeuralSymbolic2021,
  title = {Neural, {{Symbolic}} and {{Neural-Symbolic Reasoning}} on {{Knowledge Graphs}}},
  author = {Zhang, Jing and Chen, Bo and Zhang, Lingxi and Ke, Xirui and Ding, Haipeng},
  year = {2021},
  month = mar,
  journal = {arXiv:2010.05446 [cs]},
  eprint = {2010.05446},
  primaryclass = {cs},
  urldate = {2022-05-09},
  abstract = {Knowledge graph reasoning is the fundamental component to support machine learning applications such as information extraction, information retrieval, and recommendation. Since knowledge graphs can be viewed as the discrete symbolic representations of knowledge, reasoning on knowledge graphs can naturally leverage the symbolic techniques. However, symbolic reasoning is intolerant of the ambiguous and noisy data. On the contrary, the recent advances of deep learning promote neural reasoning on knowledge graphs, which is robust to the ambiguous and noisy data, but lacks interpretability compared to symbolic reasoning. Considering the advantages and disadvantages of both methodologies, recent efforts have been made on combining the two reasoning methods. In this survey, we take a thorough look at the development of the symbolic, neural and hybrid reasoning on knowledge graphs. We survey two specific reasoning tasks \textemdash{} knowledge graph completion and question answering on knowledge graphs, and explain them in a unified reasoning framework. We also briefly discuss the future directions for knowledge graph reasoning.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science},
  file = {/home/xav/Zotero/storage/MPHC9GTP/Zhang et al. - 2021 - Neural, Symbolic and Neural-Symbolic Reasoning on .pdf}
}

@misc{zhangOPTOpenPretrained2022,
  title = {{{OPT}}: {{Open Pre-trained Transformer Language Models}}},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01068},
  eprint = {arXiv:2205.01068},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.01068},
  urldate = {2023-02-16},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/58MDS3Y6/Zhang et al. - 2022 - OPT Open Pre-trained Transformer Language Models.pdf;/home/xav/Zotero/storage/PHRHZQZE/2205.html}
}

@article{zhangRecentAdvancesLeveraging2021,
  title = {Recent {{Advances}} in {{Leveraging Human Guidance}} for {{Sequential Decision-Making Tasks}}},
  author = {Zhang, Ruohan and Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  year = {2021},
  month = oct,
  journal = {Autonomous Agents and Multi-Agent Systems},
  volume = {35},
  number = {2},
  eprint = {2107.05825},
  pages = {31},
  issn = {1387-2532, 1573-7454},
  doi = {10.1007/s10458-021-09514-w},
  urldate = {2022-05-04},
  abstract = {A longstanding goal of artificial intelligence is to create artificial agents capable of learning to perform tasks that require sequential decision making. Importantly, while it is the artificial agent that learns and acts, it is still up to humans to specify the particular task to be performed. Classical task-specification approaches typically involve humans providing stationary reward functions or explicit demonstrations of the desired tasks. However, there has recently been a great deal of research energy invested in exploring alternative ways in which humans may guide learning agents that may, e.g., be more suitable for certain tasks or require less human effort. This survey provides a high-level overview of five recent machine learning frameworks that primarily rely on human guidance apart from pre-specified reward functions or conventional, step-by-step action demonstrations. We review the motivation, assumptions, and implementation of each framework, and we discuss possible future research directions.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/BXE6KJ3T/Zhang et al. - 2021 - Recent Advances in Leveraging Human Guidance for S.pdf}
}

@inproceedings{zhangSituatedQAIncorporatingExtraLinguistic2021,
  title = {{{SituatedQA}}: {{Incorporating Extra-Linguistic Contexts}} into {{QA}}},
  shorttitle = {{{SituatedQA}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Michael and Choi, Eunsol},
  year = {2021},
  month = nov,
  pages = {7371--7387},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.586},
  urldate = {2023-02-01},
  abstract = {Answers to the same question may change depending on the extra-linguistic contexts (when and where the question was asked). To study this challenge, we introduce SituatedQA, an open-retrieval QA dataset where systems must produce the correct answer to a question given the temporal or geographical context. To construct SituatedQA, we first identify such questions in existing QA datasets. We find that a significant proportion of information seeking questions have context-dependent answers (e.g. roughly 16.5\% of NQ-Open). For such context-dependent questions, we then crowdsource alternative contexts and their corresponding answers. Our study shows that existing models struggle with producing answers that are frequently updated or from uncommon locations. We further quantify how existing models, which are trained on data collected in the past, fail to generalize to answering questions asked in the present, even when provided with an updated evidence corpus (a roughly 15 point drop in accuracy). Our analysis suggests that open-retrieval QA benchmarks should incorporate extra-linguistic context to stay relevant globally and in the future. Our data, code, and datasheet are available at https://situatedqa.github.io/.},
  file = {/home/xav/Zotero/storage/VTTPSNX7/Zhang et Choi - 2021 - SituatedQA Incorporating Extra-Linguistic Context.pdf}
}

@article{zhangSurveyComplexFactual2023,
  title = {A Survey on Complex Factual Question Answering},
  author = {Zhang, Lingxi and Zhang, Jing and Ke, Xirui and Li, Haoyang and Huang, Xinmei and Shao, Zhonghui and Cao, Shulin and Lv, Xin},
  year = {2023},
  month = jan,
  journal = {AI Open},
  volume = {4},
  pages = {1--12},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2022.12.003},
  urldate = {2023-01-13},
  abstract = {Answering complex factual questions has drawn a lot of attention. Researchers leverage various data sources to support complex QA, such as unstructured texts, structured knowledge graphs and relational databases, semi-structured web tables, or even hybrid data sources. However, although the ideas behind these approaches show similarity to some extent, there is not yet a consistent strategy to deal with various data sources. In this survey, we carefully examine how complex factual question answering has evolved across various data sources. We list the similarities among these approaches and group them into the analysis\textendash extend\textendash reason framework, despite the various question types and data sources that they focus on. We also address future directions for difficult factual question answering as well as the relevant benchmarks.},
  langid = {english},
  keywords = {Complex question,Document-based question answering,Factual question,Knowledge base question answering,Multi-source question answering,Question answering,Table question answering,Text2SQL},
  file = {/home/xav/Zotero/storage/PS53MSA8/Zhang et al. - 2023 - A survey on complex factual question answering.pdf;/home/xav/Zotero/storage/LYVJ8ID4/S2666651022000249.html}
}

@misc{zhangSurveyControllableText2022,
  title = {A {{Survey}} of {{Controllable Text Generation}} Using {{Transformer-based Pre-trained Language Models}}},
  author = {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
  year = {2022},
  month = jan,
  number = {arXiv:2201.05337},
  eprint = {arXiv:2201.05337},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.05337},
  urldate = {2022-07-19},
  abstract = {Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the common tasks, main approaches and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey paper to summarize CTG techniques from the perspective of PLMs. We hope it can help researchers in related fields to quickly track the academic frontier, providing them with a landscape of the area and a roadmap for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/PSQW3JZF/Zhang et al. - 2022 - A Survey of Controllable Text Generation using Tra.pdf;/home/xav/Zotero/storage/2MJXNMLD/2201.html}
}

@misc{zhangSurveyEfficientOpen2022,
  title = {A {{Survey}} for {{Efficient Open Domain Question Answering}}},
  author = {Zhang, Qin and Chen, Shangsi and Xu, Dongkuan and Cao, Qingqing and Chen, Xiaojun and Cohn, Trevor and Fang, Meng},
  year = {2022},
  month = nov,
  number = {arXiv:2211.07886},
  eprint = {arXiv:2211.07886},
  publisher = {{arXiv}},
  urldate = {2023-01-13},
  abstract = {Open domain question answering (ODQA) is a longstanding task aimed at answering factual questions from a large knowledge corpus without any explicit evidence in natural language processing (NLP). Recent works have predominantly focused on improving the answering accuracy and achieved promising progress. However, higher accuracy often comes with more memory consumption and inference latency, which might not necessarily be efficient enough for direct deployment in the real world. Thus, a trade-off between accuracy, memory consumption and processing speed is pursued. In this paper, we provide a survey of recent advances in the efficiency of ODQA models. We walk through the ODQA models and conclude the core techniques on efficiency. Quantitative analysis on memory cost, processing speed, accuracy and overall comparison are given. We hope that this work would keep interested scholars informed of the advances and open challenges in ODQA efficiency research, and thus contribute to the further development of ODQA efficiency.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/AHAN9V7I/Zhang et al. - 2022 - A Survey for Efficient Open Domain Question Answer.pdf}
}

@misc{zhaoDenseTextRetrieval2022,
  title = {Dense {{Text Retrieval}} Based on {{Pretrained Language Models}}: {{A Survey}}},
  shorttitle = {Dense {{Text Retrieval}} Based on {{Pretrained Language Models}}},
  author = {Zhao, Wayne Xin and Liu, Jing and Ren, Ruiyang and Wen, Ji-Rong},
  year = {2022},
  month = nov,
  number = {arXiv:2211.14876},
  eprint = {arXiv:2211.14876},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.14876},
  urldate = {2023-01-22},
  abstract = {Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user's queries in natural language. From classic retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn the text representation and model the relevance matching. The recent success of pretrained language models (PLMs) sheds light on developing more capable text retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is referred to as dense retrieval, since it employs dense vectors (a.k.a., embeddings) to represent the texts. Considering the rapid progress on dense retrieval, in this survey, we systematically review the recent advances on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related work by four major aspects, including architecture, training, indexing and integration, and summarize the mainstream techniques for each aspect. We thoroughly survey the literature, and include 300+ related reference papers on dense retrieval. To support our survey, we create a website for providing useful resources, and release a code repertory and toolkit for implementing dense retrieval models. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Retrieval},
  file = {/home/xav/Zotero/storage/XZZGTXKQ/Zhao et al. - 2022 - Dense Text Retrieval based on Pretrained Language .pdf;/home/xav/Zotero/storage/FRN6FWDH/2211.html}
}

@misc{zhongVideoQuestionAnswering2022,
  title = {Video {{Question Answering}}: {{Datasets}}, {{Algorithms}} and {{Challenges}}},
  shorttitle = {Video {{Question Answering}}},
  author = {Zhong, Yaoyao and Xiao, Junbin and Ji, Wei and Li, Yicong and Deng, Weihong and Chua, Tat-Seng},
  year = {2022},
  month = nov,
  number = {arXiv:2203.01225},
  eprint = {arXiv:2203.01225},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.01225},
  urldate = {2023-01-24},
  abstract = {Video Question Answering (VideoQA) aims to answer natural language questions according to the given videos. It has earned increasing attention with recent research trends in joint vision and language understanding. Yet, compared with ImageQA, VideoQA is largely underexplored and progresses slowly. Although different algorithms have continually been proposed and shown success on different VideoQA datasets, we find that there lacks a meaningful survey to categorize them, which seriously impedes its advancements. This paper thus provides a clear taxonomy and comprehensive analyses to VideoQA, focusing on the datasets, algorithms, and unique challenges. We then point out the research trend of studying beyond factoid QA to inference QA towards the cognition of video contents, Finally, we conclude some promising directions for future exploration.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/xav/Zotero/storage/LEYBVGAJ/Zhong et al. - 2022 - Video Question Answering Datasets, Algorithms and.pdf;/home/xav/Zotero/storage/NIG8A7GA/2203.html}
}

@misc{zhouDetectingHallucinatedContent2021,
  title = {Detecting {{Hallucinated Content}} in {{Conditional Neural Sequence Generation}}},
  author = {Zhou, Chunting and Neubig, Graham and Gu, Jiatao and Diab, Mona and Guzman, Paco and Zettlemoyer, Luke and Ghazvininejad, Marjan},
  year = {2021},
  month = jun,
  number = {arXiv:2011.02593},
  eprint = {arXiv:2011.02593},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.02593},
  urldate = {2023-02-08},
  abstract = {Neural sequence models can generate highly fluent sentences, but recent studies have also shown that they are also prone to hallucinate additional content not supported by the input. These variety of fluent but wrong outputs are particularly problematic, as it will not be possible for users to tell they are being presented incorrect content. To detect these errors, we propose a task to predict whether each token in the output sequence is hallucinated (not contained in the input) and collect new manually annotated evaluation sets for this task. We also introduce a method for learning to detect hallucinations using pretrained language models fine tuned on synthetic data that includes automatically inserted hallucinations Experiments on machine translation (MT) and abstractive summarization demonstrate that our proposed approach consistently outperforms strong baselines on all benchmark datasets. We further demonstrate how to use the token-level hallucination labels to define a fine-grained loss over the target sequence in low-resource MT and achieve significant improvements over strong baseline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings. Codes and data available at https://github.com/violet-zct/fairseq-detect-hallucination.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/U49NDKQI/Zhou et al. - 2021 - Detecting Hallucinated Content in Conditional Neur.pdf;/home/xav/Zotero/storage/S5JKWM75/2011.html}
}

@inproceedings{zhouGoingVacationTakes2019,
  title = {``{{Going}} on a Vacation'' Takes Longer than ``{{Going}} for a Walk'': {{A Study}} of {{Temporal Commonsense Understanding}}},
  shorttitle = {``{{Going}} on a Vacation'' Takes Longer than ``{{Going}} for a Walk''},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP-IJCNLP}})},
  author = {Zhou, Ben and Khashabi, Daniel and Ning, Qiang and Roth, Dan},
  year = {2019},
  month = nov,
  pages = {3363--3369},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1332},
  urldate = {2023-02-01},
  abstract = {Understanding time is crucial for understanding events expressed in natural language. Because people rarely say the obvious, it is often necessary to have commonsense knowledge about various temporal aspects of events, such as duration, frequency, and temporal order. However, this important problem has so far received limited attention. This paper systematically studies this temporal commonsense problem. Specifically, we define five classes of temporal commonsense, and use crowdsourcing to develop a new dataset, MCTACO, that serves as a test set for this task. We find that the best current methods used on MCTACO are still far behind human performance, by about 20\%, and discuss several directions for improvement. We hope that the new dataset and our study here can foster more future research on this topic.},
  file = {/home/xav/Zotero/storage/2H9IIVNA/Zhou et al. - 2019 - ‚ÄúGoing on a vacation‚Äù takes longer than ‚ÄúGoing for.pdf}
}

@misc{zhouInverseReinforcementLearning2020,
  title = {Inverse {{Reinforcement Learning}} with {{Natural Language Goals}}},
  author = {Zhou, Li and Small, Kevin},
  year = {2020},
  month = dec,
  number = {arXiv:2008.06924},
  eprint = {arXiv:2008.06924},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2008.06924},
  urldate = {2022-10-29},
  abstract = {Humans generally use natural language to communicate task requirements to each other. Ideally, natural language should also be usable for communicating goals to autonomous machines (e.g., robots) to minimize friction in task specification. However, understanding and mapping natural language goals to sequences of states and actions is challenging. Specifically, existing work along these lines has encountered difficulty in generalizing learned policies to new natural language goals and environments. In this paper, we propose a novel adversarial inverse reinforcement learning algorithm to learn a language-conditioned policy and reward function. To improve generalization of the learned policy and reward function, we use a variational goal generator to relabel trajectories and sample diverse goals during training. Our algorithm outperforms multiple baselines by a large margin on a vision-based natural language instruction following dataset (Room-2-Room), demonstrating a promising advance in enabling the use of natural language instructions in specifying agent goals.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/64ELVYS7/Zhou et Small - 2020 - Inverse Reinforcement Learning with Natural Langua.pdf;/home/xav/Zotero/storage/RYGTBPBI/2008.html}
}

@misc{zhouLeasttoMostPromptingEnables2022,
  title = {Least-to-{{Most Prompting Enables Complex Reasoning}} in {{Large Language Models}}},
  author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  year = {2022},
  month = may,
  number = {arXiv:2205.10625},
  eprint = {2205.10625},
  primaryclass = {cs},
  institution = {{arXiv}},
  urldate = {2022-07-25},
  abstract = {We propose a novel prompting strategy, least-to-most prompting, that enables large language models to better perform multi-step reasoning tasks. Least-to-most prompting first reduces a complex problem into a list of subproblems, and then sequentially solves the subproblems, whereby solving a given subproblem is facilitated by the model's answers to previously solved subproblems. Experiments on symbolic manipulation, compositional generalization and numerical reasoning demonstrate that least-to-most prompting can generalize to examples that are harder than those seen in the prompt context, outperforming other prompting-based approaches by a large margin. A notable empirical result is that the GPT-3 code-davinci-002 model with least-to-most-prompting can solve the SCAN benchmark with an accuracy of 99.7\% using 14 examples. As a comparison, the neural-symbolic models in the literature specialized for solving SCAN are trained with the full training set of more than 15,000 examples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/G4PHCNF2/Zhou et al. - 2022 - Least-to-Most Prompting Enables Complex Reasoning .pdf}
}

@misc{zhouTeachingAlgorithmicReasoning2022,
  title = {Teaching {{Algorithmic Reasoning}} via {{In-context Learning}}},
  author = {Zhou, Hattie and Nova, Azade and Larochelle, Hugo and Courville, Aaron and Neyshabur, Behnam and Sedghi, Hanie},
  year = {2022},
  month = nov,
  number = {arXiv:2211.09066},
  eprint = {arXiv:2211.09066},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.09066},
  urldate = {2023-02-07},
  abstract = {Large language models (LLMs) have shown increasing in-context learning capabilities through scaling up model and data size. Despite this progress, LLMs are still unable to solve algorithmic reasoning problems. While providing a rationale with the final answer has led to further improvements in multi-step reasoning problems, Anil et al. 2022 showed that even simple algorithmic reasoning tasks such as parity are far from solved. In this work, we identify and study four key stages for successfully teaching algorithmic reasoning to LLMs: (1) formulating algorithms as skills, (2) teaching multiple skills simultaneously (skill accumulation), (3) teaching how to combine skills (skill composition) and (4) teaching how to use skills as tools. We show that it is possible to teach algorithmic reasoning to LLMs via in-context learning, which we refer to as algorithmic prompting. We evaluate our approach on a variety of arithmetic and quantitative reasoning tasks, and demonstrate significant boosts in performance over existing prompting techniques. In particular, for long parity, addition, multiplication and subtraction, we achieve an error reduction of approximately 10x, 9x, 5x and 2x respectively compared to the best available baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/KMX2H6N3/Zhou et al. - 2022 - Teaching Algorithmic Reasoning via In-context Lear.pdf;/home/xav/Zotero/storage/3IIQJ98J/2211.html}
}

@inproceedings{zhouTextFusionPrivacyPreservingPretrained2022,
  title = {{{TextFusion}}: {{Privacy-Preserving Pre-trained Model Inference}} via {{Token Fusion}}},
  shorttitle = {{{TextFusion}}},
  booktitle = {Conference on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhou, Xin and Lu, Jinzhu and Gui, Tao and Ma, Ruotian and Fei, Zichu and Wang, Yuran and Ding, Yong and Cheung, Yibo and Zhang, Qi and Huang, Xuanjing},
  year = {2022},
  urldate = {2023-02-08},
  abstract = {Recently, more and more pre-trained language models are released as a cloud service. It allows users who lack computing resources to perform inference with a powerful model by uploading data to the cloud. The plain text may contain private information, as the result, users prefer to do partial computations locally and upload intermediate representations to the cloud for subsequent inference.However, recent studies have shown that intermediate representations can also be recovered to plain text with reasonable accuracy, thus the risk of privacy leakage still exists. To address this issue, we propose TextFusion, a novel method for preserving inference privacy.Specifically, we train a Fusion Predictor to dynamically fuse token representations, which hides multiple private token representations behind an unrecognizable one.Furthermore, an adversarial training regime is employed to privatize these representations. In this way, the cloud only receives incomplete and perturbed representations, making it difficult to accurately recover the complete plain text.The experimental results on diverse classification tasks show that our approach can effectively preserve inference privacy without significantly sacrificing performance in different scenarios.},
  file = {/home/xav/Zotero/storage/8YHYT62M/Zhou et al. - 2022 - TextFusion Privacy-Preserving Pre-trained Model I.pdf}
}

@inproceedings{zhuApproximateOntologyReasoning2021,
  title = {Approximate {{Ontology Reasoning}} for {{Domain-Specific Knowledge Graph}} Based on {{Deep Learning}}},
  booktitle = {2021 7th {{International Conference}} on {{Big Data}} and {{Information Analytics}} ({{BigDIA}})},
  author = {Zhu, Xixi and Liu, Bin and Ding, Zhaoyun and Zhu, Cheng and Yao, Li},
  year = {2021},
  month = oct,
  pages = {172--179},
  doi = {10.1109/BigDIA53151.2021.9619694},
  abstract = {Ontology reasoning has great potential in application of domain-specific knowledge graph. However, the traditional ontology reasoner is difficult to perform rapidly reasoning on domain-specific knowledge graph as its complex logical expression and large-scale assertion set. On the other hand, the knowledge graph reasoning method based on presentation learning has better scalability, but it is hard to incorporate expert knowledge in the schema-level information effectively, so it can't implement complex reasoning. The above restricts the application of reasoning in domain-specific knowledge graph. In order to solve the problem, researchers try to combine ontology reasoning with deep learning to achieve approximate ontology reasoning. However, the state-of-the-art method has shortcomings of model design and evaluation which make it lose generalization ability. This paper proposes a novel method to transform ontology reasoning into a graph-to-graph relational mapping learning model, and realizes approximate ontology reasoning based on deep learning. Besides, this paper also defines new knowledge graph reasoning evaluation criterion, and synthesizes new test data. On this basis, relevant comparative experiments are carried out. The results show that the proposed method achieves good performance on LUBM synthetic dataset, even though the data mode that not appear in the training can also be inferred effectively, which can make the model applicable to a wider range of application scenarios.},
  keywords = {Approximate Ontology Reasoning,Big Data,Cognition,Deep learning,Deep Learning,Domain-specific Knowledge Graph,Knowledge Reasoning,Ontologies,Scalability,Training,Transforms},
  file = {/home/xav/Zotero/storage/5SFKQF4H/9619694.html}
}

@misc{zhuBiologicallyInspiredDesign2022,
  title = {Biologically {{Inspired Design Concept Generation Using Generative Pre-Trained Transformers}}},
  author = {Zhu, Qihao and Zhang, Xinyu and Luo, Jianxi},
  year = {2022},
  month = dec,
  number = {arXiv:2212.13196},
  eprint = {arXiv:2212.13196},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2212.13196},
  urldate = {2023-01-03},
  abstract = {Biological systems in nature have evolved for millions of years to adapt and survive the environment. Many features they developed can be inspirational and beneficial for solving technical problems in modern industries. This leads to a specific form of design-by-analogy called bio-inspired design (BID). Although BID as a design method has been proven beneficial, the gap between biology and engineering continuously hinders designers from effectively applying the method. Therefore, we explore the recent advance of artificial intelligence (AI) for a data-driven approach to bridge the gap. This paper proposes a generative design approach based on the generative pre-trained language model (PLM) to automatically retrieve and map biological analogy and generate BID in the form of natural language. The latest generative pre-trained transformer, namely GPT-3, is used as the base PLM. Three types of design concept generators are identified and fine-tuned from the PLM according to the looseness of the problem space representation. Machine evaluators are also fine-tuned to assess the mapping relevancy between the domains within the generated BID concepts. The approach is evaluated and then employed in a real-world project of designing light-weighted flying cars during its conceptual design phase The results show our approach can generate BID concepts with good performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/INJ74LVX/Zhu et al. - 2022 - Biologically Inspired Design Concept Generation Us.pdf}
}

@article{zhuGenerativePreTrainedTransformer2021,
  title = {Generative {{Pre-Trained Transformer}} for {{Design Concept Generation}}: {{An Exploration}}},
  shorttitle = {Generative {{Pre-Trained Transformer}} for {{Design Concept Generation}}},
  author = {Zhu, Qihao and Luo, Jianxi},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.08489 [cs]},
  eprint = {2111.08489},
  primaryclass = {cs},
  urldate = {2022-05-05},
  abstract = {Novel concepts are essential for design innovation and can be generated with the aid of data stimuli and computers. However, current generative design algorithms focus on diagrammatic or spatial concepts that are either too abstract to understand or too detailed for early phase design exploration. This paper explores the uses of generative pre-trained transformers (GPT) for natural language design concept generation. Our experiments involve the use of GPT-2 and GPT-3 for different creative reasonings in design tasks. Both show reasonably good performance for verbal design concept generation.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/xav/Zotero/storage/LYINW5ME/Zhu et Luo - 2021 - Generative Pre-Trained Transformer for Design Conc.pdf}
}

@misc{zhuReasonChainQATextbasedComplex2022,
  title = {{{ReasonChainQA}}: {{Text-based Complex Question Answering}} with {{Explainable Evidence Chains}}},
  shorttitle = {{{ReasonChainQA}}},
  author = {Zhu, Minjun and Weng, Yixuan and He, Shizhu and Liu, Kang and Zhao, Jun},
  year = {2022},
  month = oct,
  number = {arXiv:2210.08763},
  eprint = {arXiv:2210.08763},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.08763},
  urldate = {2023-01-19},
  abstract = {The ability of reasoning over evidence has received increasing attention in question answering (QA). Recently, natural language database (NLDB) conducts complex QA in knowledge base with textual evidences rather than structured representations, this task attracts a lot of attention because of the flexibility and richness of textual evidence. However, existing text-based complex question answering datasets fail to provide explicit reasoning process, while it's important for retrieval effectiveness and reasoning interpretability. Therefore, we present a benchmark \textbackslash textbf\{ReasonChainQA\} with explanatory and explicit evidence chains. ReasonChainQA consists of two subtasks: answer generation and evidence chains extraction, it also contains higher diversity for multi-hop questions with varying depths, 12 reasoning types and 78 relations. To obtain high-quality textual evidences for answering complex question. Additional experiment on supervised and unsupervised retrieval fully indicates the significance of ReasonChainQA. Dataset and codes will be made publicly available upon accepted.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/95WCEAKY/Zhu et al. - 2022 - ReasonChainQA Text-based Complex Question Answeri.pdf;/home/xav/Zotero/storage/92H8FB6Q/2210.html}
}

@article{zhuRetrievingReadingComprehensive2021,
  title = {Retrieving and {{Reading}}: {{A Comprehensive Survey}} on {{Open-domain Question Answering}}},
  shorttitle = {Retrieving and {{Reading}}},
  author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
  year = {2021},
  month = may,
  journal = {arXiv:2101.00774 [cs]},
  eprint = {2101.00774},
  primaryclass = {cs},
  urldate = {2022-05-04},
  abstract = {Open-domain Question Answering (OpenQA) is an important task in Natural Language Processing (NLP), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on OpenQA, particularly on techniques that integrate with neural Machine Reading Comprehension (MRC). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on QA systems. In this work, we review the latest research trends in OpenQA, with particular attention to systems that incorporate neural MRC techniques. Specifically, we begin with revisiting the origin and development of OpenQA systems. We then introduce modern OpenQA architecture named ``Retriever-Reader'' and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing OpenQA systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in OpenQA research, so as to stimulate further progress in this field.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/xav/Zotero/storage/AMGGMAUD/Zhu et al. - 2021 - Retrieving and Reading A Comprehensive Survey on .pdf}
}

@misc{zhuSolvingMathWord2022,
  title = {Solving {{Math Word Problem}} via {{Cooperative Reasoning}} Induced {{Language Models}}},
  author = {Zhu, Xinyu and Wang, Junjie and Zhang, Lin and Zhang, Yuxiang and Gan, Ruyi and Zhang, Jiaxing and Yang, Yujiu},
  year = {2022},
  month = oct,
  number = {arXiv:2210.16257},
  eprint = {arXiv:2210.16257},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.16257},
  urldate = {2022-12-09},
  abstract = {Large-scale pre-trained language models (PLMs) bring new opportunities to challenge problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufficient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Cooperative Reasoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the verifier. In our approach, the generator is responsible for generating reasoning paths, and the verifiers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.8\% increase over best baselines.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/7UV3VNFX/Zhu et al. - 2022 - Solving Math Word Problem via Cooperative Reasonin.pdf;/home/xav/Zotero/storage/LLV24WW8/2210.html}
}

@misc{zongSurveyGPT32022,
  title = {A Survey on {{GPT-3}}},
  author = {Zong, Mingyu and Krishnamachari, Bhaskar},
  year = {2022},
  month = dec,
  number = {arXiv:2212.00857},
  eprint = {arXiv:2212.00857},
  publisher = {{arXiv}},
  urldate = {2023-02-02},
  abstract = {This paper provides an introductory survey to GPT-3. We cover some of the historical development behind this technology, some of the key features of GPT-3, and discuss the machine learning model and the datasets used. We survey both academic and commercial efforts applying GPT-3 in diverse domains such as developing conversational AI chatbots, software development, creative work, domain knowledge, and business productivity. We discuss some of the challenges that GPT-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers. We also discuss the future research opportunities in this area.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/BX5PXEKZ/Zong et Krishnamachari - 2022 - a survey on GPT-3.pdf}
}

@misc{ZoteroDownloads,
  title = {Zotero | {{Downloads}}},
  urldate = {2023-03-24},
  howpublished = {https://www.zotero.org/download/},
  file = {/home/xav/Zotero/storage/WURVGKSP/download.html}
}

@article{zouTopicOrientedSpokenDialogue2021,
  title = {Topic-{{Oriented Spoken Dialogue Summarization}} for {{Customer Service}} with {{Saliency-Aware Topic Modeling}}},
  author = {Zou, Yicheng and Zhao, Lujun and Kang, Yangyang and Lin, Jun and Peng, Minlong and Jiang, Zhuoren and Sun, Changlong and Zhang, Qi and Huang, Xuanjing and Liu, Xiaozhong},
  year = {2021},
  month = jun,
  journal = {arXiv:2012.07311 [cs]},
  eprint = {2012.07311},
  primaryclass = {cs},
  urldate = {2022-05-13},
  abstract = {In a customer service system, dialogue summarization can boost service efficiency by automatically creating summaries for long spoken dialogues in which customers and agents try to address issues about specific topics. In this work, we focus on topic-oriented dialogue summarization, which generates highly abstractive summaries that preserve the main ideas from dialogues. In spoken dialogues, abundant dialogue noise and common semantics could obscure the underlying informative content, making the general topic modeling approaches difficult to apply. In addition, for customer service, role-specific information matters and is an indispensable part of a summary. To effectively perform topic modeling on dialogues and capture multi-role information, in this work we propose a novel topic-augmented two-stage dialogue summarizer (TDS) jointly with a saliency-aware neural topic model (SATM) for topic-oriented summarization of customer service dialogues. Comprehensive studies on a real-world Chinese customer service dataset demonstrated the superiority of our method against several strong baselines.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/home/xav/Zotero/storage/7K3MULII/Zou et al. - 2021 - Topic-Oriented Spoken Dialogue Summarization for C.pdf}
}
